{
  "ticker": "AMD",
  "last_updated": "2026-02-04T14:39:29.824Z",
  "total_transcripts": 2,
  "transcripts": [
    {
      "ticker": "AMD",
      "title": "Advanced Micro Devices, Inc. (AMD) Q3 FY2025 earnings call transcript",
      "published_date": "2026-02-04T14:39:26.235Z",
      "fiscal_year": "2025",
      "quarter": "Q3",
      "url": "https://finance.yahoo.com/quote/AMD/earnings/AMD-Q3-2025-earnings_call-370771.html",
      "content": "**Operator**\nGreetings and welcome to the AMD Third Quarter 2025 conference call. At this time, all participants are in a listen-only mode. A question-and-answer session will follow the formal presentation. If anyone should require operator assistance, please press star zero on your telephone keypad. As a reminder, this conference call is being recorded. It is now my pleasure to introduce to you Matt Ramsay, VP, Financial Strategy and Investor Relations. Thank you, Matt. You may begin.\n\n**Matt Ramsay** (VP of Financial Strategy and Investor Relations)\nThank you and welcome to AMD Third Quarter 2025 Financial Results Conference Call. By now, you should have had the opportunity to review a copy of our earnings press release and the accompanying slides. If you have not had the opportunity to review these materials, they can be found on the Investor Relations page of AMD.com. We will refer primarily to non-GAAP financial measures during today's call. The full non-GAAP to GAAP reconciliations are available in today's press release and the slides posted on our website. Participants in today's conference call are Dr. Lisa Su, our Chair and CEO, and Jean Hu, our Executive Vice President, CFO, and Treasurer. This is a live call and will be replayed via webcast on our website. Before we begin the call, I would like to note that Dr.\n\n**Matt Ramsay** (VP of Financial Strategy and Investor Relations)\nLisa Su, along with members of AMD's executive team, will present our long-term financial strategy at our Financial Analyst Day next Tuesday, November 11th, in New York. Dr. Lisa Su will present at the UBS Global Technology and AI Conference on Wednesday, December 3rd. And finally, Jean Hu will present at the 23rd Annual Barclays Global Technology Conference on Wednesday, December 10th. Today's discussion contains forward-looking statements based on our current beliefs, assumptions, and expectations, speak only as of today, and as such involve risks and uncertainties that could cause results to differ materially from our current expectations. Please refer to the cautionary statement in our press release for more information on these factors that could cause actual results to differ materially. And with that, I will hand the call over to Lisa.\n\n**Lisa Su** (Chair and CEO)\nThank you, Matt, and good afternoon to all those listening today. We delivered an outstanding quarter with record revenue and profitability reflecting broad-based demand across our data center AI, server, and PC businesses. Revenue grew 36% year over year to $9.2 billion. Net income rose 31% and free cash flow more than tripled, led by record EPYC, Ryzen, and Instinct processor sales. Our record third-quarter performance marks a clear step up in our growth trajectory as a combination of our expanding compute franchise and rapidly scaling data center AI business drives significant revenue and earnings growth. Turning to our segments, data center segment revenue increased 22% year over year to a record $4.3 billion, led by the ramp of our Instinct MI350 series GPUs and server share gains.\n\n**Lisa Su** (Chair and CEO)\nServer CPU revenue reached an all-time high as adoption of fifth-gen EPYC Turin processors accelerated rapidly, accounting for nearly half of overall EPYC revenue in the quarter. Sales of our prior-generation EPYC processors were also very robust in the quarter, reflecting their strong competitive positioning across a wide range of workloads. In cloud, we had record sales as hyperscalers expanded EPYC CPU deployments to power both their own first-party services and public cloud offerings. Hyperscalers launched more than 160 EPYC-powered instances in the quarter, including new Turin offerings from Google, Microsoft Azure, Alibaba, and others that deliver unmatched performance and price performance across a wide range of workloads. There are now more than 1,350 public EPYC cloud instances available globally, a nearly 50% increase from a year ago.\n\n**Lisa Su** (Chair and CEO)\nAdoption of EPYC in the cloud by large businesses more than tripled year over year, as our on-prem share gains are driving increased demand from enterprise customers for AMD cloud instances to support hybrid compute. We expect cloud demand to remain very strong as hyperscalers are significantly increasing their general-purpose compute capacity as they scale their AI workloads. Many customers are now planning substantially larger CPU buildouts over the coming quarters to support increased demands from AI, serving as a powerful new catalyst for our server business. Turning to enterprise adoption, EPYC server sell-through increased sharply year over year and sequentially, reflecting accelerating enterprise adoption. More than 170 fifth-gen EPYC platforms are in market from HPE, Dell, Lenovo, Supermicro, and others, our broadest portfolio to date, with solutions optimized for virtually every enterprise workload.\n\n**Lisa Su** (Chair and CEO)\nWe close large new wins in the quarter with leading Fortune 500 technology, telecom, financial services, retail, streaming, social, and automotive companies as we expand our footprint across major verticals. The performance and TCO advantages of our EPYC portfolio, combined with our increased go-to-market investments and the expanded breadth of offerings from the leading server and solutions providers, position us well for continued enterprise share gains. Looking ahead, we remain on track to launch our next-generation 2-nanometer Venice EPYC processors in 2026. Venice silicon is in the labs and performing very well, delivering substantial gains in performance, efficiency, and compute density. Customer pull and engagement for Venice are the strongest we have seen, reflecting our competitive positioning and the growing demand for more data center compute.\n\n**Lisa Su** (Chair and CEO)\nMultiple cloud and OEM partners have already brought their first Venice platforms online, setting the stage for broad solution availability and cloud deployments at launch. Turning to data center AI, our Instinct GPU business continues to accelerate. Revenue grew year over year, driven by the sharp ramp of MI350 series GPU sales and broader MI300 series deployments. Multiple MI350 series deployments are underway with large cloud and AI providers, with additional large-scale rollouts on track to ramp over the coming quarters. Oracle became the first hyperscaler to publicly offer MI355X instances, delivering significantly higher performance for real-time inference and multimodal training workloads on OCI ZetaScale Supercluster. Neocloud providers Crusoe, DigitalOcean, TensorWave, Vultr, and others also began ramping availability of their MI350 series public cloud offerings in the quarter. MI300 series GPU deployments with AI developers also broadened in the quarter.\n\n**Lisa Su** (Chair and CEO)\nIBM and Zyphra will train multiple generations of future multimodal models on a large-scale MI300X cluster, and Cohere is now using MI300X at OCI to train its command family of models. For inference, a number of new partners, including Character AI and Luma AI, are now running production workloads on MI300 series, demonstrating the performance and TCO advantages of our architecture for real-time AI applications. We also made significant progress on the software front in the quarter. We launched ROCm7, our most advanced and feature-rich release to date, delivering up to 4.6x higher inference and 3x higher training performance compared to ROCm6. ROCm7 also introduces seamless distributed inference, enhanced code portability across hardware, and new enterprise tools that simplify the deployment and management of Instinct solutions. Importantly, our open software strategy is resonating with developers.\n\n**Lisa Su** (Chair and CEO)\nHugging Face, VLLM, SGLang, and others contributed directly to ROCm7, as we make ROCm the open platform for AI development at scale. Looking ahead, our data center AI business is entering its next phase of growth, with customer momentum building rapidly ahead of the launch of our next-gen MI400 series accelerators and Helios Rack Scale solutions in 2026. The MI400 series combines a new compute engine with industry-leading memory capacity and advanced networking capabilities to deliver a major leap in performance for the most demanding AI training and inference workloads. The MI400 series brings together our Silicon software and systems expertise to power Helios, our Rack Scale AI platform. Designed to redefine performance and efficiency at data center scale.\n\n**Lisa Su** (Chair and CEO)\nHelios integrates our Instinct MI400 series GPUs, Venice EPYC CPUs, and Pensando NICs in a double-wide rack solution optimized for the performance, power, cooling, and serviceability required for the next generation of AI infrastructure and supports Meta's new open rack wide standard. Development of both our MI400 series GPUs and Helios rack is progressing rapidly, supported by deep technical engagements across a growing set of hyperscalers, AI companies, and OEM and ODM partners to enable large-scale deployments next year. The ZT Systems team we acquired last year is playing a critical role in Helios development, leveraging their decades of experience building infrastructure for the world's largest cloud providers to ensure customers can deploy and scale Helios quickly within their environments. In addition, last week we completed the sale of the ZT manufacturing business to Sanmina and entered a strategic partnership that makes them our lead manufacturing partner for Helios.\n\n**Lisa Su** (Chair and CEO)\nThis collaboration will accelerate large customer deployments of our Rack Scale AI solutions. On the customer front, we announced a comprehensive multi-year agreement with OpenAI to deploy 6 gigawatts of Instinct GPUs, with the first gigawatt of MI450 series accelerators scheduled to start coming online in the second half of 2026. The partnership establishes AMD as a core compute provider for OpenAI and underscores the strength of our hardware, software, and full-stack solutions strategy. Moving forward, AMD and OpenAI will work even more closely on future hardware, software, networking, and system-level roadmaps and technologies. OpenAI's decision to use AMD Instinct platforms for its most sophisticated and complex AI workloads sends a clear signal that our Instinct GPUs and ROCm open software stack deliver the performance and TCO required for the most demanding deployments.\n\n**Lisa Su** (Chair and CEO)\nWe expect this partnership will significantly accelerate our data center AI business, with the potential to generate well over $100 billion in revenue over the next few years. Oracle announced they will also be a lead launch partner for the MI450 series, deploying tens of thousands of MI450 GPUs across Oracle Cloud Infrastructure beginning in 2026 and expanding through 2027 and beyond. Our Instinct platforms are also gaining traction with sovereign AI and national supercomputing programs. In the UAE, Cisco and G42 will deploy a large-scale AI cluster powered by Instinct MI350X GPUs to support the nation's most advanced AI workloads. In the US, we are partnering with the Department of Energy and Oak Ridge National Labs to build Lux AI, the first AI factory dedicated to scientific discovery, together with our industrial partners OCI and HPE.\n\n**Lisa Su** (Chair and CEO)\nPowered by our Instinct MI350 series GPUs, EPYC CPUs, and Pensando networking, Lux AI will provide a secure open platform for large-scale training and distributed inference when it comes online in early 2026. The US Department of Energy also selected our upcoming MI430X GPUs and EPYC Venice CPUs to power Discovery, the next flagship supercomputer at Oak Ridge, designed to set the standard for AI-driven scientific computing and extend US high-performance computing leadership. Our MI430X GPUs are designed specifically to power nation-scale AI and supercomputing programs, extending our leadership powering the world's most powerful computers to enable the next generation of scientific breakthroughs. In summary, our AI business is entering a new phase of growth and is on a clear trajectory towards tens of billions in annual revenue in 2027, driven by our leadership, Rack Scale solutions, expanding customer adoption, and an increasing number of large-scale global deployments.\n\n**Lisa Su** (Chair and CEO)\nI look forward to providing more details on our data center AI growth plans at our Financial Analyst Day next week. In client and gaming, segment revenue increased 73% year over year to $4 billion. Our PC processor business is performing exceptionally well, with record quarterly sales as the strong demand environment and breadth of our leadership Ryzen portfolio accelerates growth. Desktop CPU sales reach an all-time high, with record channel sell-in and sell-out led by robust demand for our Ryzen 9000 processors, which deliver unmatched performance across gaming, productivity, and content creation applications. OEM sell-through of Ryzen-powered notebooks also increased sharply in the quarter, reflecting sustained end-customer pull for premium gaming and commercial AMD PCs.\n\n**Lisa Su** (Chair and CEO)\nCommercial momentum accelerated in the quarter, with Ryzen PC sell-through up more than 30% year over year as enterprise adoption grew sharply, driven by large wins with Fortune 500 companies across healthcare, financial services, manufacturing, automotive, and pharmaceuticals. Looking ahead, we see significant opportunity to continue growing our client business faster than the overall PC market, based on the strength of our Ryzen portfolio, broader platform coverage, and expanded go-to-market investments. In gaming, revenue increased 181% year over year to $1.3 billion. Semi-custom revenue increased as Sony and Microsoft prepare for the upcoming holiday sales period. In gaming graphics, revenue and channel sell-out grew significantly, driven by the performance-per-dollar leadership of our Radeon 9000 family. FSR4, our machine learning upscaling technology that boosts frame rates and creates more immersive visuals, saw rapid adoption this quarter, with the number of supported games doubling since launch to more than 85.\n\n**Lisa Su** (Chair and CEO)\nTurning to our embedded segment, revenue decreased 8% year over year to $857 million. Sequentially, revenue and sell-through increased as the demand environment strengthened across multiple markets, led by tests in emulation, aerospace and defense, and industrial vision and healthcare. We expanded our embedded product portfolio with new solutions that extend our leadership across adaptive and x86 computing. We began shipping industry-leading Versal Prime Series Gen 2 adaptive SoCs to lead customers, delivered our first Versal RF development platforms to support several next-generation design wins, and introduced the Ryzen embedded 9000 series with industry-leading performance per watt and latency for robotics, edge computing, and smart factory applications. The design momentum remains very strong across our embedded portfolio.\n\n**Lisa Su** (Chair and CEO)\nWe are on track for a second straight year of record design wins, already totaling more than $14 billion year to date, reflecting the growing adoption of our leadership products across a broad range of markets and expanding set of applications. In summary, our record third-quarter results and strong fourth-quarter outlook reflect the significant momentum building across our business, driven by sustained product leadership and disciplined execution. Our data center AI, server, and PC businesses are each entering periods of strong growth, led by an expanding TAM, accelerating adoption of our Instinct platforms, and EPYC and Ryzen CPU share gains. The demand for compute has never been greater, as every major breakthrough in business, science, and society now relies on access to more powerful, efficient, and intelligent computing. These trends are driving unprecedented growth opportunities for AMD.\n\n**Lisa Su** (Chair and CEO)\nI look forward to sharing more on our strategy, roadmaps, and long-range financial targets at our Financial Analysts meeting next week. Now I'll turn the call over to Jean to provide additional color on our third-quarter results. Jean?\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nThank you, Lisa, and good afternoon, everyone. I'll start with a review of our financial results and then provide our outlook for the fourth quarter of fiscal 2025. We're pleased with our strong third-quarter financial results. We delivered a record revenue of $9.2 billion, up 36% year over year, exceeding the high end of our guidance, reflecting strong momentum across our business. Our third-quarter results do not include any revenue from shipment of the MI308 GPU products to China. Revenue increased 20% sequentially, driven by strong growth in the data center and client and gaming segment, and modest growth in the embedded segment.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nGross margin was 54%, up 40 basis points year over year, primarily driven by product mix. Operating expenses were approximately $2.8 billion, an increase of 42% year over year as we continue to invest aggressively in R&D to capitalize on significant AI opportunities and go-to-market activities for revenue growth. Operating income was $2.2 billion, representing a 24% operating margin. Taxes, interest expense, and other totaled $273 million. For the third quarter of 2025, diluted earnings per share were $1.20 compared to $0.92 a year ago, an increase of 30% year over year. Now turning to our reportable segments, starting with the data center. Data center segment revenue was a record of $4.3 billion, up 22% year over year, primarily driven by the strong demand for fifth-generation EPYC processors and Instinct MI350 series GPUs. On a sequential basis, data center revenue increased 34%.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nPrimarily driven by strong ramp of our AMD Instinct MI350 series GPUs. The data center segment operating income was $1.1 billion, or 25% of revenue, compared to $1 billion a year ago, or 29% of revenue, driven by higher revenue, partially offset by higher R&D investment to capitalize on significant AI opportunities. Client and gaming segment revenue was a record of $4 billion, up 73% year over year and 12% sequentially, driven by strong demand for the latest generation of client and graphics processors and stronger sales of console gaming products. In the client business, revenue was a record $2.8 billion, up 46% year over year and 10% sequentially, driven by record sales of our Ryzen processors and the richer product mix. Gaming revenue rose to $1.3 billion, up 181% year over year and 16% sequentially, reflecting higher semi-custom revenue and strong demand for our Radeon GPUs.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nClient and gaming segment operating income was $867 million, or 21% of revenue, compared to $288 million, or 12% a year ago, driven by higher revenue, partially offset by increase in go-to-market investment to support our revenue growth. Embedded segment revenue was $857 million, down 8% year over year. Embedded was up 4% sequentially as we saw certain end-market demand strengthen. Embedded segment operating income was $283 million, or 33% of revenue, compared to $372 million, or 40% a year ago. The decline in operating income was primarily due to lower revenue and end-market mix. Before I review the balance sheet and the cash flow, as a reminder, we closed the sale of ZT Systems Manufacturing business to Sanmina last week. The third-quarter financial results of the ZT Manufacturing business are reported separately in our financial statements as discontinued operations and are excluded from our non-GAAP financials.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nTurning to the balance sheet and the cash flow, during the quarter, we generated $1.8 billion in cash from operating activities of continuing operations, and the free cash flow was a record of $1.5 billion. We returned $89 million to shareholders through share repurchases, resulting in $1.3 billion in share repurchases for the first three quarters of 2025. Exiting the quarter, we have $9.4 billion authorization remaining and our share repurchase program. At the end of the quarter, cash, cash equivalent, and short-term investment was $7.2 billion. Our total debt was $3.2 billion. Now turning to our fourth quarter 2025 outlook, please note that our fourth quarter outlook does not include any revenue from AMD Instinct MI308 shipment to China. For the fourth quarter of 2025, we expect revenue to be approximately $9.6 billion, plus or minus $300 million.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nThe midpoint of our guidance represents approximately 25% year-over-year revenue growth, driven by strong double-digit growth in our data center and client and gaming segment, and a return to growth in our embedded segment. Sequentially, we expect revenue to grow by approximately 4%. Driven by double-digit growth in the data center segment, with strong growth in server and continued ramp of our MI350 series GPUs. A decline in our client and gaming segment, with client revenue increasing and gaming revenue down strong double digits. And double-digit growth in our embedded segment. In addition, we expect fourth-quarter non-GAAP gross margin to be approximately 54.5%. And we expect non-GAAP operating expenses to be approximately $2.8 billion. We expect net interest and other expenses to be gain of approximately $37 million. We expect our non-GAAP effective tax rate to be 13%. And diluted share count is expected to be approximately 1.65 billion shares.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nIn closing, we executed very well, delivering record revenue for the first three quarters of the year. The strategic investment we are making positions us well to capitalize on expanding AI opportunities across all our end markets, driving sustainable long-term revenue growth and earnings expansion for compelling shareholder value creation. With that, I'll turn it back to Matt for the Q&A session.\n\n**Matt Ramsay** (VP of Financial Strategy and Investor Relations)\nThank you very much, Jean. John, we can go ahead and poll the audience for questions now.\n\n**Operator**\nThank you. We will now be conducting a question-and-answer session. If you would like to ask a question, please press star one on your telephone keypad. A confirmation tone will indicate that your line is in the queue. You may press star two to remove yourself from the queue. For participants using speaker equipment, it may be necessary to pick up your handset before pressing the star keys.\n\n**Operator**\nWe ask that you please limit yourself to one question and one follow-up. Thank you. One moment while we poll for questions. And the first question comes from the line of Vivek Arya with Bank of America Securities. Please proceed with your question.\n\n**Vivek Arya** (Managing Director and Senior Analyst)\nThank you for the question. I had a near-term and a medium-term question. For the near-term, Lisa, I was hoping if you could give us some sense of the CPU/GPU mix in Q3 and Q4. And just tactically, how are you managing this transition from your MI355 towards MI400 in the second half of next year? Can you continue to grow in the first half of next year from these Q4 levels, or should we expect some kind of pause or digestion before customers get on board the MI400 series?\n\n**Lisa Su** (Chair and CEO)\nSure, Vivek. Thanks for the question. So a couple of comments.\n\n**Lisa Su** (Chair and CEO)\nWe had a very strong Q3 for the data center business. I think we saw strong outperformance in both the server as well as the data center AI business. And a reminder that that was without any MI308 sales. The MI355 has ramped really nicely. We expected a sharp ramp into the third quarter, and that proceeded well. And as I mentioned, we've also seen some strengthening of the server CPU sales. And not just, let's call it near-term, but we're seeing our customers are giving us some visibility in the next few quarters that they see elevated demand, which is positive. Going into the fourth quarter, again, strong data center performance, up double digits sequentially, and up in both server and data center AI, again, on the strength of those businesses.\n\n**Lisa Su** (Chair and CEO)\nAnd to your question, I mean, we're not diving into 2026 yet, obviously, but given what we see today, we see a very good demand environment into 2026. So we would expect that MI355 continue to ramp in the first half of 2026. And then. As we mentioned, MI450 series comes online in the second half of 2026, and we would expect a sharper ramp as we go into the second half of 2026 of our data center AI business.\n\n**Vivek Arya** (Managing Director and Senior Analyst)\nGot it. And from my follow-up, there is some industry debate, Lisa, about OpenAI's ability to kind of simultaneously engage with all three merchants and the ASIC suppliers, just given the constraints around power and CapEx and their existing kind of CSP partners and so forth. So how are you thinking about that?\n\n**Vivek Arya** (Managing Director and Senior Analyst)\nWhat is your level of visibility in the initial engagement, and then, more importantly, how it kind of broadens out into 2027? Is there a way that one can model what the allocation would be, or just how should we think about the level of visibility in this very important customer? Thank you.\n\n**Lisa Su** (Chair and CEO)\nYeah, absolutely, Vivek. Look, we're very, obviously, very excited about our relationship with OpenAI. It's a very significant relationship. Think about it as it's a pretty unique time for AI right now. There's just so much compute demand across all of the workloads. I think in our work with OpenAI, we are planning multiple quarters out, ensuring that the power is available, that the supply chain is available. The key point is the first gigawatt we will start deploying in the second half of 2026, and that work is well underway.\n\n**Lisa Su** (Chair and CEO)\nAnd we continue, just given where lead times are and things like that, we are planning very closely with OpenAI as well as the CSP partners to ensure that we're all prepared with Helios so that we can deploy the technology as we stated. So I think overall, we're working very closely together. I think we have good visibility into the MI450 ramp, and things are progressing very well.\n\n**Operator**\nAnd the next question comes from the line of Thomas O'Malley with Barclays. Please proceed with your question.\n\n**Thomas O'Malley** (Director of Equity Research)\nGood morning. Thanks for taking my question and congrats on the good results. I had a first question on Helios. Obviously, with the announcement of OCP, customer interaction has to be growing. Could you talk about into next year, what your view is on discrete sales versus system sales? When do you see that crossover kind of happening?\n\n**Thomas O'Malley** (Director of Equity Research)\nAnd just what initial responses have been from customers after getting a better look at it at the show?\n\n**Lisa Su** (Chair and CEO)\nYeah, sure. Tom, thanks for the question. There's a lot of excitement around MI450 and Helios. I think the OCP reception was phenomenal. We had numerous customers and, frankly, bringing their engineering teams to understand more about the system, more about how it's built. There's always been some discussion about just how complex these rack scale systems are, and they certainly are. And we are very proud of the Helios design. I think it has all of the features, functions, reliability, performance, power performance that you would expect. I think the interest in MI450 and Helios has just expanded over the last number of weeks, certainly with some of the announcements that we've made with OpenAI and OCI, as well as the OCP show with Meta.\n\n**Lisa Su** (Chair and CEO)\nI think overall, from our perspective, I think things are going really well in both the development as well as the customer engagement there. So in terms of rack scale solutions, we would expect that the early customers for MI450 will really be around the rack scale solutions. We will have other form factors as well for the MI450 series, but there's a lot of interest in the full rack scale solution.\n\n**Thomas O'Malley** (Director of Equity Research)\nSuper helpful. Then as my follow-up, it's a broader question as well and similar to kind of what Vivek asked. If you look at the power requirements that are out there for some of the early announcements into next year, they're pretty substantial. Then you also have component issues that you're seeing across interconnected memory. Just from your perspective as an industry leader, where do you think that the constraint will be?\n\n**Thomas O'Malley** (Director of Equity Research)\nWill it come first with components not being available, or do you think that both data center footprint in terms of infrastructure and/or power is the gating factor to some of these deployments into next year, just as we really see some larger numbers start to get deployed? Thank you.\n\n**Lisa Su** (Chair and CEO)\nYeah, sure, Tom. I think what you're pointing out is what we as an industry have to do together. The entire ecosystem has to plan together, and that is exactly what we're doing. We're working with our customers on their power plans over the next, actually, I would say, two years. From a silicon and a memory and a packaging and a component supply chain, we're working with our supply chain partners to make sure all of that capacity is available.\n\n**Lisa Su** (Chair and CEO)\nI can tell you from our visibility, we feel very good that we have a strong supply chain that is prepared to deliver sort of these very significant growth rates and large amounts of compute that is out there. I think all of this is going to be tight. There is a—you can see from some of the CapEx spending that there's a desire to put on more compute, and we're working closely together. I will say that the ecosystem is very—I would say works very hard when there are these types of, let's call it, tightness out there. We also see things open up as that we're working, getting more power, getting more supply, all of those things. The net-net is I think we are well positioned to grow significantly as we transition into the second half of 2026, into 2027 with the MI450 and Helios.\n\n**Operator**\nThe next question comes from the line of Joshua Buchalter with TD Cowen. Please proceed with your question.\n\n**Joshua Buchalter** (Director and Senior Analyst of Semiconductors Equity Research)\nHey, guys. Thank you for taking my question. Actually, I wanted to start on the CPU side. You and your largest competitor in that space have talked about near-term strength supporting AI workloads on general-purpose servers from Agentic. Maybe you could speak to the sustainability of these trends. They called out supply constraints. Are you seeing any of those in your supply chain? And are we in a period where we should think about the CPU business on the data center side as being aseasonal, or should we expect normal seasonality in the first half of next year? Thank you.\n\n**Lisa Su** (Chair and CEO)\nSure, Josh. So a couple of comments on the CPU server side. I think we've been watching this trend for the last couple of quarters.\n\n**Lisa Su** (Chair and CEO)\nAnd we started seeing, let's call it, some positive signs in CPU demand, actually, a couple of quarters ago. And what's happened as we've gone through 2025 is now we see sort of a broadening of that CPU demand. So we have a number of our large hyperscale clients are now forecasting significant CPU builds into 2026. And so from that standpoint, I think it's a positive demand environment. And it is because AI is requiring quite a bit of general-purpose compute, and that's great. It catches our cycle as we're ramping Turin. So the Turin ramp has gone extremely fast, and we see good pull for that product, as well as consistent, strong demand for our general product line as well. So back to seasonality as we go into 2026, I think we expect that the CPU demand environment into 2026 is going to be, let's call it, positive.\n\n**Lisa Su** (Chair and CEO)\nAnd so we'll guide more as we get into the end of the year, but I would expect a positive demand environment for CPUs as we see this demand. I do feel like it's durable. It is not a short-term thing. I think it is a multi-quarter phenomenon as we're seeing just much more demand as these AI workloads really turn into have to do real work.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nSo, Josh, on the supply side, we have supplies to support our growth, and especially in 2026, we're prepared for the ramp.\n\n**Joshua Buchalter** (Director and Senior Analyst of Semiconductors Equity Research)\nGot it. Thank you both. And for my follow-up, Lisa, in your prepared remarks, you highlighted progress you guys have made on ROCm 7. I know this has been an area of focus. And can you maybe spend a minute or two talking about where you feel you're at competitively with ROCm?\n\n**Joshua Buchalter** (Director and Senior Analyst of Semiconductors Equity Research)\nHow wide is the breadth of support you're able to offer to the developer community? And what areas do you still have work to do to close any potential competitive gap? Thank you.\n\n**Lisa Su** (Chair and CEO)\nYeah, Josh, thanks for the question. Look, we've made great progress with ROCm. ROCm 7 is a significant step forward in terms of performance and sort of all the frameworks that we support. It's been really, really important for us to get sort of day-zero support of all the newest models and native support for all the newest frameworks. I would say most customers who are starting with AMD now have a very smooth experience as they're bringing on their workloads to AMD. There's obviously always more work to do.\n\n**Lisa Su** (Chair and CEO)\nWe're continuing to augment the libraries and the overall environment that we have, especially as we go to some of the newer workloads where you see training and inference really coming together with reinforcement learning. But overall, I think very strong progress with ROCm. And by the way, we're going to continue to invest in this area because it's so important to really make our customer development experience as smooth as we can.\n\n**Operator**\nAnd the next question comes from the line of C J Muse with Tanner Fitzgerald. Please proceed with your question.\n\n**CJ Muse** (Senior Managing Director)\nYeah, good afternoon. Thank you for taking the question. I guess first question, as you think about the 355 to 400 transition and moving to full rack scale, is there a framework that we should be thinking about for gross margins throughout calendar 2026?\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nYes, C J, thanks for the question.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nI think in general, as we said in the past, for our data center GPU business, the gross margin continues to improve when we ramp a new generation of product. Typically, at the beginning of the ramp, you go through a transition period, then you will normalize the gross margin. We're not guiding 2026, but our priority in data center GPU business is to really expand the top-line revenue growth and the gross margin dollars. And of course, at the same time, we'll continue to drive a gross margin percentage up too.\n\n**CJ Muse** (Senior Managing Director)\nVery helpful. And I guess maybe, Lisa, to kind of probe kind of your growth expectations through 2026 and beyond, and you talked about tens of billions of dollars in 2027.\n\n**CJ Muse** (Senior Managing Director)\nCan you kind of speak at a high level how you're thinking about OpenAI and other large customers and how we should be thinking about the breadth of your customer kind of penetration throughout calendar 2026, 2027? Any help on that would be super. Thank you.\n\n**Lisa Su** (Chair and CEO)\nSure, C J. And we'll certainly address this topic in more detail at our analyst day next week. But let me give you some maybe higher-level points. Look, I think we're really excited about our roadmap. I think we have seen great traction amongst the largest customers. The OpenAI relationship is extremely important to us. And it's great to be able to talk at the multi-gigawatt scale because I think that really is what we believe we can deliver to the marketplace. But there are numerous other customers that we are in deep engagements with. We talked about OCI.\n\n**Lisa Su** (Chair and CEO)\nWe also announced a couple of systems with the Department of Energy that are significant systems. And we have many other engagements. So the way you should think about it is there are multiple customers that we would expect to have, let's call it, very significant scale in the MI450 generation. And that's sort of the breadth of the customer engagements that we've built. And it's also how we're dimensioning the supply chain to ensure that we can supply certainly our. OpenAI partnership as well as the numerous other partnerships that are well underway.\n\n**Operator**\nAnd the next question comes from the line of Stacy Rasgon with Bernstein Research. Please proceed with your question.\n\n**Stacy Rasgon** (Managing Director and Senior Analyst of U.S. Semiconductors and Semiconductor Capital Equipment)\nHi, guys. Thanks for taking my questions. My first one, for data center in the quarter, what grew more year over year. On a dollar-to-percentage basis, the servers or the GPUs? In data center.\n\n**Lisa Su** (Chair and CEO)\nYeah, Stacy, I think our commentary was data center grew nicely year over year in both of the areas, both for servers as well as data center AI.\n\n**Stacy Rasgon** (Managing Director and Senior Analyst of U.S. Semiconductors and Semiconductor Capital Equipment)\nYeah, but could you—I mean, just directionally, which one grew more than the other? I'm not even asking for numbers just directionally.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nDirectionally, they are similar, but servers a little bit better. Servers a little bit better.\n\n**Stacy Rasgon** (Managing Director and Senior Analyst of U.S. Semiconductors and Semiconductor Capital Equipment)\nOkay. And then on the guidance. You said that servers—I mean, data center overall up double digits. You said servers up strong double digits. What does that mean? Is that more than 20%? Or how do I think about what you mean by strong double digits? Because again, I'm trying to—I mean, for the GPUs for the year, do you think you're—I mean, you were saying roughly like $6.5 billion or something last quarter for the year. Do you think it's still in that range? It kind of feels like they're still there.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nStacy, here's what we guided. We guided sequentially data center will be up double digits. And we said server will go up strongly. And at the same time, we also said that MI350 also going to ramp. So we did not—I don't think what you just mentioned was what we guided.\n\n**Stacy Rasgon** (Managing Director and Senior Analyst of U.S. Semiconductors and Semiconductor Capital Equipment)\nOh, okay. So I mean, if you say servers are up strongly, does that mean they're up more than the Instinct? Because you didn't really make that commentary on Instinct.\n\n**Lisa Su** (Chair and CEO)\nNo, look, Stacy, let me say it. So data center up sequentially double-digit percentage, both server and data center AI are going to be up as well. And from the standpoint of where they are, I think we're pleased with how both of them are performing. The strong double-digit percentage comment perhaps was applying to the year-over-year commentary.\n\n**Operator**\nThank you. And the next question comes from the line of Timothy Arcuri with UBS. Please proceed with your question.\n\n**Timothy Arcuri** (Managing Director)\nThanks a lot. Lisa, I know it's only been a month since you announced this idea with OpenAI, but can you give us maybe some anecdotes of how this has influenced your position in the market with other customers? Are you engaged with customers that you wouldn't have been engaged with if you hadn't done this deal? That's the first part of the question. And then the second part relates to a prior question, which is that it looks like they could be something like half of your data center GPU revenue in the 2027, 2028 timeframe. So how much risk in your mind is there around that single customer for you?\n\n**Lisa Su** (Chair and CEO)\nSure, Tim. So let me say a couple of things.\n\n**Lisa Su** (Chair and CEO)\nFirst of all, the OpenAI deal has been in the works for quite some time. We're happy to be able to talk about it broadly and also talk about the scale of the deployment and the scale of the engagement being multi-year, multi-gigawatt. I think all those things were very positive. We've had a number of other engagements as well. I think over the last—if you were asked to ask specifically over the last month—I would say that. It's been a number of factors. I think the OpenAI deal was one of them. I think. Being able to show the Helios rack in full force at OpenCompute was also a very important milestone because people could see the engineering and sort of the capabilities of the Helios rack. And if you're asking whether we've seen an increase of interest or an acceleration of interest, I think the answer is yes.\n\n**Lisa Su** (Chair and CEO)\nI think customers are broadly engaged and perhaps broadly engaged at a higher scale, which is a good thing. And then from the standpoint of customer concentration, I think a very key foundation for us in this business is to have a broad set of customers. We've always been engaged with a number of customers. I think we're dimensioning the supply chain in such a way that we would have ample supply to have multiple customers at similar scale as we go into the 2027, 2028 timeframe. And that's certainly the goal.\n\n**Operator**\nThank you. And the next question comes from the line of Aaron Rakers with Wells Fargo. Please proceed with your question.\n\n**Aaron Rakers** (Managing Director and Technology Analyst)\nYeah, thanks for taking the question.\n\n**Aaron Rakers** (Managing Director and Technology Analyst)\nI'm curious on the server strength that you're seeing, if there's a way to unpack how we think about unit growth versus ASP expansion as we move through the Turin product cycle. And how do you guys just kind of think about that going forward?\n\n**Lisa Su** (Chair and CEO)\nYeah. So, Aaron, on the server CPU side, Turin certainly is more content. So we see ASPs grow as Turin ramps. But I also mentioned in the prepared remarks that we're actually seeing a very good mix of Genoa still there. So Turin is ramping very quickly, but we are also seeing Genoa demand continue well as the hyperscalers are not able to move everything to the latest generation immediately. So from our standpoint, I think it's broad-based CPU demand across a number of different workloads.\n\n**Lisa Su** (Chair and CEO)\nThis is a little bit, let's call it server refresh, but it seems like from our customer conversations, the workloads are broadly due to the fact that AI workloads are spawning more traditional compute, so more buildout is necessary. I think going forward, one of the things that we see is there is more of a desire for the latest generation. And so as much as we're happy with how Turin is ramping. We're seeing actually a strong pull on Venice and a lot of early engagements in Venice, which kind of says a lot about kind of the importance of general-purpose compute at this point in time.\n\n**Aaron Rakers** (Managing Director and Technology Analyst)\nYeah, thanks. As a quick follow-up, I'm curious and not to steal maybe the discussion from next week, but Lisa, you've been very consistent, like 500 billion of total AI silicon TAM opportunity and obviously progressing above that.\n\n**Aaron Rakers** (Managing Director and Technology Analyst)\nI'm curious, as we think about these large megawatt kind of deployments, how you think about the updated views on that AI silicon TAM as we look forward.\n\n**Lisa Su** (Chair and CEO)\nWell, Aaron, as you said, not to take too much away from what we're going to talk about next week. Look. We're going to give you a full picture of how we see the market next week, but suffice it to say, from everything that we see, we see the AI compute TAM just going up. So we'll have some updated numbers for you, but the view is whereas 500 billion sounded like a lot when we first talked about it, we think there is a larger opportunity for us over the next few years, and that's pretty exciting.\n\n**Operator**\nThank you. The next question comes from Antoine Chikaban with New Street Research. Please proceed with your question.\n\n**Antoine Chkaiban** (Equity Research Analyst)\nHi, thank you so much for taking my question. So I'd like to ask about whether the developing relationship with OpenAI could be a tailoring to the development of your software stack. Can you maybe tell us about how the collaboration works in practice and whether the partnership contributed in making ROCm more robust?\n\n**Lisa Su** (Chair and CEO)\nYeah, Antoine, thanks for the question. I think the answer is yes. I think all of our large customers contribute to, let's call it a, broadening and deepening of our software stack overall. I think the relationship with OpenAI is certainly one where our plans are to work deeply together on hardware as well as software as well as systems and future roadmap. And from that standpoint, the work that we're doing together with them on Triton is certainly very valuable.\n\n**Lisa Su** (Chair and CEO)\nBut I will say beyond OpenAI, the work that we do with all of our largest customers are super helpful to strengthen the software stack. And we have put significant new resources into not just the largest customers, but we are working with a broad set of AI native companies who are actively developing on the ROCm stack. We get lots of feedback. I think we've made significant progress in the training and inference stack, and we're going to continue to double down and triple down in this area. So more customers that use AMD, I think all of that goes to enhancing the ROCm stack. And we'll talk a little bit more about this next week, but we're also using AI to. Help us accelerate the rate and pace of some of the ROCm kernel development and just the overall ecosystem.\n\n**Antoine Chkaiban** (Equity Research Analyst)\nThanks, Lisa.\n\n**Antoine Chkaiban** (Equity Research Analyst)\nAnd maybe as a quick follow-up, could you tell us about the useful lives of GPUs? I know that most CSPs depreciate them over five, six years, but in your conversations with them, I'm just wondering if you see or hear any early indication that in practice they may be planning to sweat those GPUs for longer than that.\n\n**Lisa Su** (Chair and CEO)\nI think we have seen some early indications of that, Antoine. I think the key point being. Clearly, there's a desire to get on the latest and greatest GPUs when you're building new data center infrastructure. And certainly when we're looking at MI355s, they're often going into. New liquid-cooled facilities, MI450 series as well. But then we're also seeing the other trend, which is there's just a need for more AI compute. And from that standpoint, some of the older generations, MI300X is still doing quite well in terms of just where we see people deploying and using, especially for inference. And from that standpoint. I think you see a little bit of both.\n\n**Operator**\nAnd the next question comes from the line of Joe Moore with Morgan Stanley. Please proceed with your question.\n\n**Joe Moore** (Managing Director and Semiconductor Industry Analyst)\nGreat. Thank you. You mentioned MI308. I guess what's your posture there to the extent that if there is some relief that you're able to ship, do you have readiness to do that? Can you give us a sense for how much of a swing factor that could be?\n\n**Lisa Su** (Chair and CEO)\nSure, Joe. So look, it's still a pretty dynamic situation with MI308. So that's the reason that we did not include any MI308 revenue in the Q4 guide. We have received some licenses for MI308, so we're appreciative of the administration supporting some licenses for MI308.\n\n**Lisa Su** (Chair and CEO)\nWe're still working with our customers on the demand environment and sort of what the overall opportunity is. And so we'll be able to update that more in the next couple of months.\n\n**Joe Moore** (Managing Director and Semiconductor Industry Analyst)\nOkay. But you do have. Product to support that market. If it does open up, or are you going to have to start to kind of rebuild inventory for that?\n\n**Lisa Su** (Chair and CEO)\nWe've had some work in process. I think we continue to have that. Work in process, but we'll have to see sort of how the demand environment shapes up.\n\n**Joe Moore** (Managing Director and Semiconductor Industry Analyst)\nOkay. Thank you very much.\n\n**Lisa Su** (Chair and CEO)\nThanks.\n\n**Matt Ramsay** (VP of Financial Strategy and Investor Relations)\nOperator, I think we might have time for just one more caller, please. Thank you very much.\n\n**Operator**\nNo problem. And the final question comes from the line of Ross Seymore with Deutsche Bank. Please proceed with your question.\n\n**Ross Seymore** (Managing Director)\nThanks for squeezing me in.\n\n**Ross Seymore** (Managing Director)\nLisa, this might take longer than the amount of time we have left before the top of the hour, but there's been so many of these multi-gigawatt announcements from OpenAI. How does AMD truly differentiate in there? When you see that big customer signing deals with other GPU vendors and ASIC vendors, etc., how do you attack that market differently than those competitors to not only get the 6 gigawatt initially, but hopefully more after that?\n\n**Lisa Su** (Chair and CEO)\nSure, Ross. Well, look, what I see is actually this environment where the world needs more AI compute. And from that standpoint, I think OpenAI has kind of led in the quest for more AI compute, but they're not alone. I think when you look across the large customers, there is really a demand for more AI compute as you go forward over the next couple of years.\n\n**Lisa Su** (Chair and CEO)\nI think we each have our advantages in terms of how we are positioning our products. I think MI450 series in particular, I think, is an extremely strong product, rack-scale solution. Overall, when we look at compute performance, when we look at memory performance, we think it's extremely well-positioned for both inference as well as training. I think the key here is time to market, its total cost of ownership. Its deep partnership, and thinking about not just MI450 series, but what happens after that. So we're deep in conversations on MI500 and beyond. And we certainly think we're well-positioned to not only participate, but participate in a very meaningful way across the sort of the demand environment here. And I think we have certainly learned a ton over the last couple of years with our AI roadmap.\n\n**Lisa Su** (Chair and CEO)\nWe've made significant inroads in terms of just what the largest customer needs from a workload standpoint. So I'm pretty optimistic about our ability to capture a significant piece of this market going forward.\n\n**Ross Seymore** (Managing Director)\nGreat. And I guess as my follow-up, it'll be a direct follow-on to that. You did a unique structure by granting some warrants with this deal. And I know they've asked according to a price that would be very creative and make everybody happy. Do you think that was a relatively unique agreement, or given that the world needs more processing power, that AMD is open to somewhat similar, conceptually similar creative ways to address that demand over time with other equity vehicles, etc.?\n\n**Lisa Su** (Chair and CEO)\nSure, Ross. So I would say it was a unique agreement from the standpoint that unique time in AI.\n\n**Lisa Su** (Chair and CEO)\nWhat we wanted, what we prioritized was really deep partnership and multi-year, multi-generation, significant scale. And I think we got that. We got a structure that has extremely aligned incentives. Everybody wins, right? We win, OpenAI wins, and our shareholders win. Sort of benefits from this, and all of that accrues to the overall roadmap. I think as we look forward, I think we have a lot of very interesting partnerships that are developing, whether they're with the largest AI users or you think about sovereign AI opportunities. And we look at each one of these as a unique opportunity where we're bringing sort of the whole of AMD. Both technically as well as all the rest of our capabilities to the party.\n\n**Lisa Su** (Chair and CEO)\nSo I would say OpenAI was pretty unique, but I would imagine that there are lots of other opportunities for us to bring our capabilities into the ecosystem and participate in a significant way.\n\n**Operator**\nLadies and gentlemen, that does conclude the question and answer session, and that also concludes today's teleconference. We thank you for your participation. You may disconnect your lines at this time.",
      "fetched_at": "2026-02-04T14:39:26.264Z"
    },
    {
      "ticker": "AMD",
      "title": "Yahoo Finance",
      "published_date": "2026-02-04T14:24:13.533Z",
      "fiscal_year": "2025",
      "quarter": "Q4",
      "url": "https://finance.yahoo.com/quote/AMD/earnings/AMD-Q4-2025-earnings_call-395057.html",
      "content": "**Operator**\nGreetings, and welcome to the AMD Fourth Quarter and Full Year 2025 Conference Call. At this time, all participants are in a listen-only mode. A question-and-answer session will follow the formal presentation. If anyone should require operator assistance during the conference, please press star zero on your telephone keypad. Please note that this conference is being recorded. I will now turn the conference over to Matt Ramsay, VP Financial Strategy and IR. Thank you. You may begin.\n\n**Matt Ramsay** (VP of Financial Strategy and Investor Relations)\nThank you, and welcome to AMD's Fourth Quarter and 2025 Full Year Financial Results Conference Call. By now, you should have had the opportunity to review a copy of our earnings press release and accompanying slides. If you have not had the opportunity to review these materials, they can be found on the investor relations page of amd.com. Today, we will refer primarily to non-GAAP financial measures on the call. The full non-GAAP-to-GAAP reconciliations are available in today's press release and in the slides posted on our website. Participants in today's conference call are Dr. Lisa Su, our Chair and CEO, and Jean Hu, our Executive Vice President, CFO, and Treasurer. This is a live call and will be replayed via webcast on our website.\n\n**Matt Ramsay** (VP of Financial Strategy and Investor Relations)\nBefore we begin, I would like to note that Mark Papermaster, Executive Vice President and CTO, will present at Morgan Stanley's TMT conference on Tuesday, March 3rd. Today's discussions contain forward-looking statements based on our current beliefs, assumptions, and expectations, speak only as of today, and as such involve risks and uncertainties that could cause actual results to differ materially from our current expectations. Please refer to the cautionary statement in our press release for more information on factors that could cause actual results to differ materially. With that, I will hand the call to Lisa.\n\n**Lisa Su** (Chairman and CEO)\nThank you, Matt, and good afternoon to all those listening today. 2025 was a defining year for AMD, with record revenue, net income, and free cash flow driven by broad-based demand for high-performance computing and AI products. We ended the year with significant momentum, with every part of our business performing very well. We saw demand accelerate across the data center, PC, gaming, and embedded markets, launched the broadest set of leadership products in our history, gained significant server and PC processor share, and rapidly scaled our data center AI business as Instinct and ROCm adoption increased with cloud, enterprise, and AI customers. Looking at our fourth quarter, fourth quarter revenue grew 34% year-over-year to $10.3 billion, led by record EPYC, Ryzen, and Instinct processor sales.\n\n**Lisa Su** (Chairman and CEO)\nNet income increased 42% to a record $2.5 billion, and free cash flow nearly doubled year-over-year to a record $2.1 billion. For the full year, revenue grew 34% to $34.6 billion, and we added more than $7.6 billion of data center segment and client revenue. Turning to our fourth quarter segment results, data center segment revenue increased 39% year-over-year to a record $5.4 billion, led by accelerating Instinct MI350 series GPU deployments and server share gains. In server, adoption of 5th-gen EPYC Turin CPUs accelerated in the quarter, accounting for more than half of the total server revenue. 4th-gen EPYC sales were also robust, as our prior generation CPUs continued to deliver superior performance and TCO compared to competitive offerings across a wide range of workloads.\n\n**Lisa Su** (Chairman and CEO)\nAs a result, we had record server CPU sales to both cloud and enterprise customers in the quarter and exited the year with record share. In cloud, hyperscaler demand was very strong as North American customers expanded deployments. EPYC-powered public cloud offerings grew significantly in the quarter, with AWS, Google, and others launching more than 230 new AMD instances. Hyperscalers launched more than 500 AMD-based instances in 2025, increasing the number of EPYC cloud instances more than 50% year over year to nearly 1,600. In the enterprise, we are seeing a meaningful shift in EPYC adoption driven by our leadership performance, expanded platform availability, broad software enablement, and increased go-to-market programs. The leading server providers now offer more than 3,000 solutions powered by fourth- and fifth-gen EPYC CPUs that are optimized for all major enterprise workloads.\n\n**Lisa Su** (Chairman and CEO)\nAs a result, the number of large businesses deploying EPYC on-prem more than doubled in 2025, and we exited the year with record server sell-through. Looking ahead, server CPU demand remains very strong. Hyperscalers are expanding their infrastructure to meet growing demand for cloud services and AI, while enterprises are modernizing their data centers to ensure they have the right compute required to enable new AI workflows. Against this backdrop, EPYC has become the processor of choice for the modern data center, delivering leadership performance, efficiency, and TCO. Our next-generation Venice CPU extends our leadership across each of these metrics. Customer pull for Venice is very high, with engagements underway to support large-scale cloud deployments and broad OEM platform availability when Venice launches later this year.\n\n**Lisa Su** (Chairman and CEO)\nTurning to our data center AI business, we delivered record Instinct GPU revenue in the fourth quarter, led by the ramp of MI350 series shipments. We also had some revenue from MI308 sales to customers in China. Instinct adoption broadened in the quarter. Today, 8 of the top 10 AI companies use Instinct to power production workloads across a growing range of use cases. With the MI350 series, we are entering the next phase of Instinct adoption, expanding our footprint with existing partners and adding new customers. In the fourth quarter, hyperscalers expanded MI350 series availability, leading AI companies scale their deployments to support additional workloads, and multiple neocloud providers launched MI350 series offerings that deliver on-demand access to Instinct infrastructure in the cloud.\n\n**Lisa Su** (Chairman and CEO)\nTurning to our AI software stack, we expanded the ROCm ecosystem in the fourth quarter, enabling customers to deploy Instinct faster and with higher performance across a broader range of workloads. Millions of large language and multimodal models run out of the box on AMD, with the leading models launching with day zero support for Instinct GPUs. This capability highlights our rapidly expanding open-source community enablement, including new upstream integration of AMD GPUs in vLLM, one of the most widely used inference engines. To drive Instinct adoption with industry-specific use cases, we're also adding support for domain-specific models in key verticals. As one example, in healthcare, we added ROCm support for the leading medical imaging framework to enable developers to train and deploy highly performant deep learning models on Instinct GPUs.\n\n**Lisa Su** (Chairman and CEO)\nFor large businesses, we introduced our Enterprise AI Suite, a full-stack software platform with enterprise-grade tools, inference microservices, and solutions blueprints designed to simplify and accelerate production deployments at scale. We also announced a strategic partnership with Tata Consultancy Services to co-develop industry-specific AI solutions and help customers deploy AI across their operations. Looking ahead, customer engagements for our next-gen MI400 series and Helios platform continue expanding. In addition to our multi-generation partnership with OpenAI to deploy 6 GW of Instinct GPUs, we are in active discussions with other customers on at-scale multi-year deployments starting with Helios and MI450 later this year. With the MI400 series, we are also expanding our portfolio to address the full range of cloud, HPC, and enterprise AI workloads.\n\n**Lisa Su** (Chairman and CEO)\nThis includes MI455X and Helios for AI superclusters, MI430X for HPC and sovereign AI, and MI440X servers for enterprise customers requiring leadership training and inference performance in a compact 8-GPU solution that integrates easily into existing infrastructure. Multiple OEMs publicly announced plans to launch Helios systems in 2026, with deep engineering engagement underway to support smooth production ramps. In December, HPE announced that they will offer Helios racks with purpose-built HPE Juniper Ethernet switches and optimized software for high-bandwidth scale-up networking. And in January, Lenovo announced plans to offer Helios racks. MI430X adoption also grew in the quarter, with new Exascale-class supercomputers announced by GENCI in France and HLRS in Germany. Looking further ahead, development of our next-generation MI500 series is well underway. MI500 is powered by our CDNA six architecture, built on advanced 2-nanometer process technology, and features high-speed HBM4e memory.\n\n**Lisa Su** (Chairman and CEO)\nWe are on track to launch MI500 in 2027 and expect MI500 to deliver another major leap in AI performance to power the next wave of large-scale multimodal models. In summary, our AI business is accelerating, with the launch of MI400 series and Helios representing a major inflection point for the business as we deliver leadership performance and TCO at the chip, compute tray, and rack level. Based on the strength of our EPYC and Instinct roadmaps, we are well positioned to grow data center segment revenue by more than 60% annually over the next three to five years and scale our AI business to tens of billions in annual revenue in 2027. Turning to client and gaming, segment revenue increased 37% year-over-year to $3.9 billion. In client, our PC processor business performed exceptionally well.\n\n**Lisa Su** (Chairman and CEO)\nRevenue increased 34% year-over-year to a record $3.1 billion, driven by increased demand for multiple generations of Ryzen desktop and mobile CPUs. Desktop CPU sales set a record for the fourth consecutive quarter. Ryzen CPUs topped the bestseller lists at major global retailers and e-tailers throughout the holiday period, with strong demand across all price points in every region driving record desktop channel sellout. In mobile, strong demand for AMD-powered notebooks drove record Ryzen PC sell-through in the quarter. That momentum extended into commercial PCs, where Ryzen adoption accelerated as we established a new long-term growth engine for our client business. Sell-through of Ryzen CPUs for commercial notebooks and desktops grew by more than 40% year-over-year in the fourth quarter, and we closed large wins with major telecom, financial services, aerospace, automotive, energy, and technology customers.\n\n**Lisa Su** (Chairman and CEO)\nAt CES, we expanded our Ryzen portfolio with CPUs that further extend our performance leadership. Our new Ryzen AI 400 mobile processors deliver significantly faster content creation and multitasking performance than the competition. Notebooks powered by Ryzen AI 400 are already available, with the broadest lineup of AMD-based consumer and commercial AI PCs set to launch throughout the year. We also introduced our Ryzen AI Halo platform, the world's smallest AI development system, featuring our highest-end Ryzen AI Max processor with 128 GB of unified memory that can run models with up to 200 billion parameters locally. In gaming, revenue increased 50% year-over-year to $843 million. Semi-custom sales increased year-over-year and declined sequentially, as expected. For 2026, we expect semi-custom SoC annual revenue to decline by a significant double-digit percentage as we enter the seventh year of what has been a very strong console cycle.\n\n**Lisa Su** (Chairman and CEO)\nFrom a product standpoint, Valve is on track to begin shipping its AMD-powered Steam Machine early this year, and development of Microsoft's next-gen Xbox featuring an AMD semi-custom SoC is progressing well to support a launch in 2027. Gaming GPU revenue also increased year-over-year, with higher channel sellout driven by demand throughout the holiday sales period for our latest generation Radeon RX 9000 Series GPUs. We also launched FSR 4 Redstone in the quarter, our most advanced AI-powered upscaling technology, delivering higher image quality and smoother frame rates for gamers. Turning to our embedded segment, revenue increased 3% year-over-year to $950 million, led by strength with test and measurement and aerospace customers and growing adoption of our embedded x86 CPUs. Channel sell-through accelerated in the quarter as end-customer demand improved across several end markets, led by test, measurement, and emulation.\n\n**Lisa Su** (Chairman and CEO)\nDesign win momentum remains one of the clearest indicators of long-term growth for our embedded business, and we delivered another record year. We closed $17 billion in design wins in 2025, up nearly 20% year-over-year, as we've now won more than $50 billion of embedded designs since acquiring Xilinx. We also strengthened our embedded portfolio in the quarter. We began production of our Versal AI Edge Gen 2 SoCs for low-latency inference workloads and started shipping our highest-end Spartan UltraScale+ devices for cost-optimized applications. We also launched new embedded CPUs, including our EPYC 2005 Series for network security and industrial edge applications, Ryzen P100 Series for in-vehicle infotainment and industrial systems, and Ryzen X100 Series for physical AI and autonomous platforms. In summary, 2025 was an excellent year for AMD, marking the start of a new growth trajectory for the company.\n\n**Lisa Su** (Chairman and CEO)\nWe are entering a multi-year demand supercycle for high performance and AI computing that is creating significant growth opportunities across each of our businesses. AMD is well positioned to capture that growth, with highly differentiated products, a proven execution engine, deep customer partnerships, and significant operational scale. As AI reshapes the compute landscape, we have the breadth of solutions and partnerships required for end-to-end leadership, from Helios in the cloud for at-scale training and inference to an expanded Instinct portfolio for sovereign supercomputing and enterprise AI deployment. At the same time, demand for EPYC CPUs is surging as agentic and emerging AI workloads require high-performance CPUs to power head nodes and run parallel tasks alongside GPUs. At the edge and in PCs where AI adoption is just beginning, our industry-leading Ryzen and embedded processors are powering real-time on-device AI.\n\n**Lisa Su** (Chairman and CEO)\nAs a result, we expect significant top-line and bottom-line growth in 2026, led by increased adoption of EPYC and Instinct, continued client share gains, and a return to growth in our embedded segment. Looking further ahead, we see a clear path to achieve the ambitious targets we laid out at our financial analyst day last November, including growing revenue at greater than 35% CAGR over the next three to five years, significantly expanding operating margins, and generating annual EPS of more than $20 in the strategic time frame, driven by growth in all of our segments and the rapid scaling of our data center AI business. Now I'll turn the call over to Jean to provide additional color on our fourth quarter results and full-year results. Jean?\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nThank you, Lisa, and good afternoon, everyone. I'll start with a review of our financial results and then provide our current outlook for the first quarter of fiscal 2026. AMD executed very well in 2025, delivering record revenue of $34.6 billion, up 34% year-over-year, driven by 32% growth in our data center segment and 51% growth in our client and gaming segment. Gross margin was 52%, and we delivered record earnings per share of $4.17, up 26% year-over-year while continuing to invest aggressively in AI and the data center to support our long-term growth. For the fourth quarter of 2025, revenue was a record $10.3 billion, growing 34% year-over-year, driven by strong growth in the data center and client gaming segments, including approximately $390 million in revenue from MI308 sales to China, which was not included in our fourth quarter guidance.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nRevenue was up 11% sequentially, primarily driven by continued strong growth in data center from both server and data center AI businesses, as well as a return to year-over-year growth in the embedded segment. Gross margin was 57%, up 290 basis points year-over-year. We benefited from the release of $360 million in previously written-down MI308 inventory reserves. Excluding the inventory reserve release and the MI308 revenue from China, gross margin would have been approximately 55%, up 80 basis points year-over-year, driven by favorable product mix. Operating expenses were $3 billion, an increase of 42% year-over-year as we continue to invest in R&D go-to-market activities to support our AI roadmap and long-term growth opportunities, as well as higher employee performance-based incentives.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nOperating income was a record $2.9 billion, representing a 28% operating margin, tax increase, and other, resulting in a net expense of approximately $335 million. For the fourth quarter, diluted earnings per share was a record $1.53, an increase of 40% year-over-year, reflecting strong execution and operating leverage in our business model. Now turning to our reportable segment. Starting with the data center segment, revenue was a record $5.4 billion, up 39% year-over-year and 24% sequentially, driven by strong demand for EPYC processors and the continued ramp of MI350 products. Data center segment operating income was $1.8 billion, or 33% of revenue, compared to $1.2 billion, or 30% a year ago, reflecting higher revenue and the inventory reserve release, partially offset by continued investment to support our AI hardware and software roadmaps.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nClient gaming segment revenue was $3.9 billion, up 37% year-over-year, driven primarily by strong demand for our leadership AMD Ryzen processors. On a sequential basis, revenue was down 3% due to lower semi-customer revenue. The client business revenue was a record $3.1 billion, up 34% year-over-year and 13% sequentially, led by strong demand from both the channel and the PC OEMs and continued market share gains. The gaming business revenue was $843 million, up 50% year-over-year, primarily driven by higher semi-customer revenue and strong demand for AMD Radeon GPUs. Sequentially, gaming revenue was down 35% due to lower semi-customer sales. Client gaming segment operating income was $725 million, or 18% of revenue, compared to $496 million, or 17% a year ago. Embedded segment revenue was $950 million, up 3% year-over-year and 11% sequentially as demand strengthened across several end markets.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nEmbedded segment operating income was $357 million, or 38% of revenue, compared to $362 million, or 39% a year ago. Before I reveal the balance sheet and the cash flow, as a reminder, we closed the sale of ZT Systems manufacturing business to Sanmina in late October. The fourth quarter financial results of the ZT manufacturing business are reported separately in our financial statement as discontinued operations and are excluded from our non-GAAP financials. Turning to the balance sheet and the cash flow. During the quarter, we generated a record $2.3 billion in cash from continuing operations and a record of $2.1 billion in free cash flow. Inventory increased sequentially by approximately $607 million-$7.9 billion to support strong data center demand. At the end of the quarter, cash, cash equivalents, and short-term investment were $10.6 billion.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nFor the year, we repurchased 12.4 million shares and returned $1.3 billion to shareholders. We ended the year with $9.4 billion authorization remaining and our share repurchase program. Now turning to our first quarter 2026 outlook. We expect revenue to be approximately $9.8 billion, plus or minus $300 million, including approximately $100 million of MI308 sales to China. At the middle point of our guidance, revenue is expected to be up 32% year-over-year, driven by strong growth in our data center and client gaming segments and modest growth in our embedded segment. Sequentially, we expect revenue to be down approximately 5%, driven by seasonal decline in our client gaming and embedded segment, partially offset by growth in our data center segment.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nIn addition, we expect fourth quarter non-GAAP gross margin to be approximately 55%, non-GAAP operating expense to be approximately $3.05 billion, non-GAAP other net income to be approximately $35 million, non-GAAP effective tax rate to be 13%, and diluted share count is expected to be approximately 1.65 billion shares. In closing, 2025 was an outstanding year for AMD, reflecting disciplined execution across the business to deliver strong revenue growth, increase profitability, and cash generation while investing aggressively in AI and innovation to support our long-term growth strategy. Looking ahead, we are very well positioned for continued strong top-line revenue growth and earnings expansion in 2026 with a focus on driving data center AI growth, operating leverage, and delivering long-term value to shareholders. With that, I'll turn it back to Matt for the Q&A session.\n\n**Matt Ramsay** (VP of Financial Strategy and Investor Relations)\nYes. Thank you very much, Jean. Operator, please go ahead and open the Q&A session. Thank you.\n\n**Operator**\nThank you, Matt. We will now be conducting the question-and-answer session. If you would like to ask a question, please press star one on your telephone keypad. A confirmation tone will indicate that your line is in the queue. You may press star two to remove yourself from the queue. For participants using speaker equipment, it may be necessary to pick up your handset before pressing the star keys. One moment, please, while we pull for questions. And the first question comes from the line of Aaron Rakers with Wells Fargo. Please proceed with your question.\n\n**Aaron Rakers** (Managing Director and Senior Equity Research Analyst)\nYeah. Thanks for taking the question. Lisa, at your analyst day back in November, you seemed to kind of endorse the high $20 billion AI revenue expectation that was out there on the street for 2027. I know today you're reaffirming the path to strong double-digit growth. So I guess my question is, can you talk a little bit about what you've seen as far as customer engagements, how those might have expanded? I think you've alluded to in the past multiple multi-gigawatt opportunities. Just double-click on what you've seen for the MI455 and Helios platform from a demand-shaping perspective as we look into the back half of the year.\n\n**Lisa Su** (Chairman and CEO)\nYeah. Sure, Aaron. Thanks for the question. So first of all, I think the MI450 series development is going extremely well. So we're very happy with the progress that we have. We're right on track for a second-half launch and beginning of production. And as it relates to sort of the shape of the ramp and the customer engagements, I would say the customer engagements continue to proceed very well. We have, obviously, a very strong relationship with OpenAI, and we're planning that ramp starting in the second half of the year, going into 2027. That is on track. We're also working closely with a number of other customers who are very interested in ramping MI450 quickly, just given the strength of the product. And we see that across both inference and training. And that is the opportunity that we see in front of us.\n\n**Lisa Su** (Chairman and CEO)\nSo we feel very good about sort of the data center growth overall for us in 2026. And then certainly, going into 2027, we've talked about $ tens of billions of data center AI revenue, and we feel very good about that.\n\n**Operator**\nThank you. The next question comes from the line of Tim Arcuri with UBS. Please proceed with your question.\n\n**Tim Arcuri** (Managing Director and Senior Equity Analyst)\nThanks a lot. Jean, I'm wondering if you can maybe give us a little bit of detail under the hood for the March guidance. I know you basically told us that you told us about what embedded's going to be up a bit year over year. Client sounds like it's down seasonally, which I take to be maybe down 10. So can you give us a sense maybe of the other pieces? And then also, can you give us a sense of how data center GPU is going to ramp through the year? I know it's a back half loaded year, but I think people are thinking, Lisa, somewhere in the $14 billion range this year. That's what investors were thinking. I'm not asking you to endorse that.\n\n**Tim Arcuri** (Managing Director and Senior Equity Analyst)\nIf you can give us a little flavor for sort of how the ramp will look through the year, that'd be great. Thanks.\n\n**Lisa Su** (Chairman and CEO)\nHi, Tim. Thanks for your question. We're guiding one quarter at a time, but I can give you some color about our Q1 guide. First, it's right sequentially, we guided the decline around 5%, but data center is actually going to be up. And when you think about it, it's right. Our CPU business seasonal, actually, in a regular seasonal pattern, it's going to be down high single digit. And in our current guide, we actually guide CPU revenue up sequentially very nicely. Also, with the data center GPU side, we also feel really good about GPU revenue, including China, will be also up. So very nice guide for the data center overall. On the client side, we do see seasonality, sequentially decline. Embedded and gaming, they also have a seasonal decline.\n\n**Lisa Su** (Chairman and CEO)\nAnd maybe, Tim, if I just give you a little bit on the full-year commentary. I think the important thing, as we look at the full year, we're very bullish on the year. We're not, if you look at the key themes, we're seeing very strong growth in the data center. And that's across two growth vectors. We see server CPU growth, actually, very strong. I mean, we've talked about the fact that CPUs are very important as AI continues to ramp. And we've seen the CPU order book continue to strengthen as we go through the last few quarters and especially over the last 60 days. So we see that as a strong growth driver for us. As Jean said, we see server CPU growing from Q4 into Q1 in what normally is seasonally down. And that continues throughout the year.\n\n**Lisa Su** (Chairman and CEO)\nThen on the data center AI side, it's a very important year for us. It's really an inflection point. MI355 has done well, and we were pleased with the performance in Q4. We continue to ramp that in the first half of the year. As we get into the second half of the year, the MI450 is really an inflection point for us. That revenue will start in the third quarter, but it will ramp significant volume in the fourth quarter as we get into 2027. That gives you a little bit of sort of what the data center ramp looks like throughout the year.\n\n**Tim Arcuri** (Managing Director and Senior Equity Analyst)\nThank you, Lisa.\n\n**Operator**\nThe next question comes from the line of Vivek Arya with Bank of America. Please proceed.\n\n**Vivek Arya** (Managing Director and Senior Equity Research Analyst)\nThank you. First, just clarification on what you're assuming for your China MI308 sales beyond Q1. And then, Lisa, specific to 2026, can your data center revenues grow at your target 60%+ growth rate? I realize that that's a multi-year target, but do you think that there are enough drivers, whether it's on the server CPU side or GPU side, for you to grow at that target base even in 2026? Thank you.\n\n**Lisa Su** (Chairman and CEO)\nYeah. Sure, Vivek. So let me talk a little bit about China first because that's, I think, important for us to make sure that's clear. Look, we were pleased to have some MI308 sales in the fourth quarter. They were actually a license that was approved through work with the administration. And those orders were actually from very early in 2025. And so we saw some revenue in Q4, and we're forecasting for about $100 million of revenue in Q1. We are not forecasting any additional revenue from China just because it's a very dynamic situation. So given that it's a dynamic situation, we're still waiting. We've submitted licenses for the MI325, and we're continuing to work with customers and understanding sort of their customer demand. We thought it prudent not to forecast any additional revenue other than the $100 million that we called out in the Q1 guide.\n\n**Lisa Su** (Chairman and CEO)\nNow, as it relates to overall data center, as I mentioned in the question to Tim, we're very bullish about data center. I think the combination of drivers that we have across our CPU franchise, I mean, the EPYC product line, both Turin and Genoa, continue to ramp well. And in the second half of the year, we will be launching Venice, which we believe actually extends our leadership. And the MI450 ramp, which is also very significant in the second half of 2026. We're not, obviously, guiding specifically by segment, but the long-term target of, let's call it, greater than 60% is certainly possible in 2026.\n\n**Vivek Arya** (Managing Director and Senior Equity Research Analyst)\nThank you, Lisa.\n\n**Operator**\nThank you. And as a reminder, if you would like to ask a question, please press star one. We ask that you limit yourself to one question and one follow-up. Thank you. The next question comes from the line of CJ Muse with Cantor. Please proceed.\n\n**C.J. Muse** (Senior Managing Director and Senior Equity Research Analyst)\nYeah. Good afternoon. Thanks for taking the question. I'm curious on the server CPU side of the house. And given the dramatic tightness, curious your ability to source incremental capacity from TSMC and elsewhere. And I guess how long will it take for that to see wafers out? And how should we think about the implications for kind of the growth trajectory throughout all of calendar 2026? And I guess as part of that, if you could speak to how we should be thinking about inflection in pricing as well, that would be very helpful.\n\n**Lisa Su** (Chairman and CEO)\nSure, CJ. So a couple of points about the server CPU market. First of all, we think the overall server CPU TAM is going to grow, let's call it, strong double digits in 2026, just given, as we said, the relationship between CPU demand and overall AI ramp. So I think that's a positive. Relative to our ability to support that, we've been seeing this trend for the last couple of quarters. So we have increased our supply capacity capability for server CPUs. And that's one of the reasons we're able to increase our Q1 guide as it relates to the server business. And we see the ability to continue to grow that throughout the year. There's no question that demand continues to be strong. And so we're working with our supply chain partners to increase supply as well.\n\n**Lisa Su** (Chairman and CEO)\nFrom what we see today, I think the overall server situation is strong. We are increasing supply to address that.\n\n**Operator**\nHey, C.J. Do you have a follow-up question?\n\n**C.J. Muse** (Senior Managing Director and Senior Equity Research Analyst)\nI do. Maybe for Jean, if you could kind of touch on gross margins through the year. And as you balance kind of strengthening server CPU with perhaps greater GPU accelerating in the second half, is there kind of a framework that we should be working off of? Thanks so much.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nYeah. Thank you for the question. We are very pleased with our gross margin Q4 performance and the Q1 guide at 55%, which actually is 130 basis points up year-over-year while we continue to ramp our MI355 year-over-year very significantly. I think we are benefiting from a very favorable product mix across all our business. If you think about it in data center, we're ramping our new product, new generation product, Turin, and the MI355, which helps the gross margin in client. We continue to move up the stack and also gaining momentum in our commercial business. Our client business gross margin has been improving nicely. In addition, certainly, we see the recovery of our embedded business, which is also margin accretive. So all those tailwinds we are seeing, we continue to see in the next few quarters.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nWhen MI450 ramp, of course, in Q4, our gross margin will be driven largely by mix. I think we'll give you more color when we get there. Overall, we feel really good about our gross margin progression this year.\n\n**Operator**\nThank you. The next question comes from the line of Joe Moore with Morgan Stanley. Please proceed.\n\n**Joe Moore** (Managing Director and Head of U.S. Semiconductors Research)\nGreat. Thank you. On the MI455 ramp, will 100% of the business be racks? Will there be kind of an eight-way server business around that architecture? And then is the revenue recognition when you ship to the rack vendor, or is there something to understand about that? Thank you.\n\n**Lisa Su** (Chairman and CEO)\nYes, Joe. So we do have multiple variants of the MI450 series, including an eight-way GPU form factor. But for 2026, I would say the vast majority of it is going to be rack-scale solutions. And yes, we will take revenue when we ship to the rack builder.\n\n**Joe Moore** (Managing Director and Head of U.S. Semiconductors Research)\nOkay. Great. And then can you talk to any risks that you may have in terms of once you get silicon out, turning that into racks, any potential issues as you ramp that? I know your competitor had some last year, and you said you learned from that. Is there anything you've done with kind of pre-building racks to sort of ensure you won't have those issues? Just any risk do we need to understand around that?\n\n**Lisa Su** (Chairman and CEO)\nYeah. I mean, Joe, the main thing is the development's going really well. We're right on track with the MI450 series as well as the Helios rack development. We've done a lot of testing already, both at the rack-scale level as well as at the silicon level. So far, so good. We are getting, let's call it, a lot of input from our customers on things to test so that we can do a lot of testing in parallel. And our expectation is that we will be on track for our second-half launch.\n\n**Operator**\nThank you. Our next question comes from the line of Stacy Rasgon with Bernstein Research. Please proceed.\n\n**Stacy Rasgon** (Managing Director and Senior Analyst)\nHi, guys. Thanks for taking my questions. First one, Lisa, I just wanted to ask about OpEx. Every quarter, you guys are guiding it up, and then it's coming in even higher, and then you're guiding it up again. And I understand, given the growth trajectory, that you need to invest. But how should we think about the ramp of that OpEx and that spending number, especially as the GPU revenue starts to inflect? Do we get leverage on that, or should we be expecting the OpEx to be growing even more materially as the AI revenue starts to ramp?\n\n**Lisa Su** (Chairman and CEO)\nYeah. Sure, Stacy. Thanks for the question. Look, I think in terms of OpEx, we're at a point where we have very high conviction in the roadmap that we have. And so in 2025, as the revenue increased, we did lean in on OpEx. And I think it was for all the right reasons. As we get into 2026 and as we see some of the significant growth that we're expecting, we should absolutely see leverage. And the way to think about it is we've always said in our long-term model that OpEx should grow slower than revenue. And we would expect that in 2026 as well, especially as we get into the second half of the year and we see inflection in the revenue.\n\n**Lisa Su** (Chairman and CEO)\nBut at this point, I think if you look at our free cash flow generation and the overall revenue growth, I think the investment in OpEx is absolutely the right thing to do.\n\n**Stacy Rasgon** (Managing Director and Senior Analyst)\nThank you. For my follow-up, I actually have two sort of one-line answers I'm looking for. Just first, the $100 million in China revenue in Q1, does that also drop through a zero-cost basis like we had in Q4, and is that a margin headwind? And number two, I know you don't give us the AI number, but could you just give us the annual 2025 Instinct number now that we're through the year? How big was it?\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nStacy, let me answer your first question on the $100 million revenue in Q1. Actually, the inventory reserve reversed in Q4, which was $360 million, not only is associated with Q4 revenue, China revenue, but also covers the $100 million revenue we expect to ship in Q1 to China with our MI308. The Q1 gross margin guide is a very clean guide.\n\n**Lisa Su** (Chairman and CEO)\nStacy, for your second question, as you know, we don't guide at the business level. But to help you with your models, I think you can if you look at the Q4 data center AI number, even if you were to back out the China number, which was, let's call it, not a recurring number, you would still see growth. You'll see growth from Q3 to Q4. So that should help you a little bit with your modeling.\n\n**Operator**\nThank you. The next question comes from the line of Joshua Buchalter with TD Cowen. Please proceed.\n\n**Joshua Buchalter** (Director and Senior Equity Research Analyst)\nHey, guys. Thanks for taking my question. I wanted to ask about clients. So the segment beat pretty handily in the fourth quarter. And I recognize you guys have been gaining share with Ryzen. But I think given what we've been seeing in the memory market, there's a lot of concern about inflationary costs and the potential for pull-ins. Were there any changes in your order patterns during the quarter? And maybe bigger picture, how are you thinking about client growth and the health of that market into 2026?\n\n**Lisa Su** (Chairman and CEO)\nYeah. Thanks for the question, Josh. The client market has performed extremely well for us throughout 2025. Very strong growth for us, both in terms of ASP mixing up the stack as well as just unit growth. Going into 2026, we are certainly watching the development of the business. I think the PC market is an important market. Based on everything that we see today, we're probably seeing the PC TAM down a bit, just given some of the inflationary pressures of the commodities pricing, including memory. The way we are modeling the year is, let's call it, second half a bit subseasonal to first half, just given everything that we see. Even in that environment with the PC market down, we believe we can grow our PC business. Our focus areas are enterprise.\n\n**Lisa Su** (Chairman and CEO)\nThat's a place where we're making very nice progress in 2025, and we expect that into 2026, and just continuing to grow sort of at the premium, higher end of the market.\n\n**Joshua Buchalter** (Director and Senior Equity Research Analyst)\nThank you for the color there. Then I wanted to ask about the Instinct family. So we've seen your big GPU competitor make a deal with an SRAM-based spatial architecture provider. And then OpenAI has reportedly been linked to one as well. Could you speak to the competitive implications of that? You've done well in inferencing, I think, partly because of your leadership in HBM content. So I was wondering if you could maybe address the pull seemingly motivated by lower latency inference and how Instinct is positioned to service this if you're indeed seeing it as well. Thank you.\n\n**Lisa Su** (Chairman and CEO)\nYeah. I think, Josh, it's really, I think, the evolution that you might expect as the AI market matures. What we're seeing is, as inference ramps, really the tokens per dollar or the efficiency of the inference stack becomes more and more important. As you know, with our chiplet architecture, we have a lot of ability to optimize across inference, training, and even across sort of the different stages of inference as well. So I think I view this as very much as you go into the future, you'll see more workload-optimized products. And you can do that with GPUs as well as with other more ASIC-like architectures. I think we have the full compute stack to do all of those things.\n\n**Lisa Su** (Chairman and CEO)\nFrom that standpoint, we're going to continue to lean into inference as we view that as a significant opportunity for us in addition to ramping our training capabilities.\n\n**Operator**\nThank you. The next question comes from the line of Ben Reitzes with Melius Research. Please proceed.\n\n**Ben Reitzes** (Managing Director and Head of Technology Research)\nYeah. Hey. Thanks. Appreciate it. Hey, Lisa, I wanted to ask you about OpenAI. I'm sure a lot of the volatility out there is not lost on you. Is everything on track for the second half for starting the 6 GW and the three and a half year timeline as far as you know? And is there any other color that you'd just like to give on that relationship? And then I have a follow-up. Thank you.\n\n**Lisa Su** (Chairman and CEO)\nYeah. I mean, Ben, what I would say is we're very much working in partnership with OpenAI as well as our CSP partners to deliver on MI450 series and deliver on the ramp. The ramp is on schedule to start in the second half of the year. MI450 is doing great. Helios is doing well. We are in, let's call it, deep co-development across all of those parties. And as we look forward, I think we are optimistic about the MI450 ramp for OpenAI. But I also want to remind everyone that we have a broad set of customers that are very excited about MI450 series. And so in addition to the work that we're doing with OpenAI, there are a number of customers that we're working to ramp in that timeframe as well.\n\n**Ben Reitzes** (Managing Director and Head of Technology Research)\nAll right. I appreciate that. I wanted to shift to the server CPU and just talk about x86 versus ARM. There's some view out there that x86 has particular edge in agents, big picture. Do you agree with that? And what are you seeing from customers? And in particular, obviously, your big competitor is going to be selling an ARM CPU separately now in the second half. So if there's just anything on that competitive dynamic versus ARM and what NVIDIA is doing and your views on that, that'd be great to hear. Thanks.\n\n**Lisa Su** (Chairman and CEO)\nYeah. Ben, what I would say about the CPU market is there is a great need for high-performance CPUs right now. And that goes towards agentic workloads where when you have these AI processes or AI agents that are spinning off a lot of work in an enterprise, they're actually going to a lot of traditional CPU tasks. And the vast majority of them are on x86 today. I think the beauty of EPYC is that we've optimized. We've done workload optimization. So we have the best cloud processor out there. We have the best enterprise processor. We also have some lower-cost variants for storage and other elements. And I think all of that comes into play as we think about the entirety of the AI infrastructure that needs to be put in place.\n\n**Lisa Su** (Chairman and CEO)\nI think the CPUs are going to continue to be as important as a piece of the AI infrastructure ramp. That's one of the things that we mentioned at our Analyst Day back in November, really this multi-year CPU cycle. We continue to see that. I think we've optimized EPYC to satisfy all of those workloads. We're going to continue to work with our customers to expand our EPYC footprint.\n\n**Operator**\nThe next question comes from the line of Tom O'Malley with Barclays. Please proceed.\n\n**Tom O'Malley** (Director and Senior Equity Research Analyst)\nHey, Lisa. How are you? I just wanted to ask, you mentioned on memory earlier as a sticking point in terms of inflationary cost. Different customers do this in different ways. Different suppliers do this in different ways. But could you maybe talk about your procurement of memory, when that takes place, particularly on the HBM side? Is that something that gets done a year in advance, six months in advance? Different accelerator guys have talked about different timelines. We'd be curious to kind of hear when you do the procurement.\n\n**Lisa Su** (Chairman and CEO)\nYeah. I mean, given the lead times for things like HBM and wafers and these parts of the supply chain, I mean, we're working closely with our suppliers over a multi-year timeframe in terms of what we see in demand, how we ramp, how we ensure that our development is very closely tied together. So I feel very good about our supply chain capabilities. We have been planning for this ramp. So independent of the current market conditions, we've been planning for a significant ramp in both CPU as well as our GPU business over the past couple of years. And so from that standpoint, I think we're well-positioned to grow substantially in 2026. And now we're also doing multi-year agreements that extend beyond that given tightness of the supply chain.\n\n**Tom O'Malley** (Director and Senior Equity Research Analyst)\nThanks. And just as a follow-up, you've seen a variety of different things in the industry in terms of system accelerator, so KV cache offload, more discrete ASIC-style compute, CPX. If you look at what your competitors are doing and you look at your first generation of system architecture coming out, maybe spend some time on, do you see yourself following in the footsteps of some of these different types of architectural changes? Do you think that you'll go in a different direction? Anything just on the evolution of your system-based architecture and then the adjoining products and/or silicon within. Thank you.\n\n**Lisa Su** (Chairman and CEO)\nI think, Tom, what we have is the ability with a very flexible architecture with our chiplet architecture. And then we also have a flexible platform architecture that allows us to really have different system solutions for the different requirements. I think we're very cognizant that there will be different solutions. So there's no—I've often said there's no one-size-fits-all. And I'll say that again. There's no one-size-fits-all. But that being the case, it's clear that the rack scale architecture is very, very good for the highest-end applications when you're talking about distributed inference and training. But we also see an opportunity with enterprise AI to use some of these other form factors. And so we're investing across that spectrum.\n\n**Operator**\nThe next question comes from the line of Ross Seymore with Deutsche Bank. Please proceed.\n\n**Ross Seymore** (Managing Director and Senior Equity Research Analyst)\nHi. Thanks for my last couple of questions. I guess my first question is back on the gross margin side of things. As you go from the MI300 to the 400 to the 500 eventually, do you see any changes in the gross margin throughout that period? In the past, you've talked about optimizing dollars more so than percentages. But just on the percentage side, does it go up, down? Is there volatility as you go from one to the next for any reason? Just wondered on the trajectory there.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nRoss, thank you for the question. At a very high level, each generation, we actually provide much more capabilities, more memory, help our customer more. So in general, the gross margin should progress each generation when you offer more capabilities to your customers. But typically, when you first ramp at the beginning of ramp of a generation, it tends to be lower. When you get to the scale, get to the yield improvement, the test improvement, and also overall performance improvement, you will see gross margin improving within each generation. So it's kind of a dynamic gross margin. But in the longer term, you should expect each generation should have a higher gross margin.\n\n**Ross Seymore** (Managing Director and Senior Equity Research Analyst)\nThanks for that, Jean. Then one small segment of your business, but it seems quite volatile. You talked a little bit about further off than you usually do, is the gaming side of things. What is the magnitude down you're talking about this year? Because in 2025, you thought it was going to be flat. And it ended up growing 50%, which was a nice positive surprise. But now that you're talking about this year being down, but then the next-gen Xbox ramping in 2027, I just hope to get some color on what you see as kind of the annual trajectory there.\n\n**Jean Hu** (EVP, CFO, and Treasurer)\nYeah. So Lisa can add more. So 2026, actually, it's the seventh year of a current product cycle. Typically, when you're at this stage of the cycle, revenue tends to come down. We do expect the revenue on the semi-customer revenue side to come down significantly, double-digit for 2026, as Lisa mentioned in her prepared remarks. For the next generation?\n\n**Lisa Su** (Chairman and CEO)\nYeah. Yeah. I think we'll certainly talk about that going forward. But as we ramp the new generation, you would expect a reversal of that.\n\n**Matt Ramsay** (VP of Financial Strategy and Investor Relations)\nOperator, I think we have time for one more caller on the call, please. Thank you.\n\n**Operator**\nOur final question comes from the line of Jim Schneider with Goldman Sachs. Please proceed.\n\n**Jim Schneider** (Senior Equity Analyst)\nGood afternoon. Thanks for taking my question. Relative to the ramp of your rack-scale systems, would you expect any kind of bottleneck in terms of supply constraints in terms of the ramp as you ramp the second half of the year to potentially impact or limit the revenue growth? In other words, maybe talk about whether you expect supply to really kind of meet the growth in Q4 sequentially relative to sorry, Q3 relative to Q4.\n\n**Lisa Su** (Chairman and CEO)\nYeah. Jim, we are planning this at every component level. So I think relative to our data center AI ramp, I do not believe that we will be supply-limited in terms of the ramp that we put in place. I think we have an aggressive ramp. I think it's a very doable ramp. And as we think about the size and scale of AMD, clearly, our priority is ensuring that the data center ramps go very well. And that's both on the data center AI, the GPU side, as well as on the CPU side.\n\n**Jim Schneider** (Senior Equity Analyst)\nThank you. And then maybe as a follow-up to the earlier question on OPEX, could you maybe address what are some of the largest investment areas you made in 2025? And then what are the largest incremental OPEX investment areas for 2026? Thank you.\n\n**Lisa Su** (Chairman and CEO)\nYeah, Jim. On the 2025 investment, the priority and the largest investment in data center AI, our hardware roadmap, we accelerated that roadmap. We expand our software capabilities. We also acquired ZT Systems, which add significant system-level solutions and capabilities. Those are the primary investment in 2025. We also invest heavily in go-to-market to really expand our go-to-market capabilities to support revenue growth and also expand our commercial business and enterprise business for our CPU franchise. In 2026, you should expect us to continue to invest aggressively. But as Lisa mentioned earlier, we do expect revenue to expand faster than operating expense increase to drive the earnings per share expansion.\n\n**Jim Schneider** (Senior Equity Analyst)\nAll right. Thank you, everybody, for participating on the call. Operator, I think we can go ahead and close the call now. Thank you. Good evening.\n\n**Operator**\nThank you. Ladies and gentlemen, that does conclude the question-and-answer session. That also concludes today's teleconference. You may disconnect your lines at this time and have a great rest of the day.",
      "fetched_at": "2026-02-04T14:24:13.548Z"
    }
  ]
}