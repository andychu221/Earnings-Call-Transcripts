{
  "ticker": "NVDA",
  "last_updated": "2026-02-04T15:38:33.993Z",
  "total_transcripts": 5,
  "transcripts": [
    {
      "ticker": "NVDA",
      "title": "NVIDIA Corporation (NVDA) Q3 FY2026 earnings call transcript",
      "published_date": "Nov 19, 2025, 5:00 PM EST",
      "fiscal_year": "2026",
      "quarter": "Q3",
      "url": "https://finance.yahoo.com/quote/NVDA/earnings/NVDA-Q3-2026-earnings_call-379484.html",
      "content": "**Operator**\nGood afternoon. My name is Sarah, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's third quarter earnings call. All lines have been placed on mute to prevent any background noise. After the speaker's remarks, there will be a question-and-answer session. If you would like to ask a question during this time, simply press star, followed by the number one on your telephone keypad. If you would like to withdraw your question, press star one again. Thank you. Toshiya Hari, you may begin your conference.\n\n**Toshiya Hari** (VP of Investor Relations)\nThank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2026. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer, and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter of fiscal 2026. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially.\n\n**Toshiya Hari** (VP of Investor Relations)\nFor a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, November 19, 2025, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\n\n**Colette Kress** (EVP and CFO)\nThank you, Toshiya. We delivered another outstanding quarter with revenue of $57 billion, up 62% year over year, and a record sequential revenue growth of $10 billion, or 22%. Our customers continue to lean into three platform shifts, fueling exponential growth for accelerated computing, powerful AI models, and agentic applications. Yet, we are still in the early innings of these transitions that will impact our work across every industry. We currently have visibility to $500 billion in Blackwell and Rubin revenue from the start of this year through the end of calendar year 2026. By executing our annual product cadence and extending our performance leadership through full-stack design, we believe NVIDIA will be the superior choice for the $3 trillion-$4 trillion in annual AI infrastructure build we estimate by the end of the decade. Demand for AI infrastructure continues to exceed our expectations.\n\n**Colette Kress** (EVP and CFO)\nThe clouds are sold out, and our GPU-installed base, both new and previous generations, including Blackwell, Hopper, and Ampere, is fully utilized. Record Q3 data center revenue of $51 billion increased 66% year over year, a significant feat at our scale. Compute grew 56% year over year, driven primarily by the GB300 ramp, while networking more than doubled given the onset of NVLink scale-up and robust double-digit growth across Spectrum X Ethernet and Quantum X InfiniBand. The world hyperscalers, a trillion-dollar industry, are transforming search, recommendations, and content understanding from classical machine learning to generative AI. NVIDIA CUDA excels at both and is the ideal platform for this transition, driving infrastructure investment measured in hundreds of billions of dollars. At Meta, AI recommendation systems are delivering higher quality and more relevant content, leading to more time spent on apps such as Facebook and Threads.\n\n**Colette Kress** (EVP and CFO)\nAnalyst expectations for the top CSPs and hyperscalers in 2026 aggregate CapEx have continued to increase and now sit roughly at $600 billion, more than $200 billion higher relative to the start of the year. We see the transition to accelerated computing and generative AI across current hyperscale workloads contributing toward roughly half of our long-term opportunity. Another growth pillar is the ongoing increase in compute spend driven by foundation model builders such as Anthropic, Mistral, OpenAI, Reflection, Safe Superintelligence, Thinking Machines Lab, and xAI, all scaling compute aggressively to scale intelligence. The three scaling laws, pre-training, post-training, and inference remain intact. In fact, we see a positive virtuous cycle emerging whereby the three scaling laws and access to compute are generating better intelligence and, in turn, increasing adoption and profits.\n\n**Colette Kress** (EVP and CFO)\nOpenAI recently shared that their weekly user base has grown to $800 million, enterprise customers have increased to 1 million, and that their gross margins were healthy. While Anthropic recently reported that its annualized run rate revenue has reached $7 billion as of last month, up from $1 billion at the start of the year. We are also witnessing a proliferation of agentic AI across various industries and tasks. Companies such as Cursor, Anthropic, Open Evidence, Epic, and Abridge are experiencing a surge in user growth as they supercharge the existing workforce, delivering unquestionable ROI for coders and healthcare professionals. The world's most important enterprise software platforms like ServiceNow, CrowdStrike, and SAP are integrating NVIDIA's accelerated computing and AI stack. Our new partner, Palantir, is supercharging the incredibly popular ontology platform with NVIDIA CUDA X libraries and AI models for the first time.\n\n**Colette Kress** (EVP and CFO)\nPreviously, like most enterprise software platforms, Ontology runs only on CPUs. Lowe's is leveraging the platform to build supply chain agility, reducing costs and improving customer satisfaction. Enterprises broadly are leveraging AI to boost productivity, increase efficiency, and reduce costs. RBC is leveraging agentic AI to drive significant analyst productivity, slashing report generation time from hours to minutes. AI and digital twins are helping Unilever accelerate content creation by 2x and cut costs by 50%. Salesforce's engineering team has seen at least a 30% productivity increase in new code development after adopting Cursor. This past quarter, we announced AI factory and infrastructure projects amounting to an aggregate of 5 million GPUs. This demand spans every market: CSPs, sovereigns, model builders, enterprises, and supercomputing centers, and includes multiple landmark buildouts.\n\n**Colette Kress** (EVP and CFO)\nxAI's Colossus 2, the world's first gigawatt-scale data center, Lilly's AI factory for drug discovery, the pharmaceutical industry's most powerful data center. Just today, AWS and Humane expanded their partnership, including the deployment of up to 150,000 AI accelerators, including our GB300. xAI and Humane also announced a partnership in which the two will jointly develop a network of world-class GPU data centers anchored by the flagship 500-megawatt facility. Blackwell gained further momentum in Q3 as GB300 crossed over GB200 and contributed roughly two-thirds of the total Blackwell revenue. The transition to GB300 has been seamless, with production shipments to the major cloud service providers, hyperscalers, and GPU clouds, and is already driving their growth. The Hopper platform, in its 13th quarter since inception, recorded approximately $2 billion in revenue in Q3. H20 sales were approximately $50 million.\n\n**Colette Kress** (EVP and CFO)\nSizable purchase orders never materialized in the quarter due to geopolitical issues and the increasingly competitive market in China. While we were disappointed in the current state that prevents us from shipping more competitive data center compute products to China, we are committed to continued engagement with the U.S. and China governments and will continue to advocate for America's ability to compete around the world. To establish a sustainable leadership position in AI computing, America must win the support of every developer and be the platform of choice for every commercial business, including those in China. The Rubin platform is on track to ramp in the second half of 2026. Powered by seven chips, the Vera Rubin platform will once again deliver an X-factor improvement in performance relative to Blackwell.\n\n**Colette Kress** (EVP and CFO)\nWe have received silicon back from our supply chain partners and are happy to report that NVIDIA teams across the world are executing the bring-up beautifully. Rubin is our third-generation rack-scale system, substantially redefined the manufacturability while remaining compatible with Grace Blackwell. Our supply chain data center ecosystem and cloud partners have now mastered the build-to-installation process of NVIDIA's rack architecture. Our ecosystem will be ready for a fast Rubin ramp. Our annual X-factor performance leap increases performance per dollar while driving down computing costs for our customers. The long useful life of NVIDIA's CUDA GPUs is a significant TCO advantage over accelerators. CUDA's compatibility and our massive installed base extend the life of NVIDIA systems well beyond their original estimated useful life. For more than two decades, we have optimized the CUDA ecosystem, improving existing workloads, accelerating new ones, and increasing throughput with every software release.\n\n**Colette Kress** (EVP and CFO)\nMost accelerators without CUDA and NVIDIA's time-tested and versatile architecture became obsolete within a few years as model technologies evolve. Thanks to CUDA, the A100 GPUs we shipped six years ago are still running at full utilization today, powered by vastly improved software stack. We have evolved over the past 25 years from a gaming GPU company to now an AI data center infrastructure company. Our ability to innovate across the CPU, the GPU, networking, and software, and ultimately drive down cost per token, is unmatched across the industry. Our networking business, purpose-built for AI and now the largest in the world, generated revenue of $8.2 billion, up 162% year over year, with NVLink, InfiniBand, and Spectrum X Ethernet all contributing to growth. We are winning in data center networking as the majority of AI deployments now include our switches with Ethernet GPU attach rates roughly on par with InfiniBand.\n\n**Colette Kress** (EVP and CFO)\nMeta, Microsoft, Oracle, and xAI are building gigawatt AI factories with Spectrum X Ethernet switches, and each will run its operating system of choice, highlighting the flexibility and openness of our platform. We recently introduced Spectrum XGS, a scale-across technology that enables gigascale AI factories. NVIDIA is the only company with AI scale-up, scale-out, and scale-across platforms, reinforcing our unique position in the market as the AI infrastructure provider. Customer interest in NVLink Fusion continues to grow. We announced a strategic collaboration with Fujitsu in October, where we will integrate Fujitsu's CPUs and NVIDIA GPUs via NVLink Fusion, connecting our large ecosystems. We also announced a collaboration with Intel to develop multiple generations of custom data center and PC products, connecting NVIDIA and Intel's ecosystems using NVLink.\n\n**Colette Kress** (EVP and CFO)\nThis week at Supercomputing 25, Arm announced that it will be integrating NVLink IP for customers to build CPU SoCs that connect with NVIDIA. Currently on its fifth generation, NVLink is the only proven scale-up technology available on the market today. In the latest MLPerf training results, Blackwell Ultra delivered 5x faster time to train than Hopper. NVIDIA swept every benchmark. Notably, NVIDIA is the only training platform to leverage bridge FP4 while meeting MLPerf's strict accuracy standards. In semi-analysis inference max benchmark, Blackwell achieved the highest performance and lowest total cost of ownership across every model and use case. Particularly important is Blackwell's NVLink's performance on a mixture of experts, the architecture for the world's most popular reasoning models. On DeepSeek R1, Blackwell delivered 10x higher performance per watt and 10x lower cost per token versus H200, a huge generational leap fueled by our extreme code design approach.\n\n**Colette Kress** (EVP and CFO)\nNVIDIA Dynamo, an open-source, low-latency modular inference framework, has now been adopted by every major cloud service provider. Leveraging Dynamo's enablement and disaggregated inference, the resulting increase in performance of complex AI models such as MOE models, AWS, Google Cloud, Microsoft Azure, and OCI have boosted AI inference performance for enterprise cloud customers. We are working on a strategic partnership with OpenAI focused on helping them build and deploy at least 10 gigawatts of AI data centers. In addition, we have the opportunity to invest in the company. We serve OpenAI through their cloud partners, Microsoft Azure, OCI, and CoreWeave. We will continue to do so for the foreseeable future. As they continue to scale, we are delighted to support the company to add self-build infrastructure, and we are working toward a definitive agreement and are excited to support OpenAI's growth. Yesterday, we celebrated an announcement with Anthropic.\n\n**Colette Kress** (EVP and CFO)\nFor the first time, Anthropic is adopting NVIDIA, and we are establishing a deep technology partnership to support Anthropic's fast growth. We will collaborate to optimize Anthropic models for CUDA and deliver the best possible performance, efficiency, and TCO. We will also optimize future NVIDIA architectures for Anthropic workloads. Anthropic's compute commitment is initially including up to 1 gigawatt of compute capacity with Grace Blackwell and Vera Rubin systems. Our strategic investments in Anthropic, Mistral, OpenAI, Reflection, Thinking Machines, and others represent partnerships that grow the NVIDIA CUDA AI ecosystem and enable every model to run optimally on NVIDIA's everywhere. We will continue to invest strategically while preserving our disciplined approach to cash flow management. Physical AI is already a multi-billion dollar business addressing a multi-trillion dollar opportunity and the next leg of growth for NVIDIA. Leading U.S.\n\n**Colette Kress** (EVP and CFO)\nManufacturers and robotics innovators are leveraging NVIDIA's three-computer architecture to train on NVIDIA, test on Omniverse computer, and deploy real-world AI on Jetson robotic computers. PTC and Siemens introduced new services that bring Omniverse-powered digital twin workflows to their extensive installed base of customers. Companies including Belden, Caterpillar, Foxconn, Lucid Motors, Toyota, TSMC, and Wistron are building Omniverse digital twin factories to accelerate AI-driven manufacturing and automation. Agility Robotics, Amazon Robotics, Figure, and Skilled at AI are building our platform, tapping offerings such as NVIDIA Cosmos World Foundation models for development, Omniverse for simulation and validation, and Jetson to power next-generation intelligent robots. We remain focused on building resiliency and redundancy in our global supply chain. Last month, in partnership with TSMC, we celebrated the first Blackwell wafer produced on U.S. soil.\n\n**Colette Kress** (EVP and CFO)\nWe will continue to work with Foxconn, Wistron, Amcor, Spill, and others to grow our presence in the U.S. over the next four years. Gaming revenue was $4.3 billion, up 30% year-on-year, driven by strong demand as Blackwell momentum continued. End-market sell-through remains robust, and channel inventories are at normal levels heading into the holiday season. Steam recently broke its concurrent user record with 42 million gamers, while thousands of fans packed the GeForce Gamer Festival in South Korea to celebrate 25 years of GeForce. NVIDIA Pro Visualization has evolved into computers for engineers and developers, whether for graphics or for AI. Professional visualization revenue was $760 million, up 56% year-over-year, was another record. Growth was driven by DGX Spark, the world's smallest AI supercomputer built on a small configuration of Grace Blackwell. Automotive revenue was $592 million, up 32% year-over-year, primarily driven by self-driving solutions.\n\n**Colette Kress** (EVP and CFO)\nWe are partnering with Uber to scale the world's largest Level 4 ready autonomous fleet, built on the new NVIDIA Hyperion L4 Robotaxi reference architecture. Moving to the rest of the P&L, GAAP gross margins were 73.4%, and non-GAAP gross margins were 73.6%, exceeding our outlook. Gross margins increased sequentially due to our data center mix, improved cycle time, and cost structure. GAAP operating expenses were up 8% sequentially and up 11% on a non-GAAP basis. The growth was driven by infrastructure compute as well as higher compensation and benefits in engineering development costs. Non-GAAP effective tax rate for the third quarter was just over 17%, higher than our guidance of 16.5% due to the strong U.S. revenue. On our balance sheet, inventory grew 32% quarter over quarter, while supply commitments increased 63% sequentially.\n\n**Colette Kress** (EVP and CFO)\nWe are preparing for significant growth ahead and feel good about our ability to execute against our opportunity set. Okay, let me turn to the outlook for the fourth quarter. Total revenue is expected to be $65 billion, plus or minus 2%. At the midpoint, our outlook implies 14% sequential growth driven by continued momentum in the Blackwell architecture. Consistent with last quarter, we are not assuming any data center compute revenue from China. GAAP and non-GAAP gross margins are expected to be 74.8% and 75% respectively, plus or minus 50 basis points. Looking ahead to fiscal year 2027, input costs are on the rise, but we are working to hold gross margins in the mid-70s. GAAP and non-GAAP operating expenses are expected to be approximately $6.7 billion and $5 billion respectively.\n\n**Colette Kress** (EVP and CFO)\nGAAP and non-GAAP other income and expenses are expected to be an income of approximately $500 million, excluding gains and losses from non-marketable and publicly held equity securities. GAAP and non-GAAP tax rates are expected to be 17%, plus or minus 1%, excluding any discrete items. At this time, let me turn the call over to Jensen for him to say a few words.\n\n**Jensen Huang** (President and CEO)\nThanks, Colette. There has been a lot of talk about an AI bubble. From our vantage point, we see something very different. As a reminder, NVIDIA is unlike any other accelerator. We excel at every phase of AI, from pre-training and post-training to inference. With our two-decade investment in CUDA X acceleration libraries, we are also exceptional at science and engineering simulations, computer graphics, structured data processing to classical machine learning.\n\n**Jensen Huang** (President and CEO)\nThe world is undergoing three massive platform shifts at once, the first time since the dawn of Moore's Law. NVIDIA is uniquely addressing each of the three transformations. The first transition is from CPU general-purpose computing to GPU accelerated computing as Moore's Law slows. The world has a massive investment in non-AI software, from data processing to science and engineering simulations, representing hundreds of billions of dollars in compute cloud computing spend each year. Many of these applications, which ran once exclusively on CPUs, are now rapidly shifting to CUDA GPUs. Accelerated computing has reached a tipping point. Secondly, AI has also reached a tipping point and is transforming existing applications while enabling entirely new ones. For existing applications, generative AI is replacing classical machine learning in search ranking, recommender systems, ad targeting, click-through prediction to content moderation, the very foundations of hyperscale infrastructure.\n\n**Jensen Huang** (President and CEO)\nMeta's Gem, a foundation model for ad recommendations trained on large-scale GPU clusters, exemplifies this shift. In Q2, Meta reported over a 5% increase in ad conversions on Instagram and 3% gain on Facebook feed, driven by generative AI-based Gem. Transitioning to generative AI represents substantial revenue gains for hyperscalers. Now, a new wave is rising: agentic AI systems capable of reasoning, planning, and using tools. From coding assistants like Cursor and Claude Code to radiology tools like iDoc, legal assistants like Harvey, and AI chauffeurs like Tesla FSD and Waymo, these systems mark the next frontier of computing. The fastest-growing companies in the world today—OpenAI, Anthropic, xAI, Google, Cursor, Lovable, Replet, Cognition AI, Open Evidence, Abridge, Tesla—are pioneering agentic AI. There are three massive platform shifts. The transition to accelerated computing is foundational and necessary, essential in a post-Moore's Law era.\n\n**Jensen Huang** (President and CEO)\nThe transition to generative AI is transformational and necessary, supercharging existing applications and business models. The transition to agentic and physical AI will be revolutionary, giving rise to new applications, companies, products, and services. As you consider infrastructure investments, consider these three fundamental dynamics. Each will contribute to infrastructure growth in the coming years. NVIDIA is chosen because our singular architecture enables all three transitions, and thus so for any form and modality of AI across all industries, across every phase of AI, across all of the diverse computing needs in a cloud, and also from cloud to enterprise to robots. One architecture. Toshiya, back to you.\n\n**Toshiya Hari** (VP of Investor Relations)\nWe will now open the call for questions.\n\n**Operator**\nOperator, would you please pull for questions? Thank you. At this time, I would like to remind everyone in order to ask a question, press star, then the number one on your telephone keypad.\n\n**Operator**\nWe'll pause for just a moment to compile the Q&A roster. As a reminder, please limit yourself to one question. Thank you. Your first question comes from Joseph Moore with Morgan Stanley. Your line is open.\n\n**Joseph Moore** (Semiconductor Industry Analyst)\nGreat. Thank you. I wonder if you could update us. You talked about the $500 billion of revenue for Blackwell plus Rubin in 2025 and 2026 at GTC. At that time, you had talked about $150 billion of that already having been shipped. As the quarter's wrapped up, are those still kind of the general parameters that there's $350 billion in the next kind of 14 months or so? I would assume over that time, you haven't seen all the demand, but there is any possibility of upside to those numbers as we move forward.\n\n**Colette Kress** (EVP and CFO)\nYeah. Thanks, Joe. I'll start first with a response here on that. Yes, that's correct.\n\n**Colette Kress** (EVP and CFO)\nWe are working into our $500 billion forecast, and we are on track for that as we have finished some of the quarters. We have several quarters now in front of us to take us through the end of calendar year 2026. The number will grow, and we will achieve, I'm sure, additional needs for compute that will be shippable by fiscal year 2026. We shipped $50 billion this quarter, but we would be not finished if we did not say that we will probably be taking more orders. For example, just even today, our announcements with KSA and that agreement in itself is 400,000-600,000 more GPUs over three years. Anthropic is also not new. There is definitely an opportunity for us to have more on top of the $500 billion that we announced.\n\n**Operator**\nThe next question comes from C.J. Muse with Cantor Fitzgerald.\n\n**Operator**\nYour line is open.\n\n**CJ Muse** (Senior Managing Director)\nYeah. Good afternoon. Thank you for taking the question. There's clearly a great deal of consternation around the magnitude of AI infrastructure buildouts and the ability to fund such plans and the ROI. Yet at the same time, you're talking about being sold out. Every stood-up GPU is taken. The AI world hasn't seen the enormous benefit yet from B300, never mind Rubin. Gemini 3 just announced Grok 5 coming soon. The question is this: when you look at that as the backdrop, do you see a realistic path for supply to catch up with demand over the next 12 to 18 months, or do you think it can extend beyond that timeframe?\n\n**Jensen Huang** (President and CEO)\nAs you know, we've done a really good job planning our supply chain. NVIDIA's supply chain basically includes every technology company in the world.\n\n**Jensen Huang** (President and CEO)\nTSMC and their packaging and our memory vendors and memory partners and all of our system ODMs have done a really good job planning with us. We were planning for a big year. We've seen for some time the three transitions that I spoke about just a second ago: accelerated computing from general-purpose computing. It's really important to recognize that AI is not just agentic AI, but generative AI is transforming the way that hyperscalers did the work that they used to do on CPUs. Generative AI made it possible for them to move search and recommender systems and add recommendations and targeting. All of that has been moved to generative AI and is still transitioning.\n\n**Jensen Huang** (President and CEO)\nWhether you installed NVIDIA GPUs for data processing, or you did it for generative AI for your recommender system, or you're building it for agentic chatbots and the type of AIs that most people see when they think about AI, all of those applications are accelerated by NVIDIA. When you look at the totality of the spend, it's really important to think about each one of those layers. They're all growing. They're related, but not the same. The wonderful thing is that they all run on NVIDIA GPUs. Simultaneously, because the quality of the AI models are improving so incredibly, the adoption of it in the different use cases, whether it's in code assistance, which NVIDIA uses fairly exhaustively, and we're not the only one.\n\n**Jensen Huang** (President and CEO)\nI mean, the fastest-growing application in history, a combination of Cursor and Claude Code and OpenAI's Codex and GitHub Copilot, these applications are the fastest-growing in history. It's not just used for software engineers. It's used because of vibe coding. It's used by engineers and marketeers all over companies, supply chain planners all over companies. I think that that's just one example, and the list goes on, whether it's open evidence and the work that they do in healthcare or the work that's being done in digital video editing, runway. I mean, the number of really, really exciting startups that are taking advantage of generative AI and agentic AI is growing quite rapidly. Not to mention, we're all using it a lot more.\n\n**Jensen Huang** (President and CEO)\nAll of these exponentials, not to mention, just today, I was reading a text from Demis, and he was saying that pre-training and post-training are fully intact. Gemini 3 takes advantage of the scaling laws and got a received a huge jump in quality performance and model performance. We are seeing all of these exponentials kind of running at the same time. I just always go back to first principles and think about what's happening from each one of the dynamics that I mentioned before: general-purpose computing to accelerated computing, generative AI replacing classical machine learning, and of course, agentic AI, which is a brand new category.\n\n**Operator**\nThe next question comes from Vivek Aria with Bank of America Securities. Your line is open.\n\n**Vivek Arya** (Managing Director and Senior Analyst)\nThanks for taking my question. I'm curious, what assumptions are you making on NVIDIA content per gigawatt in that $500 billion number?\n\n**Vivek Arya** (Managing Director and Senior Analyst)\nBecause we have heard numbers as low as $25 billion per gigawatt of content to as high as $30 or $40 billion per gigawatt. I am curious what power and what dollar per gigawatt assumptions you are making as part of that $500 billion number. Longer-term, Jensen, the $3 to $4 trillion in data center by 2030 was mentioned. How much of that do you think will require vendor financing, and how much of that can be supported by cash flows of your large customers or governments or enterprises? Thank you.\n\n**Jensen Huang** (President and CEO)\nIn each generation, from Ampere to Hopper, from Hopper to Blackwell, Blackwell to Rubin, our part of the data center increases. Hopper generation was probably something along the lines of 20-somewhat, 20-25. Blackwell generation, Grace Blackwell particularly, is probably 30-30, say 30 plus or minus.\n\n**Jensen Huang** (President and CEO)\nRubin is probably higher than that. In each one of these generations, the speedup is X factors. Therefore, their TCO, the customer TCO, improves by X factors. The most important thing is, in the end, you still only have one gigawatt of power, one gigawatt data centers, one gigawatt of power. Therefore, performance per watt, the efficiency of your architecture, is incredibly important. The efficiency of your architecture can't be brute forced. There is no brute forcing about it. That one gigawatt translates directly, your performance per watt translates directly, absolutely directly to your revenues, which is the reason why choosing the right architecture matters so much now. The world doesn't have an excess of anything to squander.\n\n**Jensen Huang** (President and CEO)\nWe have to be really, really—we use this concept called co-design across our entire stack, across the frameworks and models, across the entire data center, even power and cooling optimized across the entire supply chain in our ecosystem. Each generation, our economic contribution will be greater. Our value delivered will be greater. The most important thing is our energy efficiency per watt is going to be extraordinary every single generation. With respect to growing into continuing to grow, our customers' financing is up to them. We see the opportunity to grow for quite some time. Remember, today, most of the focus has been on the hyperscalers. One of the areas that is really misunderstood about the hyperscalers is that the investment on NVIDIA GPUs not only improves their scale, speed, and cost from general-purpose computing—that is number one—because Moore's Law scaling has really slowed.\n\n**Jensen Huang** (President and CEO)\nMoore's Law is about driving cost down. It's about deflationary cost, the incredible deflationary cost of computing over time. That has slowed. Therefore, a new approach is necessary for them to keep driving the cost down. Going to NVIDIA GPU computing is really the best way to do so. The second is revenue boosting in their current business models. Recommender systems drive the world's hyperscalers every single, whether it's watching short-form videos or recommending books or recommending the next item in your basket to recommending ads to recommending news to—it's all about recommenders. The internet has trillions of pieces of content. How could they possibly figure out what to put in front of you and your little tiny screen unless they have really sophisticated recommender systems to do so? That has gone generative AI.\n\n**Jensen Huang** (President and CEO)\nThe first two things that I've just said, hundreds of billions of dollars of CapEx is going to have to be invested, is fully cash flow funded. What is above it, therefore, is agentic AI. This is net new, net new consumption, but it's also net new applications. And some of the applications I mentioned before, but these new applications are also the fastest-growing applications in history. Okay? I think that you're going to see that once people start to appreciate what is actually happening under the water, if you will, from the simplistic view of what's happening to CapEx investment, recognizing there's these three dynamics. Lastly, remember, we were just talking about the American CSPs. Each country will fund their own infrastructure. You have multiple countries. You have multiple industries.\n\n**Jensen Huang** (President and CEO)\nMost of the world's industries haven't really engaged agentic AI yet, and they're about to. All the names of companies that you know we're working with, whether it's autonomous vehicle companies or digital twins for physical AI for factories and the number of factories and warehouses being built around the world, just the number of digital biology startups that are being funded so that we could accelerate drug discovery. All of those different industries are now getting engaged, and they're going to do their own fundraising. Do not just look at the hyperscalers as a way to build out for the future. You got to look at the world. You got to look at all the different industries. Enterprise computing is going to fund their own industry.\n\n**Operator**\nThe next question comes from Ben Ritzes with Melius. Your line is open.\n\n**Ben Reitzes** (Managing Director and Head of Technology Research)\nHey, thanks a lot.\n\n**Ben Reitzes** (Managing Director and Head of Technology Research)\nJensen, I wanted to ask you about cash. Speaking of half a trillion, you may generate about half a trillion in free cash flow over the next couple of years. What are your plans for that cash? How much goes to buyback versus investing in the ecosystem? How do you look at investing in the ecosystem? I think there's just a lot of confusion out there about how these deals work and your criteria for doing those, like the Anthropic, the OpenAIs, etc. Thanks a lot.\n\n**Jensen Huang** (President and CEO)\nYeah, appreciate the question. Of course, using cash to fund our growth, no company has grown at the scale that we're talking about and have the connection and the depth and the breadth of supply chain that NVIDIA has.\n\n**Jensen Huang** (President and CEO)\nThe reason why our entire customer base can rely on us is because we've secured a really resilient supply chain, and we have the balance sheet to support them. When we make purchases, our suppliers can take it to the bank. When we make forecasts and we plan with them, they take us seriously because of our balance sheet. We're not making up the offtake. We know what our offtake is. Because they've been planning with us for so many years, our reputation and our credibility is incredible. It takes really strong balance sheet to do that, to support the level of growth and the rate of growth and the magnitude associated with that. That's number one. The second thing, of course, we're going to continue to do stock buybacks. We're going to continue to do that.\n\n**Jensen Huang** (President and CEO)\nWith respect to the investments, this is really, really important work that we do. All of the investments that we've done so far, well, all the period, is associated with expanding the reach of CUDA, expanding the ecosystem. If you look at the work, the investments that we did with OpenAI, it's, of course, that relationship we've had since 2016. I delivered the first AI supercomputer ever made to OpenAI. We have had a close and wonderful relationship with OpenAI since then. Everything that OpenAI does runs on NVIDIA today. All the clouds that they deploy in, whether it's training and inference, runs NVIDIA, and we love working with them. The partnership that we have with them is one so that we could work even deeper from a technical perspective so that we could support their accelerated growth. This is a company that's growing incredibly fast.\n\n**Jensen Huang** (President and CEO)\nDo not just look at what is said in the press. Look at all the ecosystem partners and all the developers that are connected to OpenAI. They are all driving consumption of it. The quality of the AI that is being produced is a huge step up since a year ago. The quality of response is extraordinary. We invest in OpenAI for a deep partnership in co-development to expand our ecosystem and to support their growth. Of course, rather than giving up a share of our company, we get a share of their company. We invested in them in one of the most consequential once-in-a-generation companies, once-in-a-generation company that we have a share of. I fully expect that investment to translate to extraordinary returns. Now, in the case of Anthropic, this is the first time that Anthropic will be on NVIDIA's architecture.\n\n**Jensen Huang** (President and CEO)\nThe first time Anthropic will be on NVIDIA's architecture is the second most successful AI in the world in terms of total number of users. In enterprise, they're doing incredibly well. Claude Code is doing incredibly well. Claude is doing incredibly well all over the world's enterprise. Now we have the opportunity to have a deep partnership with them and bringing Claude onto the NVIDIA platform. What do we have now? NVIDIA's architecture, taking a step back, NVIDIA's architecture, NVIDIA's platform is the singular platform in the world that runs every AI model. We run OpenAI. We run Anthropic. We run xAI because of our deep partnership with Elon and xAI. We were able to bring that opportunity to Saudi Arabia, to the KSA, so that Humane could also be hosting opportunity for xAI. We run xAI. We run Gemini. We run Thinking Machines.\n\n**Jensen Huang** (President and CEO)\nLet's see, what else do we run? We run them all. Not to mention, we run the science models, the biology models, DNA models, gene models, chemical models, and all the different fields around the world. It's not just cognitive AI that the world uses. AI is impacting every single industry. We have the ability, through the ecosystem investments that we make, to partner with, deeply partner on a technical basis with some of the best companies, most brilliant companies in the world. We are expanding the reach of our ecosystem, and we're getting a share and investment in what will be a very successful company, oftentimes once-in-a-generation company. That's our investment thesis.\n\n**Operator**\nThe next question comes from Jim Schneider with Goldman Sachs. Your line is open.\n\n**Jim Schneider** (Senior Equity Analyst)\nGood afternoon. Thanks for taking my question.\n\n**Jim Schneider** (Senior Equity Analyst)\nIn the past, you've talked about roughly 40% of your shipments tied to AI inference. I'm wondering, as you look forward into next year, where do you expect that percentage could go in, say, a year's time? Can you maybe address the Rubin CPX product you expect to introduce next year and contextualize that? How big of the overall TAM you expect that can take and maybe talk about some of the target customer applications for that specific product? Thank you.\n\n**Jensen Huang** (President and CEO)\nCPX is designed for long-context type of workload generation. Long-context, basically, before you start generating answers, you have to read a lot, basically long-context. It could be a bunch of PDFs. It could be watching a bunch of videos, studying 3D images, so on and so forth. You have to absorb the context. CPX is designed for long-context type of workloads.\n\n**Jensen Huang** (President and CEO)\nIt's perf per dollars. Its perf per dollar is excellent. Its perf per watt is excellent. Which made me forget the first part of the question. Inferencing. Oh, inference. Yeah. There are three scaling laws that are scaling at the same time. The first scaling law called pretraining continues to be very effective. And the second is post-training. Post-training basically has found incredible algorithms for improving an AI's ability to break a problem down and solve a problem step by step. And post-training is scaling exponentially. Basically, the more compute you apply to a model, the smarter it is, the more intelligent it is. And then the third is inference. Inference, because of chain of thought, because of reasoning capabilities, AIs are essentially reading, thinking before it answers. The amount of computation necessary as a result of those three things has gone completely exponential.\n\n**Jensen Huang** (President and CEO)\nI think that it's hard to know exactly what the percentage will be at any given point in time and who. Of course, our hope is that inference is a very large part of the market. If inference is large, then what it suggests is that people are using it in more applications, and they're using it more frequently. We should all hope for inference to be very large. This is where Grace Blackwell is just an order of magnitude better, more advanced than anything in the world. The second best platform is H200. It's very clear now that GB300, GB200, and GB300, because of NVLink 72, the scale-up network that we have achieved, and you saw and Colette talked about in the semi-analysis benchmark, it's the largest single inference benchmark ever done.\n\n**Jensen Huang** (President and CEO)\nGB200, NVLink 72, is 10 times, 10-15 times higher performance. That is a big step up. It is going to take a long time before somebody is able to take that on. Our leadership there is surely multi-year. Yeah. I think I am hoping that inference becomes a very big deal. Our leadership in inference is extraordinary. The next question comes from Timothy Arcury with UBS. Your line is open. Thanks a lot. Jensen, many of your customers are pursuing behind-the-meter power. What is the single biggest bottleneck that worries you that could constrain your growth? Is it power, or maybe it is financing, or maybe it is something else like memory or even foundry? Thanks a lot. These are all issues, and they are all constraints.\n\n**Jensen Huang** (President and CEO)\nThe reason for that, when you're growing at the rate that we are and the scale that we are, how could anything be easy? What NVIDIA is doing, obviously, has never been done before. We have created a whole new industry. On the one hand, we are transitioning computing from general-purpose and classical or traditional computing to accelerated computing and AI. That's on the one hand. On the other hand, we created a whole new industry called AI factories. The idea that in order for software to run, you need these factories to generate it, generate every single token instead of retrieving information that was pre-created. I think this whole transition requires extraordinary scale. All the way from the supply chain, of course, the supply chain, we have much better visibility and control over it because, obviously, we're incredibly good at managing our supply chain.\n\n**Jensen Huang** (President and CEO)\nWe have great partners that we've worked with for 33 years. The supply chain part of it, we're quite confident. Now, looking down our supply chain, we've now established partnerships with so many players in land and power and shell and, of course, financing. None of these things are easy, but they're all tractable, and they're all solvable things. The most important thing that we have to do is do a good job planning. We plan up the supply chain, down the supply chain. We have established a whole lot of partners. We have a lot of routes to market. Very importantly, our architecture has to deliver the best value to the customers that we have. At this point, I'm very confident that NVIDIA's architecture is the best performance per TCO.\n\n**Jensen Huang** (President and CEO)\nIt is the best performance per watt, and therefore, for any amount of energy that is delivered, our architecture will drive the most revenues. I think the increasing rate of our success, I think that we're more successful this year at this point than we were last year at this point. The number of customers coming to us and the number of platforms coming to us after they've explored others is increasing, not decreasing. I think all of that is just all the things that I've been telling you over the years are really coming true and are becoming evident.\n\n**Operator**\nThe next question comes from Stacey Raskin with Bernstein Research. Your line is open.\n\n**Stacy Rasgon** (Senior Analyst)\nQuestions. Colette, I have some questions on margins. You said for next year, you're working to hold them in the mid-70s. I guess, first of all, what are the biggest cost increases?\n\n**Stacy Rasgon** (Senior Analyst)\nIs it just memory, or is it something else? What are you doing to work toward that? How much is cost optimizations versus pre-buys versus pricing? Also, how should we think about OpEx growth next year, given the revenues seem likely to grow materially from where we're running right now?\n\n**Colette Kress** (EVP and CFO)\nThanks, Stacey. Let me see if I can start with remembering where we were with the current fiscal year that we're in. Remember, earlier this year, we indicated that through cost improvements and mix that we would exit the year and our gross margins in the mid-70s. We've achieved that and getting ready to also execute that in Q4. Now it's time for us to communicate where are we working right now in terms of next year. Next year, there are input prices that are well-known in the industries that we need to work through.\n\n**Colette Kress** (EVP and CFO)\nOur systems are by no means very easy to work with. There are a tremendous amount of components, many different parts of it, as we think about that. We are taking all of that into account. We do believe, as we look at working again on cost improvements, cycle time, and mix, that we will work to try and hold at our gross margins in the mid-70%. That is our overall plan for gross margin. Your second question is around OpEx. Right now, our goal in terms of OpEx is to really make sure that we are innovating with our engineering teams, with all of our business teams to create more and more systems for this market. As you know, right now, we have a new architecture coming out. That means they are quite busy in order to meet that goal.\n\n**Colette Kress** (EVP and CFO)\nWe're going to continue to see our investments on innovating more and more, both our software, both our systems, and our hardware to do so. I'll leave it turned to Jensen if he wants to add any couple more comments.\n\n**Jensen Huang** (President and CEO)\nYeah. That's spot on. I think the only thing that I would add is remember that we plan, we forecast, we plan, and we negotiate with our supply chain well in advance. Our supply chain has known for quite a long time our requirements. And they've known for quite a long time our demand. We've been working with them and negotiating with them for quite a long time. I think the recent surge, obviously, quite significant. Remember, our supply chain has been working with us for a very long time.\n\n**Jensen Huang** (President and CEO)\nIn many cases, we've secured a lot of supply for ourselves because, obviously, they're working with the largest company in the world in doing so. We've also been working closely with them on the financial aspects of it and securing forecasts and plans and so on and so forth. I think all of that has worked out well for us.\n\n**Operator**\nYour final question comes from the line of Aaron Rakers with Wells Fargo. Your line is open.\n\n**Aaron Rakers** (Wall Street Analyst)\nYeah. Thanks for taking the question. Jensen, the question for you, as you think about the Anthropic deal that was announced and just the overall breadth of your customers, I'm curious if your thoughts around the role that AI ASICs or dedicated XPUs play in these architecture buildouts has changed at all. Have you seen?\n\n**Aaron Rakers** (Wall Street Analyst)\nI think you've been fairly adamant in the past that some of these programs never really see deployments. I'm curious if we're at a point where maybe that's even changed more in favor of just GPU architecture. Thank you.\n\n**Jensen Huang** (President and CEO)\nThank you very much. I really appreciate the question. First of all, you're not competing against teams. Excuse me. Again, as a company, you're competing against teams. There just aren't that many teams in the world who are extraordinary at building these incredibly complicated things. Back in the Hopper day and the Ampere days, we would build one GPU. That's the definition of an accelerated AI system. Today, we've got to build entire racks, entire three different types of switches: a scale-up, a scale-out, and a scale-across switch. It takes a lot more than one chip to build a compute node anymore.\n\n**Jensen Huang** (President and CEO)\nEverything about that computing system, because AI needs to have memory, AI didn't used to have memory at all. Now it has to remember things. The amount of memory and context it has is gigantic. The memory architecture implication is incredible. The diversity of models from a mixture of experts to dense models to diffusion models to autoregressive, not to mention biological models that obey the laws of physics. The list of different types of models has exploded in the last several years. The challenge is the complexity of the problem is much higher. The diversity of AI models is incredibly large. This is where, if I will say, the five things that make us special, if you will. The first thing I would say that makes us special is that we accelerate every phase of that transition. That's the first phase.\n\n**Jensen Huang** (President and CEO)\nThat CUDA allows us to have CUDA X for transitioning from general-purpose to accelerated computing. We are incredibly good at generative AI. We're incredibly good at agentic AI. Every single phase of that, every single layer of that transition, we are excellent at. You can invest in one architecture, use it across the board. You can use one architecture and not worry about the changes in the workload across those three phases. That's number one. Number two, we're excellent at every phase of AI. Everybody's always known that we're incredibly good at pretraining. We're obviously very good at post-training. And we're incredibly good, as it turns out, at inference because inference is really, really hard. How could thinking be easy? People think that inference is one shot, and therefore, it's easy. Anybody could approach the market that way.\n\n**Jensen Huang** (President and CEO)\nBut it turns out to be the hardest of all because thinking, as it turns out, is quite hard. We're great at every phase of AI, the second thing. The third thing is we're now the only architecture in the world that runs every AI model, every frontier AI model. We run open-source AI models incredibly well. We run science models, biology models, robotics models. We run every single model. We're the only architecture in the world that can claim that. It doesn't matter whether you're autoregressive or diffusion-based. We run everything. We run it for every major platform, as I just mentioned. We run every model. The fourth thing I would say is that we're in every cloud. The reason why developers love us is because we're literally everywhere. We're in every cloud.\n\n**Jensen Huang** (President and CEO)\nWe're in every—we could even make you a little tiny cloud called DGX Spark. We're in every computer. We're everywhere, from cloud to on-prem to robotic systems, edge devices, PCs, you name it. One architecture, things just work. It's incredible. The last thing, and this is probably the most important thing, the fifth thing, is if you are a cloud service provider, if you're a new company like Humane, if you're a new company like CoreWeave or NSCALE or Nevius, or OCI for that matter, the reason why NVIDIA is the best platform for you is because our offtake is so diverse. We can help you with offtake. It's not about just putting a random ASIC into a data center. Where's the offtake coming from? Where's the diversity coming from? Where's the resilience coming from?\n\n**Jensen Huang** (President and CEO)\nThe versatility of the architecture coming from, the diversity of capability coming from. NVIDIA has such incredibly good offtake because our ecosystem is so large. So these five things, every phase of acceleration and transition, every phase of AI, every model, every cloud to on-prem, and of course, finally, it all leads to offtake.\n\n**Operator**\nThank you. I will now turn the call to Toshiya Hari for closing remarks.\n\n**Toshiya Hari** (VP of Investor Relations)\nIn closing, please note we will be at the UBS Global Technology and AI Conference on December 2nd. And our earnings call to discuss the results of our fourth quarter of fiscal 2026 is scheduled for February 25th. Thank you for joining us today. Operator, please go ahead and close the call.\n\n**Operator**\nThank you. This concludes today's conference call. You may now disconnect.",
      "fetched_at": "2026-02-04T15:38:14.160Z"
    },
    {
      "ticker": "NVDA",
      "title": "NVIDIA Corporation (NVDA) Q2 FY2026 earnings call transcript",
      "published_date": "Aug 27, 2025, 5:00 PM EDT",
      "fiscal_year": "2026",
      "quarter": "Q2",
      "url": "https://finance.yahoo.com/quote/NVDA/earnings/NVDA-Q2-2026-earnings_call-351238.html",
      "content": "**Operator**\nGood afternoon. My name is Sarah, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's Second Quarter Fiscal twenty twenty six Financial Results Conference Call. All lines have been placed on mute to prevent any background noise. After the speakers' remarks, there will be a question and answer session. You. Toshiya Hari, you may begin your conference.\n\n**Toshiya Hari** (VP - IR &amp; Strategic Finance)\nThank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the 2026. With me today from NVIDIA are Jensen Huang, president and chief executive officer, and Colette Kress, executive vice president and chief financial officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the 2026.\n\n**Toshiya Hari** (VP - IR &amp; Strategic Finance)\nThe content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10 k and 10 Q, and the reports that we may file on Form eight ks with the Securities and Exchange Commission.\n\n**Toshiya Hari** (VP - IR &amp; Strategic Finance)\nAll our statements are made as of today, 08/27/2025, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non GAAP financial measures. You can find a reconciliation of these non GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\n\n**Colette Kress** (EVP &amp; CFO)\nThank you, Toshiya. We delivered another record quarter while navigating what continues to be a dynamic external environment. Total revenue was 46,700,000,000.0, exceeded our outlook as we grew sequentially across all market platforms. Data center revenue grew 56% year over year. Data center revenue also grew sequentially despite the 4,000,000,000 decline in h '20 revenue.\n\n**Colette Kress** (EVP &amp; CFO)\nNVIDIA's Blackwell platform reached record levels, growing sequentially by 17%. We began production shipments of GB 300 in q two. Our full stack AI solutions for cloud service providers, neo clouds, enterprises, and sovereigns are all contributing to our growth. We are at the beginning of an industrial revolution that will transform every industry. We see 3 to $4,000,000,000,000 in AI infrastructure spend in the by the end of the decade.\n\n**Colette Kress** (EVP &amp; CFO)\nThe scale and scope of these build outs present significant long term growth opportunities for NVIDIA. The g b 200 NBL system is seeing widespread adoption with deployments at CSPs and consumer Internet companies, Lighthouse model builders, including OpenAI, Meta, and Mastral are using the g b 200 NBL 72 at data center scale for both training next generation models and serving inference models in production. The new Blackwell Ultra platform has also had a strong quarter, generating tens of billions in revenue. The transition to the g b 300 has been seamless for major cloud service providers due to its shared architecture, software, and physical footprint with the g b 200, enabling them to build and deploy g b 300 racks with ease. The transition to the new g b 300 rack based architecture has been seamless.\n\n**Colette Kress** (EVP &amp; CFO)\nFactory builds in late July and early August were successfully converted to support the g b 300 ramp. And today, full production is underway. The current run rate is back at full speed, producing approximately 1,000 racks per week. This output is expected to accelerate even further throughout the third quarter as additional capacity comes online. We expect widespread market availability in the second half of the year as CoreWeave prepares to bring their g v 300 instance to market as they are already seeing 10x more inference performance on reasoning models compared to h 100.\n\n**Colette Kress** (EVP &amp; CFO)\nCompared to the previous hopper generation, g v 300 and d l 72 AI factories promise a 10 x improvement in token per watt energy efficiency, which translates to revenues as data centers are power limited. The chips of the Rubin platform are in fab. The Vera CPU, Rubin GPU, c x nine Super NIC, NVLink one forty four scale up switch, Spectrum X scale out and scale across switch, and the silicon photonics processor. Rubin remains on schedule for volume production next year. Rubin will be our third generation NVLink RackScale AI supercomputer with a mature and full scale supply chain.\n\n**Colette Kress** (EVP &amp; CFO)\nThis keeps us on track with our pace of an annual product cadence and continuous innovation across compute, networking, systems, and software. In late July, the US government began reviewing licenses for sales of h 20 to China customers. While a select number of our China based customers have received licenses over the past few weeks, we have not shipped any h 20 based on those licenses. USG officials have expressed an expectation that the USG will receive 15% of the revenue generated from licensed h 20 sales. But to date, the USG has not published a regulation codifying such requirement.\n\n**Colette Kress** (EVP &amp; CFO)\nWe have not included h 20 in our q three outlook as we continue to work through geopolitical issues. If geopolitical issues reside, we should ship $2,000,000,000 to $5,000,000,000 in h twenty revenue in q three. And if we add more orders, we can bill more. We continue to advocate for the US government to approve Blackwell for China. Our products are designed and sold for beneficial commercial use, and every license sale we make will benefit The US economy, The US leadership.\n\n**Colette Kress** (EVP &amp; CFO)\nIn highly competitive markets, we want to win the support of every developer. America's AI technology stack can be the world's standard if we race and compete globally. Notably in the quarter was an increase in hopper January and h 200 shipments. We also sold approximately 650,000,000 of h 20 in q two to an unrestricted customer outside of China. The sequential increase in Hopper demand indicates the breadth of data center workloads that run on accelerated computing and the power of CUDA libraries and full stack optimizations, which continuously enhance the performance and economic value of platform.\n\n**Colette Kress** (EVP &amp; CFO)\nAs we continue to deliver both Hopper and Blackwell GPUs, we are focusing on meeting the soaring global demand. This growth is fueled by capital expenditures from the cloud to enterprises, which are on track to invest 600,000,000,000 in data center infrastructure and compute this calendar year alone, nearly doubling in two years. We expect annual AI infrastructure investments to continue growing driven by the several factors, reasoning agentic AI requiring orders of magnitude more training and inference compute, global build outs for sovereign AI, enterprise AI adoption, and the arrival of physical AI and robotics. Blackwell has set the benchmark as it is the new standard for AI inference performance. The market for AI inference is expanding rapidly with reasoning and agentic AI gaining traction across industries.\n\n**Colette Kress** (EVP &amp; CFO)\nBlackwell's RackScale NVLink and CUDA full stack architecture addresses this by redefining the economics of inference. New NV f p four four bit precision and NVLink 72 on the g b 300 platform delivers a 50 x increase in energy efficiency per token compared to Hopper, enabling companies to monetize their compute at unprecedented scale. For instance, a 3,000,000 investment in g v 200 infrastructure can generate 30,000,000 in token revenue, a 10 x return. NVIDIA software innovation, combined with the strength of our developer ecosystem, has already improved Blackwell's performance by more than two x since its launch. Advances in CUDA, TensorRT LLM, and Dynamo are unlocking maximum efficiency.\n\n**Colette Kress** (EVP &amp; CFO)\nCUDA library contributions from the open source community along with NVIDIA's open libraries and frameworks are now integrated into millions of workflows. This plow this powerful flywheel of collaborative innovation between NVIDIA and global community contribution strengthens NVIDIA's performance leadership. NVIDIA is a top contributor to OpenAI models, data, and software. Blackwell has introduced a groundbreaking numerical approach to large language model pretraining Using NV f p four, computations on the g b 300 can now achieve seven x faster training than the h 100, which uses f p eight. This innovation delivers the accuracy of 16 bit precision with the speed and efficiency of four bit, setting a new standard for AI factor efficiency and scalability.\n\n**Colette Kress** (EVP &amp; CFO)\nThe AI industry is quickly adopting this revolutionary technology with major players such as AWS, Google Cloud, Microsoft Azure, and OpenAI, as well as Cohere, Mistral, Kimi AI, Perplexity, Reflection, and Runway, already embracing it. NVIDIA's performance leadership was further validated in the latest ML Perth training benchmarks where the g b 200 delivered a clean sweep. Be on the lookout for the upcoming m MLPerf inference results in September, which will include benchmarks based on the Blackwell Ultra. NVIDIA RTX Pro servers are in full production for the world system makers. These are air cooled PCIe based systems integrated seamlessly into standard IT environments and run traditional enterprise IT applications as well as the most advanced agentic and physical AI applications.\n\n**Colette Kress** (EVP &amp; CFO)\nNearly 90 companies, including many global leaders, are already adopting RTX Pro servers. Hitachi uses them for real time simulation and digital twins, Lilly for drug discovery, Hyundai for factory design and AV validation, and Disney for immersive storytelling. As enterprises modernize data centers, RTX Pro servers are poised to become a multibillion dollar product line. Sovereign AI is one on the rise as the nation's ability to develop its own AI using domestic infrastructure data and talent presents a significant opportunity for NVIDIA. NVIDIA is at the forefront of landmark initiatives across The UK and Europe.\n\n**Colette Kress** (EVP &amp; CFO)\nThe European Union plans to invest €20,000,000,000 to establish 20 AI factories across France, Germany, Italy, and Spain, including five gigafactories to increase its AI compute infrastructure by tenfold. In The UK, the is Umbard AI supercomputer powered by NVIDIA was unveiled at the country's most powerful AI system, delivering 21 exaflots of AI performance to accelerate breakthroughs in fields of drug discovery and climate modeling. We are on track to achieve over 20,000,000,000 in sovereign AI revenue this year, more than double than that of last year. Networking delivered record revenue of 7,300,000,000.0, and escalating demands of AI compute clusters necessitate high efficiency and low latency networking. This represents a 46% sequential and 98% year on year increase with strong demand across Spectrum X Ethernet, InfiniBand, and NVLink.\n\n**Colette Kress** (EVP &amp; CFO)\nOur Spectrum X enhanced Ethernet solutions provide the highest throughput and lowest latency network for Ethernet AI workloads. Spectrum X Ethernet delivered double digit sequential and year over year growth with annualized revenue exceeding 10,000,000,000. At Hotchips, we introduced Spectrum XGS Ethernet, a technology designed to unify disparate data centers into gigascale AI super factories. Corweave is an initial adopter of the solution, which is project projected to double GPU to GPU communication speed. InfiniBand revenue nearly doubled sequentially, fueled by the adoption of XDR technology, which provides double the bandwidth improvement over its predecessor, especially valuable for the model builders.\n\n**Colette Kress** (EVP &amp; CFO)\nThe world's fastest switch, NVLink, with 14 x the bandwidth of PCIe Gen five delivered strong growth as customers deployed Brace Blackwell NVLink Rack Scale systems. The positive reception to NVLink Fusion, which allows semi custom AI infrastructure, has been widespread. Japan's upcoming Fugaku Next will integrate Fujitsu's CPUs with our architecture via NVLink Fusion. It will run a range of workloads, including AI, supercomputing, and quantum computing. Fugaku next joins a rapidly expanding list of leading quantum supercomputing and research centers running on NVIDIA's CUDA Q quantum platform, including ULEC, AIST, NNF, and NERSC, supported by over 300 ecosystem partners, including AWS, Google Quantum AI, Quantinuum, QEra, and SciQuantum.\n\n**Colette Kress** (EVP &amp; CFO)\nJust in THOR, our new robotics computing platform is now available. THOR delivers an order of magnitude greater AI performance and energy efficiency than NVIDIA AGX Orin. It runs the latest generative and reasoning AI models at the edge in real time, enabling state of the art robotics. Adoption of NVIDIA's robotics full stack platform is growing at rapid rate. Over 2,000,000 developers and 1,000 plus hardware software applications and sensor partners taking our platform to market.\n\n**Colette Kress** (EVP &amp; CFO)\nLeading enterprises across industries have adopted Thor, including Agility Robotics, Amazon Robotics, Boston Dynamics, Caterpillar, Figure, Hexagon, Medtronic, and Meta. Robotic applications require exponentially more compute on the device and in infrastructure representing a significant long term demand driver for our data center platform. NVIDIA Omniverse with Cosmos is our data center physical AI digital twin platform built for development of robot and robotic systems. This quarter, we announced a major expansion of our partnership with Siemens to enable AI automatic factories, leading European robotics companies, including Agile Robots, Neurorobotics, and Universal Robots, are building their latest innovations with the Omniverse platform. Transitioning to a quick summary of our revenue by geography.\n\n**Colette Kress** (EVP &amp; CFO)\nChina declined on a sequential basis to low single digits percentage of data center revenue. Note, our q three outlook does not include h 20 shipments to China customers. Singapore revenue represented 22% of second quarter's billed revenue as customers have centralized their invoicing in Singapore. Over 99% of data center compute revenue billed to Singapore was for US based customers. Our gaming revenue was a record 4,300,000,000.0, a 14% sequential increase and a 49% jump year on year.\n\n**Colette Kress** (EVP &amp; CFO)\nThis was driven by the ramp of Blackwell GeForce GPUs as strong sales continued as we increased supply availability. This quarter, we shipped GeForce RTX fifty sixty desktop GPU. It brings double the performance along with advanced ray tracing, neural rendering, and AI powered DLSS four gameplay to millions of gamers worldwide. Blackwell is coming to GeForce NOW in September. This is GeForce NOW's most significant upgrade, offering RTX fifty eighty class performance, minimal latency, and five k resolution at 120 frames per second.\n\n**Colette Kress** (EVP &amp; CFO)\nWe are also doubling the GeForce NOW catalog to over 4,500 titles, the largest library of any cloud gaming service. For AI enthusiasts, on device AI performs the best RTX GPUs. We partnered with OpenAI to optimize their open source GPT models for high quality, fast, and efficient inference on millions of RTX enabled window devices. With the RTX platform stack, Window developers can create AI applications designed to run on the world's largest AI PC user base. Professional visualization revenue reached 601,000,000, a 32% year on year increase.\n\n**Colette Kress** (EVP &amp; CFO)\nGrowth was driven by an adoption of the high end RTX workstation GPUs and AI powered workload like design, simulation, and prototyping. Key customers are leveraging our solutions to accelerate their operations. Activision Blizzard uses RTX workstations to enhance creative workflows, while robotics innovator Figure AI powers its humanoid robots with RTX embedded GPUs. Automotive revenue, which includes only in car compute revenue, was 586,000,000, up 69% year on year, primarily driven by self driving solutions. We have begun shipments of NVIDIA Thor SoC, the successor to Orin.\n\n**Colette Kress** (EVP &amp; CFO)\nThor's arrival coincides with the industry's accelerating shift to vision, language, model architecture, generative AI, and higher levels of autonomy. Thor is the most successful robotics and AV computer we've ever created. Thor willpower. Our full stack drive AV software platform is now in production, opening up billions to new revenue opportunities for NVIDIA while improving vehicle safety and autonomy. Now moving to the rest of our p and l.\n\n**Colette Kress** (EVP &amp; CFO)\nGAAP gross margin was 72.4%, and non GAAP gross margin was 72.7%. These figures include a 180,000,000 or 40 basis point benefit from relief releasing previously reserved h 20 inventory. Excluding this benefit, non GAAP gross margins would have been 72.3%, still exceeding our outlook. GAAP operating expenses rose eight percent and six percent on a non GAAP basis sequentially. This increase was driven by higher compute and infrastructure costs as well as higher compensation and benefit costs.\n\n**Colette Kress** (EVP &amp; CFO)\nTo support the ramp of Blackwell and Blackwell Ultra, inventory increased sequentially from 11,000,000,000 to 15,000,000,000 in q two. While we prioritize funding our growth and strategic initiatives, in q two, we returned 10,000,000,000 to shareholders through share repurchases and cash dividends. Our board of directors recently approved a 60,000,000,000 share repurchase authorization to add to our remaining 14,700,000,000.0 of authorization at the end of q two. Okay. Let me turn it to the outlook for the third quarter.\n\n**Colette Kress** (EVP &amp; CFO)\nTotal revenue is expected to be $54,000,000,000 plus or minus 2%. This represents over $7,000,000,000 in sequential growth. Again, we do not assume any h 20 shipments to China customers in our outlook. GAAP and non GAAP gross margins are expected to be 73.3%, 73.5%, respectively, plus or minus 50 basis points. We continue to expect to exit the year with non GAAP gross margins in the mid seventies.\n\n**Colette Kress** (EVP &amp; CFO)\nGAAP and non GAAP operating expenses are expected to be approximately 5,900,000,000.0 and 4,200,000,000.0, respectively. For the full year, we expect operating expenses to grow in the high thirties range year over year, up from our prior expectations of the mid thirties. We are accelerating investments in the business to address the magnitude of growth opportunities that lie ahead. GAAP and non GAAP other income and expenses are expected to be an income of approximately 500,000,000, excluding gains and losses from nonmarketable and public held equity securities. GAAP and non GAAP tax rates are expected to be 16.5, plus or minus 1%, excluding any discrete items.\n\n**Colette Kress** (EVP &amp; CFO)\nFurther financial data are included in the CFO commentary and other information available on our website. In closing, let me highlight upcoming events for the financial community. We will be at the Goldman Sachs Technology Conference on September 8 in San Francisco. Our annual NDR will commence the October. GTC data center begins on October 27 with Jensen's keynote scheduled for the twenty eighth.\n\n**Colette Kress** (EVP &amp; CFO)\nWe look forward to seeing you at these events. Our earnings call to discuss the results of our 2026 is scheduled for November 19. We will now open the call for questions. Operator, would you please poll for questions?\n\n**Operator**\nThank you. Your first question comes from C. J. Muse with Cantor Fitzgerald. Your line is open.\n\n**CJ Muse** (Senior Managing Director)\nYes, good afternoon. Thank you for taking the question.\n\n**CJ Muse** (Senior Managing Director)\nI guess with wafer in to rack out lead times of twelve months, you confirmed on the call today that Rubin is on track for ramp in the second half. And obviously, many of these investments are multiyear projects contingent upon power, cooling, etcetera. I was hoping perhaps you could you take a high level view and speak to, you know, your vision for growth in into 2026. And as part of that, if you could kinda comment between network and data center would be very helpful. Thank you.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nYeah. Thanks, CJ. At the highest level of growth drivers would be the evolution, the the introduction, if you will, of reasoning agentic AI. You know, where chatbots used to be one shot, you give it a prompt, and it would generate the answer. Now the AI does research.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nIt thinks and does a plan, and it might use tools. And so it's called long thinking, and the longer it thinks, oftentimes, it produces better answers. And the amount of computation necessary for one shot versus reasoning agentic AI models could be a 100 times, a thousand times, and potentially even more as the amount of research and basically reading and comprehension that it goes off to do. And so the amount of computation that has that has resulted in AgenTic AI has grown tremendously. And, of course, the effectiveness has also grown tremendously.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nBecause of Agentic AI, the amount of hallucination has dropped significantly. You can now use it you can now use tools and perform tasks. Enterprises have been opened up. As a result of agentic AI and vision language models, we now are seeing a breakthrough in physical AI, in robotics, autonomous systems. So the the last year, AI has made tremendous progress, and agentic systems, reasoning systems, is completely revolutionary.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nNow we built the Blackwell MVLink 72 system, a rack scale computing system, for this moment. We've been working on it for several years. This last year, we transitioned from MVLink eight, which is a node scale computing, each node is a computer, to now NVLink 72 where each rack is a computer. That disaggregation of NVLink 72 into a rack scale system was extremely hard to do, but the results are extraordinary. We're seeing orders of magnitude speed up and, therefore, energy efficiency and, therefore, cost effectiveness of token generation because of NVLink 72.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nAnd so over the next over the next couple of years, you're gonna over well, you asked about longer term. Over the next five years, we're gonna scale into with Blackwell, with Rubin, and follow ons to scale into effectively a 3 to $4,000,000,000,000 AI infrastructure opportunity. The last couple of years, you have seen that CapEx has grown in just the top four CSPs by has doubled and grown to about $600,000,000,000. So we're in the beginning of this build out, and the AI technology advances has really enabled AI to be able to adopt and solve problems to many different industries.\n\n**Operator**\nYour next question comes from Vivek Arya with Bank of America Securities. Your line is open.\n\n**Vivek Arya** (Managing Director)\nThanks for taking my questions. Colette, just wanted to clarify the 2,000,000,000 to $5,000,000,000 in China. What needs to happen? And what is the sustainable pace of that China business as you get into Q4? And then Jensen, for you on the competitive landscape, several of your large customers already have or are planning many ASIC projects.\n\n**Vivek Arya** (Managing Director)\nI think one of your ASIC competitors Broadcom signaled that they could grow their AI business almost 55%, 60% next year. Any scenario in which you see the market moving more towards ASICs and away from NVIDIA GPU? Just what are you hearing from your customers? How are they managing this split between their use of merchant silicon and ASICs? Thank you.\n\n**Colette Kress** (EVP &amp; CFO)\nThanks, Vivek. So let me first answer your question regarding what will it take for the h twenties to be shipped. There is interest in our h twenties. There is the initial set of licenses that we received. And then, additionally, we do have supply that we are ready, and that's why we communicated that somewhere in the range of about 2 to 5,000,000,000 this quarter, we could potentially ship.\n\n**Colette Kress** (EVP &amp; CFO)\nWe're still waiting on several of the geopolitical issues going back and forth between the governments and the companies trying to determine their purchases and what they want to do. So it's still, open at this time, and we're not exactly sure what that full amount will be about this quarter. However, if more interest arrives, more licenses arrives, again, we can also still build additional h 20 and ship more as well.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nNVIDIA builds very different things than ASICs, let's talk about ASICs first. A lot of projects are started. Many startup companies are created. Very few products go into production, and the reason for that is it's really hard. Accelerated computing is unlike general purpose computing.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nYou don't write software and just compile it into a processor. Accelerated computing is a full stack co design problem. And AI factories, in the last several years, has become so much more complex because of the scale of the problems have grown so significantly. It is it is really the ultimate the most extreme computer science problem the world's ever seen, obviously. And so the stack is complicated.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nThe models are changing incredibly fast from generative based on autoregressive to generative based on diffusion to mixed models to multimodality, the number of different models that are coming out that are either derivatives of transformers or evolutions of transformers is just daunting. One of the advantages that we have is that NVIDIA's available in every cloud. We're available from every computer company. We're available from the cloud to on prem to edge to robotics on the same programming model. And so it's sensible that every framework in the in the world supports NVIDIA.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nWhen you're building a new model architecture, releasing it on NVIDIA is most sensible. And so the diversity of our platform, both in the ability to evolve into any architecture, the fact that we're everywhere, and, also, we accelerate the entire pipeline. You know, everything from data processing to pretraining to post training with reinforcement learning, all the way out to inference. And so when you build a data center with NVIDIA platform in it, the utility of it is best. The life lifetime usefulness is much, much longer.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nAnd then I I would just say that that, in addition to all of that, it is just a a really extremely complex systems problem anymore. You know, people talk about the chip itself. There's one ASIC, the GPU that that many people talk about. But in order to build Blackwell, the platform, and Rubin, the platform, we had to build CPUs that connect fast memory, low low extremely energy efficient memory for large KB caching necessary for AgenTic AI to the GPU to a super NIC to a scale up switch we call NVLink, completely revolutionary when we're in our fifth generation now, to a scale out switch, whether it's quantum or spectrum x Ethernet, to now scale across switches so that we could prepare for these AI super factories with multiple gigawatts of computing all connected together. We call that Spectrum XGS.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nWe just announced that at Hotchips this week. And so the complications, the complexity of everything that we do is really quite extraordinary. It's just done in a in a really, really extreme scale now. And then lastly, if I could just say one more thing. You know, we're in every cloud for a good reason.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nNot only do are we the most energy efficient, our perf per watt is the best of any computing platform. And in a world of power limited data centers, perf per watt drives directly to revenues. And, you know, you've heard me say before that in a lot of ways, the more you buy, the more you grow. And because our perf per dollar, the performance per dollar is so incredible, you also have extremely great margins. So the the growth opportunity with NVIDIA's architecture and the gross margins opportunity with NVIDIA's architecture is absolutely the best.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nAnd so there's a lot of reasons why NVIDIA's chosen by every cloud and every startup and every computer company. We're, you know, really a holistic full stack solution for AI factories.\n\n**Operator**\nYour next question comes from Ben Reitzes with Melius. Your line is open.\n\n**Ben Reitzes** (MD &amp; Head - Technology Research)\nJensen, I wanted to ask you about your 3,000,000,000,000 to $4,000,000,000,000 in data center infrastructure spend by the end of the decade. Previously, you talked about something in the $1,000,000,000 range, which I believe was just for compute by 2028. If you take past comments, 3,000,000,000,000 to $4,000,000,000,000 would imply maybe $2,000,000,000 plus in compute spend. And just wanted to know if that was right and that's what you're seeing by the end of the decade. And wondering what you think your share will be of that.\n\n**Ben Reitzes** (MD &amp; Head - Technology Research)\nYour share right now of total infrastructure compute wise is very high. So wanted to see. And also if there's any bottlenecks you're concerned about like power to get to the 3 to 4,000,000,000,000. Thanks a lot.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nYeah. Thanks. As you know, the CapEx of just the top four hyperscalers has doubled in two years. As the AI revolution went into full steam, as the AI race is now on, the CapEx spend has doubled to $600,000,000,000 per year. There's five years between now and the end of the decade, and six hundred billion dollars, only represents the top four hyperscalers.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nWe still have the rest of the enterprise companies building on prem. You have enter you have cloud service providers building around the world. United States represents about 60% of the world's compute. And and over time, you would think that artificial intelligence would reflect GDP scale and growth, and so and would be would be, of course, accelerating GDP growth. And so so our our contribution to that is a large part of the AI infrastructure.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nOut of out of a gigawatt AI factory, which can go anywhere from 50 to, you know, plus or minus 10%, let's say, to 60,000,000,000, we represent about 35 plus or minus of that. And 35 out of fifty fifty or so, billion dollars for a gigawatt data center. And there and, of course, what you get for that is not a GPU. I think people, you know, were famous for building the GPU and inventing the GPU. But as you know, over the last decade, we've really transitioned to become an AI infrastructure company.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nIt takes six chips just to build six different types of chips just to build an a AI an a Rubin AI supercomputer. And just to scale that out, you know, to a gigawatt, you have hundreds of thousands of of GPU compute nodes and whole bunch of racks. And so we're really we're really an AI infrastructure company, and we're we're hoping to continue to contribute to growing this industry, making AI more useful, and then very importantly, driving the performance per watt because the world, as you mentioned, limiters, it will always likely be power limitations or AI infrastructure or AI building limitations. And so we need to squeeze as much out of that factory as possible. NVIDIA's performance per unit of energy used drives the revenue growth of that factory.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nIt directly translates. If you have a 100 megawatt factory, perf per 100 megawatt drives your revenues. It's tokens per 100 megawatts of factory. In our case, also, the performance per dollar spent is so high that your gross margins are also the best. But anyhow, these are these are the the limiters going forward, and and 3 to $4,000,000,000,000 is fairly sensible for the next five years.\n\n**Operator**\nNext question comes from Joe Moore of Morgan Stanley. Your line is open.\n\n**Joseph Moore** (Managing Director)\nGreat. Thank you. Congratulations on reopening the China opportunity. Can you talk about the long term prospects there? You've talked about, I think half of AI software world being there.\n\n**Joseph Moore** (Managing Director)\nYou know, how much can NVIDIA grow in that business, and, you know, how important is it that you get the Blackbaud architecture ultimately licensed there?\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nThe China market, I've estimated to be about $50,000,000,000 of opportunity for us this year. If we were able to address it with competitive products and and if it's $50,000,000,000 this year, you would expect it to grow, say, 50% per year as as the rest of the world's AI AI market is growing as well. It is the second largest computing market in the world, and it is also the home of AI researchers. About 50% of the world's AI researchers are in China. The vast majority of the leading open source models are created in China, and so it's fairly important, I think, for the American technology companies to be able to address that market.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nAnd open source, as you know, is created in one country, but it's used all over the world. The open source models that have come out of China are really excellent. DeepSeek, of course, gained global notoriety. Q1 is excellent. Kimi is excellent.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nThere's a there's a whole bunch of new models that are coming out. They're multimodal. They're link great language models, and it and it's it's really fueled the adoption of AI in enterprises around the world because enterprises wanna build their own custom proprietary software software stacks. And so open open source model is really important for enterprise. It's really important for SaaS who also would like to build proprietary systems.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nIt has been really incredible for robotics around the world. And so open source is really important, and it's important that the American companies are able to address it. This is it's gonna be a very large market. We're talking to we're talking to, the administration about the importance of American companies to be able to address, the Chinese market. And, as you know, h 20 has been approved, for companies that are not on the entities list, and many licenses have been approved.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nAnd, so I think the, you know, the the opportunity for us to bring Blackwell to the China market is a real possibility. And so we just have to keep advocating the the sensibility of and the importance of American tech companies to be able to to lead and win the AI race and help make the American tech stack the global standard.\n\n**Operator**\nYour next question comes from the line of Aaron Rakers with Wells Fargo. Your line is open.\n\n**Aaron Rakers** (Managing Director &amp; Technology Analyst)\nYeah. Thank you for the question. I want to go back to the Spectrum XGS announcement this week. And thinking about the Ethernet product now pushing over $10,000,000,000 of annualized revenue, just what is the opportunity set that you see for Spectrum XGS? So do we think about this as kind of the the data center interconnect layer?\n\n**Aaron Rakers** (Managing Director &amp; Technology Analyst)\nAny thoughts on the sizing of this opportunity, you know, within that Ethernet portfolio? Thank you.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nWe now offer three networking technologies. One is for scale up, one is for scale out, and one for scale across. Scale up is so that we could build the largest possible virtual GPU, the virtual compute node. NVLink is revolutionary. NVLink 72 is what made it possible for Blackwell to deliver such an extraordinary generational jump over Hopper's NVLink eight.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nAt a time when when we have long thinking thinking models, agentic AI reasoning systems, the NVLink basically amplifies the, memory bandwidth, which is really critical for, for reasoning systems. And so NVLink 72 is fantastic. We then scale out with networking, which we have two. We have InfiniBand, which is unquestionably the lowest latency, the lowest jitter, the best scale out network. It does require more expertise in managing those networks.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nAnd, for supercomputing, for the leading model makers, InfiniBand quantum InfiniBand is the unambiguous choice. If you were to benchmark an AI factory, the ones with InfiniBand are the best performance. For those who would like to use Ethernet because their their whole data center is built with Ethernet, we have a new type of Ethernet called Spectrum Ethernet. Spectrum Ethernet is not off the shelf. It has a whole bunch of new technologies designed for low latency and low jitter and congestion control, and and it has the ability, to, come closer, much, much closer, to InfiniBand than anything that's out there.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nAnd that's we call that Spectrum x Ethernet. And then finally, we have Spectrum xGS, a gigascale for connecting multiple data centers, multiple AI factories into a super factory, a gigantic system. And we're gonna you're gonna see that networking obviously is very important in AI factories. In fact, choosing the right networking, the performance, the throughput improvement going from, you know, 65% to 85% or 90%, that kind of that kind of step up because of their your networking capability effectively makes networking free. You know?\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nChoosing the right networking, you're basically paying you know, you're, you'll get a return on it like you can't believe because the AI factory, a gigawatt, as I mentioned before, could be $50,000,000,000. And so the ability to improve the efficiency of that factory by tens of percent is results in $1,020,000,000,000 dollars worth of effective benefit. And so, you know, this the the networking is a very important part of it. It's the reason why NVIDIA dedicates so much in networking. It's the reason why we purchased Mellanox five and a half years ago.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nAnd Spectrum X, as we mentioned earlier, is now quite a quite a sizable business, and it's only about a year and a half old. So x Spectrum X is a home run. All all three of them are gonna be fantastic. NVLink, scale up Spectrum X and InfiniBand, scale out, and then Spectrum XGS for scale across.\n\n**Operator**\nYour next question comes from Stacy Raskin with Bernstein Research. Your line is open.\n\n**Stacy Rasgon** (MD &amp; Senior Analyst)\nHi, guys. Thanks for taking my question. I have a more tactical question for Colette. So on the guide, you're up over $7,000,000,000 The vast bulk of that is going to be from data center. How do I think about apportioning that $7,000,000,000 out across Blackwell versus Hopper versus networking?\n\n**Stacy Rasgon** (MD &amp; Senior Analyst)\nI mean it looks like Blackwell was probably $27,000,000,000 in the quarter, up from maybe 23,000,000,000 last quarter. Hopper is still 6,000,000,000 or $7,000,000,000 post the H20. Like do you think the Hopper strength continues? Just how do I think about parsing that $7,000,000,000 out across all the the three those three different components?\n\n**Colette Kress** (EVP &amp; CFO)\nThanks, Stacy, for the question. First part of it, looking at our growth between q two and q three, Blackwell is still going to be, the lion's share, of what we have in terms of data center. But keep in mind, that helps both our compute side as well as it helps our networking side because we are selling those significant systems, that are incorporating the NVLink that Jensen, just spoke about. Selling Hopper, we are still selling it. H 100, h two hundreds, we are.\n\n**Colette Kress** (EVP &amp; CFO)\nBut, again, they are HCX systems, and I still believe our Blackwell will be the lion's share of what we're doing on there. So we'll continue. We don't have any more specific details in terms of how we'll finish our quarter, but you should expect Blackwell again to be the driver of the growth.\n\n**Operator**\nYour next question comes from Jim Schneider of Goldman Sachs. Your line is open.\n\n**Jim Schneider** (Senior Equity Analyst)\nGood afternoon. Thanks for taking my question. You've been very clear about the reasoning model opportunity that you see and you've also been relatively clear about the technical specs for Rubin. But maybe you could provide a little bit of context about how you view the Rubin product transition going forward? What incremental capability does that offer to customers?\n\n**Jim Schneider** (Senior Equity Analyst)\nAnd would you say that Rubin is a bigger, smaller or similar step up in terms of performance for capability perspective relative to what we saw at Blackwell? Thank you.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nYeah. Thanks. Ruben. Ruben Ruben, we're on a we're on an annual cycle. And the reason why we're on an annual cycle is because we can do so to accelerate the cost reduction and maximize the revenue generation for our customers.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nWhen we increase the perf per watt, the token generation per amount of usage of energy, we are effectively driving the revenues of our customers. The perf per watt of Blackwell will be, for reasoning systems, an order of magnitude higher than Hopper. And so for the same amount of energy, and everybody's data center is energy limited by definition, for any data center that we using Blackwell, you'll be able to maximize your revenues compared to anything we've done in the past, compared to anything in the world today. And because the perf per dollar, the performance is so good that the perf per dollar invested in the in the capital, would also allow you to improve your gross margins. To the extent that we have great ideas for every single generation, we could improve their the revenue generation, improve the AI capability, improve the margins of our customers by releasing new architectures.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nAnd so we advise our our partners, our customers to to pace themselves and to build these data centers on an annual rhythm. And Ruben is gonna is going to have a whole bunch of new ideas. I paused for a second because, you know, I've got plenty of time between now and a year from now to tell you about all the breakthroughs that Ruben's are gonna bring. And but we Rubens has a lot of great ideas. I'm anxious to tell you, but I can't right now.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nAnd I'll save it for for for GTC, to tell you more more and more about it. But, nonetheless, for the next year, we're ramping really hard into now Grace Blackwell, g b 200, and then now Blackwell Ultra, b 300. We're ramping really hard into data centers. This this this year is obviously a record breaking year. I expect next year to be a record breaking year.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nAnd while we continue to increase, the performance of, of AI capabilities as we race towards artificial superintelligence on the one hand, and continue to increase the revenue generation capabilities of our hyperscalers on the other hand.\n\n**Operator**\nYour final question comes from Timothy Arcuri with UBS. Your line is open.\n\n**Timothy Arcuri** (Managing Director)\nThanks a lot. Jensen, I wanted to ask you just answer the question you threw at a number, you said 50% CAGR for the AI market. So I'm wondering how much visibility that you have into next year. Is that kind of a reasonable bogey in terms of how much your data center revenue should grow next year? I would think you'll grow at least in line with that CAGR.\n\n**Timothy Arcuri** (Managing Director)\nAnd maybe are there any puts and takes to that? Thanks.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nWell, I think the the best way to look at it is is we we have we have reasonable forecasts from from our large customers for next year. A very, very significant forecast. And we still have a lot of businesses that we're still winning and a lot of start ups that are still being created. Don't forget that the number of start ups for AI native startups was a 100,000,000,000 was funded last year. This year, the year is not even over yet.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nIt's a 180,000,000,000 funded. If you look at look at AI native, the top AI native startups that are generating revenues, last year was $2,000,000,000. This year is $20,000,000,000. Next year, being 10 times higher than this year is not inconceivable. And the open source models is now opening up large enterprises, SaaS companies, industrial companies, robotics companies to now join the AI revolution, another source of growth.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nAnd, you know, whether whether it's AI natives or enterprise SaaS or industrial AI or startups, we're just seeing just enormous amount of, interest in AI and demand for AI. Right now, the buzz is, I'm sure all of you know about the buzz out there. The buzz is everything's sold out. H one Hers sold out. H two hundreds are sold out.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nLarge CSPs are coming out renting capacity from other CSPs. And so the the, AI native startups were really scrambling to get capacity so that they could train their reasoning models. And so the demand is really, really high. But the long term long term outlook between where we are today, CapEx has doubled in two years. It is now running about $600,000,000,000 a year just in the large hyperscalers.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nYou know, for us to grow into that $600,000,000,000 a year, representing a significant part of that CapEx isn't unreasonable. And so I think I think the the next several years, surely through the through the through the decade, we see just a really fast growing, really significant growth opportunities ahead. Let me conclude with this. Blackwell is the next generation AI platform the world's been waiting for. It delivers an ex exceptional generational leap.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nNVIDIA's NVLink 72 rack scale computing is revolutionary, arriving just in time as reasoning AI models drive order of magnitude increases in training and inference performance requirement. Blackwell Ultra is ramping at full speed, and the demand is extraordinary. Our next platform, Rubin, is already in fab. We have six new chips that represents the Rubin platform. They have all taped out to TSMC.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nRubin will be our third generation MB LINK RackScale AI supercomputer, and so we expect to have a much more mature and fully scaled up supply chain. Blackwell and Rubin AI factory platforms will be scaling into the 3 to $4,000,000,000,000 global AI factory build out through the end of the decade. Customers are building ever greater scale AI factories from thousands of hopper GPUs in tens of megawatt data centers to now hundreds of thousands of Blackwells in 100 megawatt facilities. And soon, we'll be building millions of g millions of Rubin GPU platforms powering multi gigawatt, multisite AI super factories. With each generation, demand only grows.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nOne shot chatbots have evolved into reasoning, agentic AI that research, plan, and use tools, driving orders of magnitude jump in compute for both training and inference. Agentic AI is reaching maturity and has opened the enterprise market to build domain and company specific AI agents for enterprise workflows, products, services. The age of physical AI has arrived, unlocking entirely new industries in robotics, industrial automation. Every industry and every industrial company will need to build two factories, one to build the machines and another to build their robotic AI. This quarter, NVIDIA reached record revenues and an extraordinary milestone in our journey.\n\n**Jensen Huang** (Founder, President, CEO &amp; Director)\nThe opportunity ahead is immense. A new industrial revolution has started. The AI race is on. Thanks for joining us today, and I look forward to addressing you next week, next earnings call. Thank you.\n\n**Operator**\nThis concludes today's conference call. You may now disconnect.",
      "fetched_at": "2026-02-04T15:38:18.244Z"
    },
    {
      "ticker": "NVDA",
      "title": "NVIDIA Corporation (NVDA) Q1 FY2026 earnings call transcript",
      "published_date": "May 28, 2025, 5:00 PM EDT",
      "fiscal_year": "2026",
      "quarter": "Q1",
      "url": "https://finance.yahoo.com/quote/NVDA/earnings/NVDA-Q1-2026-earnings_call-319901.html",
      "content": "**Operator**\nGood afternoon. My name is Sarah, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's First Quarter Fiscal 2026 Financial Results Conference Call. All lines have been placed on mute to prevent any background noise. After the speakers are marked, there will be a question-and-answer session. If you would like to ask a question during this time, simply press star one on your telephone keypad. If you would like to withdraw your question, please press star one again. Thank you. Toshiya Hari, you may begin your conference.\n\n**Toshiya Hari** (Head of Investor Relations)\nThank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the first quarter of fiscal 2026. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer, and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the second quarter of fiscal 2026. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially.\n\n**Toshiya Hari** (Head of Investor Relations)\nFor a discussion of factors that could affect our future financial results in business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, May 28th, 2025, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\n\n**Colette Kress** (EVP and CFO)\nThank you, Toshiya. We delivered another strong quarter with revenue of $44 billion, up 69% year-over-year, exceeding our outlook in what proved to be a challenging operating environment. Data center revenue of $39 billion grew 73% year-on-year. AI workloads have transitioned strongly to inference, and AI factory buildouts are driving significant revenue. Our customers' commitments are firm. On April 9, the U.S. government issued new export controls on H20, our data center GPU designed specifically for the China market. We sold H20 with the approval of the previous administration. Although our H20 has been in the market for over a year and does not have a market outside of China, the new export controls on H20 did not provide a grace period to allow us to sell through our inventory.\n\n**Colette Kress** (EVP and CFO)\nIn Q1, we recognized $4.6 billion in H20 revenue, which occurred prior to April 9th, but also recognized a $4.5 billion charge as we wrote down inventory and purchase obligations tied to orders we had received prior to April 9th. We were unable to ship $2.5 billion in H20 revenue in the first quarter due to the new export controls. The $4.5 billion charge was less than what we initially anticipated as we were able to reuse certain materials. We are still evaluating our limited options to supply data center compute products compliant with the U.S. government's revised export control rules. Losing access to the China AI accelerator market, which we believe will grow to nearly $50 billion, would have a material adverse impact on our business going forward and benefit our foreign competitors in China and worldwide.\n\n**Colette Kress** (EVP and CFO)\nOur Blackwell ramp, the fastest in our company's history, drove a 73% year-on-year increase in data center revenue. Blackwell contributed nearly 70% of data center compute revenue in the quarter, with a transition from Hopper nearly complete. The introduction of GB200 NVL was a fundamental architectural change to enable data center-scale workloads and to achieve the lowest cost per inference token. While these systems are complex to build, we have seen a significant improvement in manufacturing yields, and rack shipments are moving to strong rates to end customers. GB200 NVL racks are now generally available for model builders, enterprises, and sovereign customers to develop and deploy AI. On average, major hyperscalers are each deploying nearly 1,000 NVL72 racks or 72,000 Blackwell GPUs per week and are on track to further ramp output this quarter.\n\n**Colette Kress** (EVP and CFO)\nMicrosoft, for example, has already deployed tens of thousands of Blackwell GPUs and is expected to ramp to hundreds of thousands of GB200s with OpenAI as one of its key customers. Key learnings from the GB200 ramp will allow for a smooth transition to the next phase of our product roadmap, Blackwell Ultra. Sampling of GB300 systems began earlier this month at the major CSPs, and we expect production shipments to commence later this quarter. GB300 will leverage the same architecture, same physical footprint, and the same electrical and mechanical specifications as GB200. The GB300 drop-in design will allow CSPs to seamlessly transition their systems and manufacturing used for GB200 while maintaining high yields. B300 GPUs with 50% more HBM will deliver another 50% increase in dense FP4 inference compute performance compared to the B200.\n\n**Colette Kress** (EVP and CFO)\nWe remain committed to our annual product cadence, with our roadmap extending through 2028, tightly aligned with the multiple-year planning cycles of our customers. We are witnessing a sharp jump in inference demand. OpenAI, Microsoft, and Google are seeing a step-function leap in token generation. Microsoft processed over 100 trillion tokens in Q1, a fivefold increase on a year-over-year basis. This exponential growth in Azure OpenAI is representative of strong demand for Azure AI Foundry, as well as other AI services across Microsoft's platform. Inference-serving startups are now serving models using B200, tripling their token generation rate and corresponding revenues for high-value reasoning models such as DeepSeek R1, as reported by Artificial Analysis. NVIDIA Dynamo on Blackwell NVL72 turbocharges AI inference throughput by 30x for the new reasoning models sweeping the industry.\n\n**Colette Kress** (EVP and CFO)\nDeveloper engagements increased with adoption ranging from LLM providers such as Perplexity to financial services institutions such as Capital One, who reduced agentic chatbot latency by 5x with Dynamo. In the latest MLPerf inference results, we submitted our first results using GB200 NVL72, delivering up to 30x higher inference throughput compared to our eight GPU H200 submission on the challenging Llama 3.1 benchmark. This feat was achieved through a combination of tripling the performance per GPU as well as 9x more GPUs, all connected on a single NVLink domain. While Blackwell is still early in its life cycle, software optimizations have already improved its performance by 1.5x in the last month alone. We expect to continue improving the performance of Blackwell through its operational life, as we have done with Hopper and Ampere.\n\n**Colette Kress** (EVP and CFO)\nFor example, we increased the inference performance of Hopper by four times over two years. This is the benefit of NVIDIA's programmable CUDA architecture and rich ecosystem. The pace and scale of AI factory deployments are accelerating with nearly 100 NVIDIA-powered AI factories in flight this quarter, a twofold increase year-over-year, with the average number of GPUs powering each factory also doubling in the same period. More AI factory projects are starting across industries and geographies. NVIDIA's full-stack architecture is underpinning AI factory deployments as industry leaders like AT&T, BYD, Capital One, Foxconn, MediaTek, and Telenor are strategically vital sovereign clouds like those recently announced in Saudi Arabia, Taiwan, and the UAE. We have a line of sight to projects requiring tens of GW of NVIDIA AI infrastructure in the not-too-distant future.\n\n**Colette Kress** (EVP and CFO)\nThe transition from generative to agentic AI, AI capable of perceiving, reasoning, planning, and acting, will transform every industry, every company, and country. We envision AI agents as a new digital workforce capable of handling tasks ranging from customer service to complex decision-making processes. We introduced the Llama Nemotron family of open reasoning models designed to supercharge agentic AI platforms for enterprises. Built on the Llama architecture, these models are available as NIMs or NVIDIA Inference Microservices with multiple sizes to meet diverse deployment needs. Our post-training enhancements have yielded a 20% accuracy boost and a 5x increase in inference speed. Leading platform companies, including Accenture, Cadence, Deloitte, and Microsoft, are transforming work with our reasoning models. NVIDIA NeMo Microservices are generally available across industries and are being leveraged by leading enterprises to build, optimize, and scale AI applications.\n\n**Colette Kress** (EVP and CFO)\nWith NeMo, Cisco increased model accuracy by 40% and improved response time by 10x in its code assistant. Nasdaq realized a 30% improvement in accuracy and response time in its AI platform's search capabilities. Shell's custom LLM achieved a 30% increase in accuracy when trained with NVIDIA NeMo. NeMo's parallelism techniques accelerated model training time by 20% when compared to other frameworks. We also announced a partnership with Yum Brands, the world's largest restaurant company, to bring NVIDIA AI to 500 of its restaurants this year and expanding to 61,000 restaurants over time to streamline order taking, optimize operations, and enhance service across its restaurants. For AI-powered cybersecurity, leading companies like Check Point, CrowdStrike, and Palo Alto Networks are using NVIDIA's AI security and software stack to build, optimize, and secure agentic workflows, with CrowdStrike realizing 2x faster detection triage with 50% less compute cost.\n\n**Colette Kress** (EVP and CFO)\nMoving to networking, sequential growth in networking resumed in Q1, with revenue up 64% quarter-over-quarter to $5 billion. Our customers continue to leverage our platform to efficiently scale up and scale out AI factory workloads. We created the world's fastest switch, NVLink. For scale-up, our NVLink compute fabric in its fifth generation offers 14x the bandwidth of PCIe Gen 5. NVLink 72 carries 130 TB per second of bandwidth in a single rack, equivalent to the entirety of the world's peak internet traffic. NVLink is a new growth vector and is off to a great start, with Q1 shipments exceeding $1 billion. At Computex, we announced NVLink Fusion. Hyperscale customers can now build semi-custom CCUs and accelerators that connect directly to the NVIDIA platform with NVLink.\n\n**Colette Kress** (EVP and CFO)\nWe are now enabling key partners, including ASIC providers such as MediaTek, Marvell, Alchip Technologies, and Astera Labs, as well as CPU suppliers such as Fujitsu and Qualcomm, to leverage NVLink Fusion to connect our respective ecosystems. For scale-out, our enhanced Ethernet offerings deliver the highest throughput, lowest latency networking for AI. SpectrumX posted strong sequential and year-on-year growth and is now annualizing over $8 billion in revenue. Adoption is widespread across major CSPs and consumer internet companies, including CoreWeave, Microsoft Azure, Oracle Cloud, and xAI. This quarter, we added Google Cloud and Meta to the growing list of SpectrumX customers. We introduced SpectrumX and QuantumX silicon photonics switches featuring the world's most advanced co-package optics. These platforms will enable next-level AI factory scaling to millions of GPUs through the increasingly power efficiency by 3.5x and network resiliency by 10x while accelerating customer time to market by 1.3x.\n\n**Colette Kress** (EVP and CFO)\nTransitioning to a quick summary of our revenue by geography. China, as a percentage of our data center revenue, was slightly below our expectations and down sequentially due to H20 export licensing controls. For Q2, we expect a meaningful decrease in China data center revenue. As a reminder, while Singapore represented nearly 20% of our Q1 build revenue, as many of our large customers use Singapore for centralized invoicing, our products are almost always shipped elsewhere. Note that over 99% of H100, H200, and Blackwell data center compute revenue billed to Singapore was for orders from U.S.-based customers. Moving to gaming and AI PCs. Gaming revenue was a record $3.8 billion, increasing 48% sequentially and 42% year-on-year. Strong adoption by gamers, creatives, and AI enthusiasts have made Blackwell our fastest ramp ever.\n\n**Colette Kress** (EVP and CFO)\nAgainst a backdrop of robust demand, we greatly improved our supply and availability in Q1 and expect to continue these efforts in Q2. AI is transforming PC and creator and gamers. With a 100 million user installed base, GeForce represents the largest footprint for PC developers. This quarter, we added to our AI PC laptop offerings, including models capable of running Microsoft's Copilot+. This past quarter, we brought Blackwell architecture to mainstream gaming with its launch of GeForce RTX 5060 and 5060 Ti, starting at just $299. The RTX 5060 also debuted in laptops, starting at $1,099. These systems that doubled the frame rate and slash latency. These GeForce RTX 5060 and 5060 Ti desktop GPUs and laptops are now available.\n\n**Colette Kress** (EVP and CFO)\nIn console gaming, the recently unveiled Nintendo Switch 2 leverages NVIDIA's neural rendering and AI technologies, including next-generation custom RTX GPUs with DLSS technology, to deliver a giant leap in gaming performance to millions of players worldwide. Nintendo has shipped over 150 million Switch consoles to date, making it one of the most successful gaming systems in history. Moving to probe visualization. Revenue of $509 million was flat sequentially and up 19% year-on-year. Tariff-related uncertainty temporarily impacted Q1 systems, and demand for our AI workstations is strong, and we expect sequential revenue growth to resume in Q2. NVIDIA DGX Spark and Station revolutionized personal computing by putting the power of an AI supercomputer in a desktop form factor. DGX Spark delivers up to one petaflop of AI compute, while DGX Station offers an incredible 20 petaflops and is powered by the GB300 superchip.\n\n**Colette Kress** (EVP and CFO)\nDGX Spark will be available in calendar Q3 and DGX Station later this year. We have deepened Omniverse's integration and adoption into some of the world's leading software platforms, including Databricks, SAP, and Schneider Electric. New Omniverse blueprints such as Mega for at-scale robotic fleet management are being leveraged in Kion Group, Pegatron, Accenture, and other leading companies to enhance industrial operations. At Computex, we showcased Omniverse's great traction with technology manufacturing leaders, including TSMC, Quanta, Foxconn, Pegatron. Using Omniverse, TSMC saves months in work by designing fabs virtually. Foxconn accelerates thermal simulations by 150x, and Pegatron reduced assembly line defect rates by 67%. Lastly, with our automotive group, revenue was $567 million, down 1% sequentially, but up 72% year-on-year. Year-on-year growth was driven by the ramp of self-driving across a number of customers and robust end demand for NEVs.\n\n**Colette Kress** (EVP and CFO)\nWe are partnering with GM to build the next-gen vehicles, factories, and robots using NVIDIA AI simulation and accelerated computing. We are now in production with our full-stack solution for Mercedes-Benz, starting with the new CLA, hitting roads in the next few months. We announced Isaac GR00T and won the world's first open, fully customizable foundation model for humanoid robots, enabling generalized reasoning and skill development. We also launched new open NVIDIA Cosmos World Foundation models. Leading companies include One X, Agility Robotics, Figure AI, Uber, and Wobot. We've begun integrating Cosmos into their operations for synthetic data generation, while Agility Robotics, Boston Dynamics, and XPeng Robotics are harnessing Isaac simulation to advance their humanoid efforts. GE Healthcare is using the new NVIDIA Isaac platform for healthcare simulation built on NVIDIA Omniverse and using NVIDIA Cosmos, the platform speeds development of robotic imaging and surgery systems.\n\n**Colette Kress** (EVP and CFO)\nThe era of robotics is here. Billions of robots, hundreds of millions of autonomous vehicles, and hundreds of thousands of robotic factories and warehouses will be developed. All right, moving to the rest of the P&L. GAAP gross margins and non-GAAP gross margins were 60.5% and 61%, respectively. Excluding the $4.5 billion charge, Q1 non-GAAP gross margins would have been 71.3%, slightly above our outlook at the beginning of the quarter. Sequentially, GAAP operating expenses were up 7%, and non-GAAP operating expenses were up 6%, reflecting higher compensation and employee growth. Our investments include expanding our infrastructure capabilities and AI solutions, and we plan to grow these investments throughout the fiscal year. In Q1, we returned a record $14.3 billion to shareholders in the form of share repurchases and cash dividends. Our capital return program continues to be a key element of our capital allocation strategy.\n\n**Colette Kress** (EVP and CFO)\nLet me turn to the outlook for the second quarter. Total revenue is expected to be $45 billion, + or -2%. We expect modest sequential growth across all of our platforms. In data center, we anticipate the continued ramp of Blackwell to be partially offset by a decline in China revenue. Note, our outlook reflects a loss in H20 revenue of approximately $8 billion for the second quarter. GAAP and non-GAAP gross margins are expected to be 71.8% and 72%, respectively, + or-50 basis points. We expect better Blackwell profitability to drive modest sequential improvement in gross margins. We are continuing to work towards achieving gross margins in the mid-70s range late this year.\n\n**Colette Kress** (EVP and CFO)\nGAAP and non-GAAP operating expenses are expected to be approximately $5.7 billion and $4 billion, respectively, and we continue to expect full year fiscal year 2026 operating expense growth to be in the mid-30% range. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $450 million, excluding gain and losses from non-marketable and publicly held equity securities. GAAP and non-GAAP tax rates are expected to be 16.5%, + or -1%, excluding any discrete items. Further financial details are included in the CFO commentary and other information available on our IR website, including a new Financial Information AI agent. Let me highlight upcoming events for the financial community.\n\n**Colette Kress** (EVP and CFO)\nWe will be at the B of A Global Technology Conference in San Francisco on June 4th, the Rosenblatt Virtual AI Summit and Nasdaq Investor Conference in London on June 10th, and GTC Paris at VivaTech on June 11th in Paris. We look forward to seeing you at these events. Our earnings call to discuss the results of our second quarter of fiscal 2026 is scheduled for August 27th. Now let me turn it over to Jensen to make some remarks.\n\n**Jensen Huang** (President and CEO)\nThanks, Colette. We've had a busy and productive year. Let me share my perspective on some topics we're frequently asked. On export control, China is one of the world's largest AI markets and a springboard to global success. With half of the world's AI researchers based there, the platform that wins China is positioned to lead globally.\n\n**Jensen Huang** (President and CEO)\nToday, however, the $50 billion China market is effectively closed to U.S. industry. The H20 export ban ended our Hopper data center business in China. We cannot reduce Hopper further to comply. As a result, we are taking a multi-billion dollar write-off on inventory that cannot be sold or repurposed. We are exploring limited ways to compete, but Hopper is no longer an option. China's AI moves on with or without U.S. chips. It has to compute to train and deploy advanced models. The question is not whether China will have AI. It already does. The question is whether one of the world's largest AI markets will run on American platforms. Shielding Chinese chip makers from U.S. competition only strengthens them abroad and weakens America's position. Export restrictions have spurred China's innovation and scale. The AI race is not just about chips. It's about which stack the world runs on.\n\n**Jensen Huang** (President and CEO)\nAs that stack grows to include 6G and quantum, U.S. global infrastructure leadership is at stake. The U.S. has based its policy on the assumption that China cannot make AI chips. That assumption was always questionable, and now it's clearly wrong. China has enormous manufacturing capability. In the end, the platform that wins the AI developers wins AI. Export controls should strengthen U.S. platforms, not drive half of the world's AI talent to rivals. On DeepSeek, DeepSeek and QN from China are among the best open-source AI models. Released freely, they've gained traction across the U.S., Europe, and beyond. DeepSeek R1, like ChatGPT, introduced Reasoning AI that produces better answers the longer it thinks. Reasoning AI enables step-by-step problem-solving, planning, and tool use, turning models into intelligent agents. Reasoning is compute-intensive, requires hundreds to thousands of times more tokens per task than previous one-shot inference.\n\n**Jensen Huang** (President and CEO)\nReasoning models are driving a step-function surge in inference demand. AI scaling laws remain firmly intact, not only for training, but now inference too requires massive-scale compute. DeepSeek also underscores the strategic value of open-source AI. When popular models are trained and optimized on U.S. platforms, it drives usage, feedback, and continuous improvement, reinforcing American leadership across the stack. U.S. platforms must remain the preferred platform for open-source AI. That means supporting collaboration with top developers globally, including in China. America wins when models like DeepSeek and QN run best on American infrastructure. Regarding onshore manufacturing, President Trump has outlined a bold vision to reshore advanced manufacturing, create jobs, and strengthen national security. Future plants will be highly computerized and robotics. We share this vision. TSMC is building six fabs and two advanced packaging plants in Arizona to make chips for NVIDIA.\n\n**Jensen Huang** (President and CEO)\nProcess qualification is underway, with volume production expected by year-end. Spill and Amcor are also investing in Arizona, constructing packaging, assembly, and test facilities. In Houston, we're partnering with Foxconn to construct a million-square-foot factory to build AI supercomputers. Wistron is building a similar plant in Fort Worth, Texas. To encourage and support these investments, we've made substantial long-term purchase commitments, a deep investment in America's AI manufacturing future. Our goal: from chip to supercomputer, built in America within a year. Each GB200 NVLink 72 racks contains 1.2 million components and weighs nearly 2 tons. No one has produced supercomputers on this scale. Our partners are doing an extraordinary job. On AI diffusion rule, President Trump rescinded the AI diffusion rule, calling it counterproductive, and proposed a new policy to promote U.S. AI tech with trusted partners. On his Middle East tour, he announced historic investments.\n\n**Jensen Huang** (President and CEO)\nI was honored to join him in announcing a 500 MW AI infrastructure project in Saudi Arabia and a 5 GW AI campus in the UAE. President Trump wants U.S. tech to lead. The deals he announced are wins for America: creating jobs, advancing infrastructure, generating tax revenue, and reducing the U.S. trade deficit. The U.S. will always be NVIDIA's largest market and home to the largest installed base of our infrastructure. Every nation now sees AI as core to the next industrial revolution, a new industry that produces intelligence and essential infrastructure for every economy. Countries are racing to build national AI platforms to elevate their digital capabilities. At Computex, we announced Taiwan's first AI factory in partnership with Foxconn and the Taiwan government. Last week, I was in Sweden to launch its first national AI infrastructure.\n\n**Jensen Huang** (President and CEO)\nJapan, Korea, India, Canada, France, the U.K., Germany, Italy, Spain, and more are now building national AI factories to empower startups, industries, and societies. Sovereign AI is a new growth engine for NVIDIA. Toshiya, back to you. Thank you.\n\n**Toshiya Hari** (Head of Investor Relations)\nOperator, we will now open the call for questions. Would you please pull for questions?\n\n**Operator**\nThank you. At this time, I would like to remind everyone, in order to ask a question, press star, then the number one on your telephone keypad. We'll pause for just a moment to compile the Q&A roster. Your first question comes from the line of Joe Moore with Morgan Stanley. Your line is open.\n\n**Joe Moore** (Analyst)\nGreat. Thank you. You guys have talked about this scaling up of inference around reasoning models for at least a year now, and we've really seen that come to fruition, as you talked about. We've heard it from your customers.\n\n**Joe Moore** (Analyst)\nCan you give us a sense for how much of that demand you're able to serve? Give us a sense for maybe how big the inference business is for you guys, and do we need full-on NVL72 rack scale solutions for reasoning inference going forward?\n\n**Jensen Huang** (President and CEO)\nWe would like to serve all of it, and I think we're on track to serve most of it. Grace Blackwell, NVLink 72, is the ideal engine today, the ideal computer thinking machine, if you will, for reasoning AI. There's a couple of reasons for that. The first reason is that the token generation amount, the number of tokens reasoning goes through, is 1000 times more than a one-shot chatbot. It's essentially thinking to itself, breaking down a problem step by step. It might be planning multiple paths to an answer.\n\n**Jensen Huang** (President and CEO)\nIt could be using tools, reading PDFs, reading web pages, watching videos, and then producing a result, an answer. The longer it thinks, the better the answer, the smarter the answer is. What we would like to do, and the reason why Grace Blackwell was designed to give such a giant step up in inference performance, is so that you could do all this and still get a response as quickly as possible. Compared to Hopper, Grace Blackwell is some 40 times higher speed and throughput compared. This is going to be a huge benefit in driving down the cost while improving the quality of response with excellent quality of service at the same time. That is the fundamental reason. That was the core driving reason for Grace Blackwell NVLink 72.\n\n**Jensen Huang** (President and CEO)\nOf course, in order to do that, we had to reinvent, literally redesign the entire way that these supercomputers are built. Now we're in full production. It's going to be exciting. It's going to be incredibly exciting.\n\n**Operator**\nThe next question comes from Vivek Arya with Bank of America Securities. Your line is open.\n\n**Vivek Arya** (Managing Director and Senior Analyst)\nThanks for the question. Just a clarification for Colette first. On the China impact, I think previously it was mentioned at about $15 billion. You had the $8 billion in Q2. Is there still some left as a headwind for the remaining quarters? Colette, how to model that? Question, Jensen, for you. Back at GTC, you had outlined a path towards almost a trillion dollars of AI spending over the next few years. Where are we in that build-out?\n\n**Vivek Arya** (Managing Director and Senior Analyst)\nDo you think it's going to be uniform that you will see every spender, whether it's ESP, sovereigns, enterprises, or build-out? Should we expect some periods of digestion in between? Just what are your customer discussions telling you about how to model growth for next year?\n\n**Colette Kress** (EVP and CFO)\nYes, Vivek, thanks so much for the question regarding H20. Yes, we recognized $4.6 billion H20 in Q1. We were unable to ship $2.5 billion. The total for Q1 should have been $7 billion. When we look at our Q2, our Q2 is going to be meaningfully down in terms of China data center revenue. We had highlighted in terms of the amount of orders that we had planned for H20 in Q2, and that was $8 billion. Now, going forward, we did have other orders going forward that we will not be able to fulfill.\n\n**Colette Kress** (EVP and CFO)\nThat is what was incorporated, therefore, in the amount that we wrote down of the $4.5 billion. That write-down was about inventory and purchase commitments. And our purchase commitments were about what we expected regarding the orders that we had received. Going forward, though, it's a bigger issue regarding the amount of the market that we will not be able to serve. We assess that TAM to be close to about $50 billion in the future as we don't have a product to enable for the China.\n\n**Jensen Huang** (President and CEO)\nVivek, probably the best way to think through it is that AI is several things. Of course, we know that AI is this incredible technology that's going to transform every industry, from, of course, the way we do software to healthcare and financial services to retail to, I guess, every industry, transportation, manufacturing. And we're at the beginning of that.\n\n**Jensen Huang** (President and CEO)\nMaybe another way to think about that is, where do we need intelligence? Where do we need digital intelligence? Every country is in every industry. We know, because of that, we recognize that AI is also an infrastructure. It is a way of delivering a technology that requires factories. These factories produce tokens. They, as I mentioned, are important to every single industry and every single country. On that basis, we are really at the very beginning of it because the adoption of this technology is really kind of in its early stages. Now, we have reached an extraordinary milestone with AIs that are reasoning, are thinking, what people call inference time scaling. Of course, it created a whole new era. We have entered an era where inference is going to be a significant part of the compute workload. Anyhow, it is going to be a new infrastructure.\n\n**Jensen Huang** (President and CEO)\nWe're building it out in the clouds. The United States is really the early starter and available in U.S. clouds. This is our largest market, our largest installed base, and we're going to continue to see that happening. Beyond that, we're going to see AI go into enterprise, which is on-prem, because so much of the data is still on-prem. Access control is really important. It's really hard to move every company's data into the cloud. We're going to move AI into the enterprise. You saw that we announced a couple of really exciting new products, our RTX Pro enterprise AI server that runs everything enterprise and AI, our DGX Spark and DGX Station, which is designed for developers who want to work on-prem. Enterprise AI is just taking off. Telcos.\n\n**Jensen Huang** (President and CEO)\nToday, a lot of the telco infrastructure will be, in the future, software-defined and built on AI. 6G is going to be built on AI. That infrastructure needs to be built out. It is at its very, very early stages. Of course, every factory today that makes things will have an AI factory that sits with it. The AI factory is going to be creating AI and operating AI for the factory itself, but also to power the products and the things that are made by the factory. It is very clear that every car company will have AI factories. Very soon, there will be robotics companies, robot companies, and those companies will be also building AIs to drive the robots. We are at the beginning of all of this build-out.\n\n**Operator**\nThe next question comes from CJ Muse with Cantor Fitzgerald.\n\n**Operator**\nYour line is open.\n\n**CJ Muse** (Senior Managing Director)\nYeah, good afternoon. Thank you for taking the question. There have been many large GPU cluster investment announcements in the last month, and you alluded to a few of them with Saudi Arabia, the UAE, and then also we've heard from Oracle and xAI, just to name a few. My question, are there others that have yet to be announced of the same kind of scale and magnitude? Perhaps more importantly, how are these orders impacting your lead times for Blackwell and your current visibility sitting here today, almost halfway through 2025?\n\n**Jensen Huang** (President and CEO)\nWe have more orders today than we did at the last time I spoke about orders at GTC. However, we're also increasing our supply chain and building out our supply chain. They're doing a fantastic job.\n\n**Jensen Huang** (President and CEO)\nWe're building it here on shore in the United States, but we're going to keep our supply chain quite busy for many more years coming. With respect to further announcements, I'm going to be on the road next week through Europe. Just about every country needs to build out AI infrastructure, and there are umpteen AI factories being planned. I think in the remarks, Colette mentioned there's 100 AI factories being built. There's a whole bunch that haven't been announced. I think the important concept here, which makes it easier to understand, is that like other technologies that impact literally every single industry, of course, electricity was one, and it became infrastructure. Of course, the information infrastructure, which we now know as the internet, affects every single industry, every country, every society. Intelligence is surely one of those things.\n\n**Jensen Huang** (President and CEO)\nI don't know any company, industry, country who thinks that intelligence is optional. It's essential infrastructure. We've now digitalized intelligence. I think we're clearly in the beginning of the build-out of this infrastructure. Every country will have it. I'm certain of that. Every industry will use it. That I'm certain of. What's unique about this infrastructure is that it needs factories. It's a little bit like the energy infrastructure, electricity. It needs factories. We need factories to produce this intelligence. The intelligence is getting more sophisticated. We were talking about earlier that we had a huge breakthrough in the last couple of years with reasoning AI. Now there are agents that reason, and there are super agents that use a whole bunch of tools. There's clusters of super agents where agents are working with agents, solving problems.\n\n**Jensen Huang** (President and CEO)\nYou could just imagine, compared to one-shot chatbots and the agents that are now using AI built on these large language models, how much more compute-intensive they really need to be and are. I think we're in the beginning of the build-out. There should be many, many more announcements in the future.\n\n**Operator**\nYour next question comes from Ben Reitzes with Melius. Your line is open.\n\n**Ben Reitzes** (Managing Director and Head of Technology Research)\nYeah, hi. Thanks for the question. I wanted to ask first to Colette just a little clarification around the guidance and maybe putting it in a different way. The $8 billion for H20 just seems like it's roughly $3 billion more than most people thought with regard to what you'd be foregoing in the second quarter.\n\n**Ben Reitzes** (Managing Director and Head of Technology Research)\nThat would mean that with regard to your guidance, the rest of the business, in order to hit 45, is doing $2 billion-$3 billion or so better. I was wondering if that math made sense to you. In terms of the guidance, that would imply the non-China business is doing a bit better than the street expected. Wondering what the primary driver was there in your view. This second part of my question, Jensen, I know you guide one quarter at a time. With regard to the AI diffusion rule being lifted and this momentum with Sovereign, there have been times in your history where you guys have sat on calls like this where you have more conviction and sequential growth throughout the year, etc.\n\n**Ben Reitzes** (Managing Director and Head of Technology Research)\nGiven the unleashing of demand with AI diffusion being revoked and the supply chain increasing, does the environment give you more conviction and sequential growth as we go throughout the year? First one for Colette, and then next one for Jensen. Thanks so much.\n\n**Colette Kress** (EVP and CFO)\nThanks, Ben, for the question. When we look at our Q2 guidance and our commentary that we provided that had the export controls not occurred, we would have had orders of about $8 billion for H20. That's correct. That was a possibility for what we would have had in our outlook for this quarter in Q2. What we also have talked about here is the growth that we've seen in Blackwell, Blackwell across many of our customers, as well as the growth that we continue to have in terms of supply that we need for our customers.\n\n**Colette Kress** (EVP and CFO)\nPutting those together, that's where we came through with the guidance that we provided. I'm going to turn the rest over to Jensen to see how he wants to.\n\n**Jensen Huang** (President and CEO)\nYeah, thanks. Thanks, Ben. I would say compared to the beginning of the year, compared to GTC timeframe, there are four positive surprises. The first positive surprise is the step function demand increase of reasoning AI. I think it is fairly clear now that AI is going through an exponential growth. Reasoning AI really busted through. Concerns about hallucination or its ability to really solve problems. I think a lot of people are crossing that barrier and realizing how incredibly effective agentic AI is and reasoning AI is. Number one is inference reasoning and the exponential growth there, demand growth. The second one, you mentioned AI diffusion.\n\n**Jensen Huang** (President and CEO)\nIt's really terrific to see that the AI diffusion rule was rescinded. President Trump wants America to win. He also realizes that we're not the only country in the race. He wants the United States to win and recognizes that we have to get the American stack out to the world and have the world build on top of American stacks instead of alternatives. AI diffusion happened. The rescinding of it happened at almost precisely the time that countries around the world are awakening to the importance of AI as an infrastructure, not just as a technology of great curiosity and great importance, but infrastructure for their industries and startups and society. Just as they had to build out infrastructure for electricity and internet, you got to build out infrastructure for AI. I think that that's an awakening, and that creates a lot of opportunity.\n\n**Jensen Huang** (President and CEO)\nThe third is enterprise AI. Agents work. And these agents are really quite successful. Much more than generative AI, agentic AI is game-changing. Agents can understand ambiguous and rather implicit instructions and are able to problem-solve and use tools and have memory and so on. I think this enterprise AI is ready to take off. It's taken us a few years to build a computing system that is able to integrate and run enterprise AI stacks, run enterprise IT stacks, but add AI to it. This is the RTX Pro enterprise server that we announced at Computex just last week. Just about every major IT company has joined us and are super excited about that. Computing is one part of it. Remember, enterprise IT is really three pillars. It's compute, storage, and networking.\n\n**Jensen Huang** (President and CEO)\nWe have now put all three of them together finally, and we are going to market with that. Lastly, industrial AI. Remember, one of the implications of the world reordering, if you will, is regions onshoring manufacturing and building plants everywhere. In addition to AI factories, of course, there are new electronics manufacturing, chip manufacturing being built around the world. All of these new plants and these new factories are creating exactly the right time when Omniverse and AI and all the work that we are doing with robotics is emerging. This fourth pillar is quite important. Every factory will have an AI factory associated with it. In order to create these physical AI systems, you really have to train a vast amount of data. Back to more data, more training, more AIs to be created, more computers.\n\n**Jensen Huang** (President and CEO)\nThese four drivers are really kicking into turbocharge.\n\n**Operator**\nYour next question comes from Timothy Arcuri with UBS. Your line is open.\n\n**Timothy Arcuri** (Managing Director)\nThanks a lot. Jensen, I wanted to ask about China. It sounds like the July guidance assumes there's no SKU replacement for the H20. If the president wants the U.S. to win, it seems like you're going to have to be allowed to ship something into China. I guess I had two points on that. First of all, have you been approved to ship a new modified version into China? You're currently building it, but you just can't ship it in fiscal Q2. You were sort of run rating $7 billion-$8 billion a quarter into China. Can we get back to those sorts of quarterly run rates once you get something that you're allowed to ship back into China?\n\n**Timothy Arcuri** (Managing Director)\nI think we're all trying to figure out how much to add back to our models and when. Whatever you can say there would be great. Thanks.\n\n**Jensen Huang** (President and CEO)\nThe president has a plan. He has a vision, and I trust him. With respect to our export controls, it's a set of limits. The new set of limits pretty much make it impossible for us to reduce Hopper any further for any productive use. The new limits, it's kind of the end of the road for Hopper. We have limited options. The key is to understand the limits. The key is to understand the limits and see if we can come up with interesting products that could continue to serve the Chinese market. We don't have anything at the moment, but we're considering it. We're thinking about it.\n\n**Jensen Huang** (President and CEO)\nObviously, the limits are quite stringent at the moment. We have nothing to announce today. When the time comes, we'll engage the administration and discuss that.\n\n**Operator**\nYour final question comes from the line of Aaron Rakers with Wells Fargo. Your line is open.\n\n**Operator**\nHi. This is Jake On for Aaron. Thanks for taking the question and congrats on the great quarter. I was wondering if you could give some additional color around the strength you saw within the networking business, particularly around the adoption of your Ethernet solutions at CSPs, as well as any change you're seeing in network attach rates.\n\n**Jensen Huang** (President and CEO)\nYeah, thank you for that. We now have three networking platforms, maybe four. The first one is the scale-up platform to turn a computer into a much larger computer. Scaling up is incredibly hard to do.\n\n**Jensen Huang** (President and CEO)\nScaling out is easier to do, but scaling up is hard to do. That platform is called NVLink. NVLink comes with chips and switches and NVLink spines. It is really complicated. Anyways, that is our new platform, scale-up platform. In addition to InfiniBand, we also have SpectrumX. We have been fairly consistent that Ethernet was designed for a lot of traffic that are independent. In the case of AI, you have a lot of computers working together. The traffic of AI is insanely bursty. Latency matters a lot because the AI is thinking, and it wants to get work done as quickly as possible. You have a whole bunch of nodes working together. We enhanced Ethernet, added capabilities like extremely low latency, congestion control, adaptive routing, the type of technologies that were available only in InfiniBand to Ethernet.\n\n**Jensen Huang** (President and CEO)\nAs a result, we improved the utilization of Ethernet in these clusters. These clusters are gigantic, from as low as 50% to as high as 85%, 90%. The difference is if you had a cluster that's $10 billion and you improved its effectiveness by 40%, that's worth $4 billion. It's incredible. SpectrumX has been really, quite frankly, a home run in this last quarter. As we said in the prepared remarks, we added two very significant CSPs to the SpectrumX adoption. The last one is BlueField, which is our control plane.\n\n**Jensen Huang** (President and CEO)\nIn those four, the control plane, the network, which is used for storage, is used for security, and for many of these clusters that want to achieve isolation among its users, multi-tenant clusters, and still be able to use and have extremely high-performance bare metal performance, BlueField is ideal for that and is used in a lot of these cases. We have these four networking platforms. They are all growing. We are doing really well. I am very proud of the team.\n\n**Jensen Huang** (President and CEO)\nJensen, over to you.\n\n**Operator**\nThat is all the time we have for questions. Jensen and I will turn the call back to you.\n\n**Toshiya Hari** (Head of Investor Relations)\nThank you. This is the start of a powerful new wave of growth. Grace Blackwell is in full production. We are off to the races. We now have multiple significant growth engines. Inference, one's the light of workload, is surging with revenue-generating AI services.\n\n**Toshiya Hari** (Head of Investor Relations)\nAI is growing faster and will be larger than any platform shifts before, including the internet, mobile, and cloud. Blackwell is built to power the full AI lifecycle from training frontier models to running complex inference and reasoning agents at scale. Training demands continue to rise with breakthroughs in post-training and reinforcement learning and synthetic data generation. Inference is exploding. Reasoning AI agents require orders of magnitude more compute. The foundations of our next growth platforms are in place and ready to scale. Sovereign AI nations are investing in AI infrastructure like they once did for electricity and internet. Enterprise AI must be deployable on-prem and integrated with existing IT. Our RTX Pro, DGX Spark, and DGX Station enterprise AI systems are ready to modernize the $500 billion IT infrastructure on-prem or in the cloud. Every major IT provider is partnering with us.\n\n**Toshiya Hari** (Head of Investor Relations)\nIndustrial AI, from training to digital twin simulation to deployment, NVIDIA Omniverse and Isaac GR00T are powering next-generation factories and humanoid robotic systems worldwide. The age of AI is here. From AI infrastructures, inference at scale, sovereign AI, enterprise AI, and industrial AI, NVIDIA is ready. Join us at GTC Paris. I'll keynote at VivaTech on June 11, talking about quantum GPU computing, robotic factories and robots, and celebrate our partnerships building AI factories across the region. The NVIDIA band will tour France, the U.K., Germany, and Belgium. Thank you for joining us at the earnings call today. See you in Paris.\n\n**Operator**\nThis concludes today's conference call. You may now disconnect.",
      "fetched_at": "2026-02-04T15:38:23.155Z"
    },
    {
      "ticker": "NVDA",
      "title": "NVIDIA Corporation (NVDA) Q4 FY2025 earnings call transcript",
      "published_date": "Feb 26, 2025, 5:00 PM EST",
      "fiscal_year": "2025",
      "quarter": "Q4",
      "url": "https://finance.yahoo.com/quote/NVDA/earnings/NVDA-Q4-2025-earnings_call-251681.html",
      "content": "**Operator**\nGood afternoon. My name is Krista, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's Fourth Quarter Earnings Call. All lines have been placed on mute to prevent any background noise.\n\n**Operator**\nAfter the speakers' remarks, there will be a question and answer session. Thank you. Stuart Stecker, you may begin your conference.\n\n**Stewart Stecker** (Senior Director - IR)\nThank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the fourth quarter of fiscal twenty twenty five. With me today from NVIDIA are Jensen Wong, President and Chief Executive Officer and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the first quarter of fiscal twenty twenty six.\n\n**Stewart Stecker** (Senior Director - IR)\nThe content of today's call is NVIDIA's property. It can't be reproduced or transcribed without prior written consent. During this call, we may make forward looking statements based on current expectations. These are subject to a number of significant risks and uncertainties and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10 ks and 10 q, and the reports that we may file on Form eight ks with the Securities and Exchange Commission.\n\n**Stewart Stecker** (Senior Director - IR)\nAll our statements are made as of today, 02/26/2025, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non GAAP financial measures and find a reconciliation of these non GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\n\n**Colette Kress** (EVP &amp; CFO)\nThanks, Stuart. Q4 was another record quarter. Revenue of $39,300,000,000 was up 12% sequentially and up 78% year on year, and above our outlook of thirty seven point five billion dollars For fiscal twenty twenty five, revenue was $130,500,000,000 up 114% in the prior year. Let's start with data center. Data center revenue for fiscal twenty twenty five was $115,200,000,000 more than doubling from the prior year.\n\n**Colette Kress** (EVP &amp; CFO)\nIn the fourth quarter, data center revenue of $35,600,000,000 was a record of 16% sequentially and 93% year on year. As the Blackwell ramp commenced and Hopper 200 continued sequential growth. In Q4, Blackwell sales exceeded our expectations. We delivered $11,000,000,000 of Blackwell revenue to meet strong demand. This is the fastest product ramp in our company's history, unprecedented in its speed and scale.\n\n**Colette Kress** (EVP &amp; CFO)\nBlackwell production is in full gear across multiple configurations and we are increasing supply quickly, expanding customer adoption. Our Q4 data center compute revenue jumped 18% sequentially and over 2X year on year. Customers are racing to scale infrastructure to train the next generation of cutting edge models and unlock the next level of AI capabilities. With Blackwell, it will be common for these clusters to start with 100,000 GPUs or more. Shipments have already started for multiple infrastructures of this size.\n\n**Colette Kress** (EVP &amp; CFO)\nPost training and model customization are fueling demand for NVIDIA infrastructure and software as developers and enterprises leverage techniques such as fine tuning, reinforcement learning and distillation to tailor models for domain specific use cases. Hugging Face alone hosts over 90,000 derivatives created from the LAMA Foundation model. The scale of post training and model customization is massive and can collectively demand orders of magnitude more compute than pre training. Our inference demand is accelerating, driven by test time scaling and new reasoning models like OpenAI's O3, DeepSeq R1 and GROC3. Long thinking reasoning AI can require 100 X more compute per task compared to one shot inferences.\n\n**Colette Kress** (EVP &amp; CFO)\nBlackwell was architected for reasoning AI inference. Blackwell supercharges reasoning AI models with up to 25 X higher token throughput and 20 X lower cost versus hover 100. It is revolutionary. Transformer engine is built for LLM and mixture of experts infants. And its NVLink domain delivers 14 X the throughput of PCIe Gen five, ensuring the response time, throughput and cost efficiency needed to tackle the growing complexity of inference at scale.\n\n**Colette Kress** (EVP &amp; CFO)\nCompanies across industries are tapping into NVIDIA's full stack inference platform to boost performance and slash costs. Now tripled inference throughput and cut costs by 66% using NVIDIA TensorRT for its screenshot feature. Perplexity sees four thirty five million monthly queries and reduced its inference costs 3 X with NVIDIA Triton Inference Server and TensorRT LLM. Microsoft Bing achieved a five x speed up and major TCO savings for visual search across billions of images with NVIDIA, TensorRT and acceleration libraries. Blackwell has great demand for inference.\n\n**Colette Kress** (EVP &amp; CFO)\nMany of the early GB200 deployments are earmarked for inference, a first for a new architecture. Blackwell addresses the entire AI market from pre training, post training to inference across clouds to on premise to enterprise. CUDA's programmable architecture accelerates every AI model and over 4,400 applications, ensuring large infrastructure investments against obsolescence in rapidly evaluating markets. Our performance and pace of innovation is unmatched. We're driven to a 200X reduction in inference cost in just the last two years.\n\n**Colette Kress** (EVP &amp; CFO)\nWe delivered the lowest TCO and the highest ROI. And full stack optimizations for NVIDIA and our large ecosystem, including 5,900,000 developers continuously improve our customers' economics. In Q4, large CSPs represented about half of our data center revenue. And these sales increased nearly 2X year on year. Large CSPs were some of the first to stand up Blackwell with Azure, GCP, AWS and OCI bringing GB200 systems to cloud regions around the world to meet surging customer demand for AI.\n\n**Colette Kress** (EVP &amp; CFO)\nRegional cloud hosting NVIDIA GPUs increased as a percentage of data center revenue, reflecting continued AI factory build outs globally and rapidly rising demand for AI reasoning models and agents. Or we've launched a 100,000 GV200 cluster based instance with NVLink switch and Quantum two InfiniBand. Consumer Internet revenue grew 3X year on year, driven by an expanding set of generative AI and deep learning use cases. These include recommender systems, vision language understanding, synthetic data generation search and agentic AI. For example, XAI is adopting the GB200 to train and inference its next generation of GROG AI models. Meta's cutting edge Andromeda advertising engine runs on NVIDIA's Grace Hopper Superchip, serving vast quantities of ads across Instagram, Facebook applications. Andromeda harnesses Grace Hopper's fast interconnect and large memory to boost inference throughput by three X, enhance ad personalization and deliver meaningful jumps in monetization and ROI. Enterprise revenue increased nearly 2X year on accelerating demand for model fine tuning, RAG and agentic AI workflows and GPU accelerated data processing. We introduced NVIDIA llama Numeron model family NIMS to help developers create and deploy AI agents across a range of applications, including customer support, fraud detection and product supply chain and inventory management.\n\n**Colette Kress** (EVP &amp; CFO)\nLeading AI agent platform providers, including SAP and ServiceNow, are among the first to use new models. Healthcare leaders IQVIA, Illumina and Mayo Clinic are well, as ARC Institute, are using NVIDIA AI to speed drug discovery, enhance genomic research and pioneer advanced healthcare services with generative and agentic AI. As AI expands beyond the digital world, NVIDIA infrastructure and software platforms are increasingly being adopted to power robotics and physical AI development. One of the early and largest robotics applications and autonomous vehicles where virtually every AV company is developing on NVIDIA in the data center, the car or Bulls. NVIDIA's automotive vertical revenue is expected to grow to approximately $5,000,000,000 this fiscal year.\n\n**Colette Kress** (EVP &amp; CFO)\nAt CES, at CES, Hyundai Motor Group announced it is adopting NVIDIA technologies to accelerate AV and robotics development and smart factory initiatives. Vision transformers, self supervised learning, multimodal sensor fusion and high fidelity simulation are driving breakthroughs in AB development and will require 10X more compute. At TEDx, we announced the NVIDIA Cosmo World Foundation Model Platform. Just as language foundation models have revolutionized language AI, Cosmos is a physical AI to revolutionize robotics. The robotics and automotive companies, including ride sharing giant Uber, are among the first to adopt the platform.\n\n**Colette Kress** (EVP &amp; CFO)\nFrom a geographic perspective, sequential growth in our data center revenue was strongest in The US, driven by the initial ramp of BlackRock. Countries across the globe are building their AI ecosystems and demand for compute infrastructure is surging. France's Two Hundred Billion Euro AI investment and the EU's two hundred billion euros Invest AI initiatives offer a glimpse into the build out to set redefined global AI infrastructure in the coming years. Now, as a percentage of total data center revenue, data center sales in China remained well below levels seen on the onset of export controls. Absent any change in regulations, we believe that China shipments will remain roughly at the current percentage.\n\n**Colette Kress** (EVP &amp; CFO)\nThe market in China for data center solutions remains very competitive. We will continue to comply with export controls while serving our customers. Networking revenue declined 3% sequentially. Our networking attached to GPU compute systems is robust at over 75%. We are transitioning from small NVLink eight with InfiniBand to large NVLink 72 with SpectrumX.\n\n**Colette Kress** (EVP &amp; CFO)\nSpectrumX and NVLink switch revenue increased and represents a major new growth sector. We expect networking to return to growth in Q1. AI requires a new class of networking. NVIDIA offers NVLink switch systems for scale up compute. For scale out, we offer Quantum InfiniBand for HPC supercomputers and SpectrumX for Ethernet environments.\n\n**Colette Kress** (EVP &amp; CFO)\nSpectrumX enhances the Ethernet for AI computing and has been a huge success. Microsoft Azure, OCI, CoreWeave and others are building large AI factories with SpectrumX. The first Stargate data centers will use SpectrumX. Yesterday, Cisco announced integrating SpectrumX into their networking portfolio to help enterprises build AI infrastructure. With its large enterprise footprint and global reach, Cisco will bring NVIDIA Ethernet to every industry.\n\n**Colette Kress** (EVP &amp; CFO)\nNow, moving to gaming and AI PCs. Gaming revenue of $2,500,000,000 decreased 22% sequentially and 11% year on year. Full year revenue of $11,400,000,000 increased 9% year on year. And demand remained strong throughout the holiday. However, Q4 shipments were impacted by supply constraints.\n\n**Colette Kress** (EVP &amp; CFO)\nWe expect strong sequential growth in Q1 as supply increases. The new GeForce RTX 50 series desktop and laptop GPUs are here. Built for gamers, creators and developers, they fuse AI and graphics redefining visual computing. Powered by the Blackwell architecture, fifth generation tensor cores and fourth generation RT cores, and featuring up to 3,400 AI top. These GPUs deliver a 2x performance leap and new AI driven rendering, including neural shaders, digital human technologies, geometry and lighting.\n\n**Colette Kress** (EVP &amp; CFO)\nThe new DLSS four boosts frame rates up to 8x with AI driven frame generation, turning one rendered frame into three. It also features the industry's first real time application of transformer models, packing 2x more parameters and 4x the compute for unprecedented visual fidelity. We also announced a wave of GeForce Blackwell laptop GPUs with new NVIDIA Max Q technology that extends battery life by up to an incredible 40%. These laptops will be available starting in March from the world's top manufacturers. Moving to our professional visualization business.\n\n**Colette Kress** (EVP &amp; CFO)\nRevenue of $511,000,000 was up 5% sequentially and 10% year on year. Full year revenue of $1,900,000,000 increased 21% year on year. Key industry verticals driving demand include automotive and healthcare. NVIDIA technologies and generative AI are reshaping design, engineering and simulation workloads. Increasingly, these technologies are being leveraged in leading software platforms from ANSYS, Cadence and Siemens, fueling demand for NVIDIA RTX workstation.\n\n**Colette Kress** (EVP &amp; CFO)\nNow moving to automotive. Revenue was a record $570,000,000 up 27% sequentially and up 103% year on year. Full year revenue of 1,700,000,000 increased 55% year on year. Strong growth was driven by the continued ramp in autonomous vehicles, including cars and robotaxis. At CES, we announced Toyota, the world's largest automaker, will build its next generation vehicles on NVIDIA Orin, running the safety certified NVIDIA DRIVE OS.\n\n**Colette Kress** (EVP &amp; CFO)\nWe announced Aurora and Continental will deploy driverless trucks at scale powered by NVIDIA DRIVE four. Finally, our end to end autonomous vehicle platform NVIDIA DRIVE Hyperion has passed industry safety assessments by Toussud and Toussud Ryland, two of the industry's foremost authorities for automotive grade safety and cybersecurity. NVIDIA is the first AV platform to receive a comprehensive set of third party assessments. Okay. Moving to the rest of the P and L.\n\n**Colette Kress** (EVP &amp; CFO)\nGAAP gross margins was 73% and non GAAP gross margins were 73.5%, down sequentially as expected with our first deliveries of the Blackwell architecture. As discussed last quarter, Blackwell is a customizable AI infrastructure with several different types of NVIDIA build chips, multiple networking options and for air and liquid cooled data center. We exceeded our expectations in Q4 in ramping Blackwell, increasing system availability, providing several configurations to our customers. As Blackwell ramps, we expect gross margins to be in the low 70s. We initially, we are focused on expediting the manufacturing of Blackwell systems to meet strong customer demand as they race to build out Blackwell infrastructure.\n\n**Colette Kress** (EVP &amp; CFO)\nWhen fully ramped, we have many opportunities to improve the cost and gross margin will improve and return to the mid-70s late this fiscal year. Sequentially, GAAP operating expenses were up 9% and non GAAP operating expenses were 11%, reflecting higher engineering development costs and higher compute and infrastructure costs for new product introductions. In Q4, we returned 8,100,000,000 to shareholders in the form of share repurchases and cash dividends. Let me turn to the outlook in the first quarter. Total revenue is expected to be $43,000,000,000 plus or minus 2%.\n\n**Colette Kress** (EVP &amp; CFO)\nContinuing with its strong demand, we expect a significant ramp of Blackwell in Q1. We expect sequential growth in both data center and gaming. Within data center, we expect sequential growth from both compute and networking. GAAP and non GAAP gross margins are expected to be 70.671% respectively, plus or minus 50 basis points. GAAP and non GAAP operating expenses are expected to be approximately $5,200,000,000 and $3,600,000,000 respectively.\n\n**Colette Kress** (EVP &amp; CFO)\nWe expect full year fiscal year 2026 operating expenses to grow to be in the mid-30s. GAAP and non GAAP other income and expenses are expected to be an income of approximately $400,000,000 excluding gains and losses from non marketable and publicly held equity securities. GAAP and non GAAP tax rates are expected to be 17% plus or minus 1% excluding any discrete items. Further financial details are included in the CFO commentary and other information available on our IR website, including a new financial information AI agent. In closing, let me highlight upcoming events for the financial community.\n\n**Colette Kress** (EVP &amp; CFO)\nWe will be at the TD Cowen Healthcare Conference in Boston on March 3, and at the Morgan Stanley Technology, Media and Telecom Conference in San Francisco on March 5. Please join us for our annual GTC conference starting Monday, March 17 in San Jose, California. Jensen will deliver a news packed keynote on March 18, and we will host a Q and A session for our financial analysts the next day, March 19. We look forward to seeing you at these events. Our earnings call to discuss the results for our first quarter of fiscal twenty twenty six is scheduled for 05/28/2025.\n\n**Colette Kress** (EVP &amp; CFO)\nWe are going to open up the call, operator, to questions. If you could start that, that would be great.\n\n**Operator**\nAnd your first question comes from C. J. Muse with Cantor Fitzgerald. Please go ahead.\n\n**CJ Muse** (Senior Managing Director)\nYes. Good afternoon. Thank you for taking the question. I guess, for me, Judson, as tough comp computing reinforcement learning shows such promise, we're clearly seeing increasing blurring in the lines between training and inference. What does this mean for the potential future of potentially inference dedicated clusters?\n\n**CJ Muse** (Senior Managing Director)\nAnd how do you think about the overall impact to NVIDIA and your customers? Thank you.\n\n**Jensen Huang** (President &amp; CEO)\nYes, I appreciate that, CJ. There are now multiple scaling laws. There's the pre training scaling law. And that's going to continue to scale because we have multi modality, we have, data that came from, reasoning that are now used to, do pretraining. And then the second is post training scaling law using reinforcement learning human feedback, reinforcement learning AI feedback, reinforcement learning verifiable rewards.\n\n**Jensen Huang** (President &amp; CEO)\nThe amount of computation you use for post training is actually higher than pre training. And it's kind of sensible in the sense that you could, while you're using reinforcement learning, generate an enormous amount of synthetic data or synthetically generated tokens. AI models are basically generating tokens to train AI models. And that's post training. And the third the third part, this is the part that you mentioned, is test time compute or reasoning, long thinking, inference scaling.\n\n**Jensen Huang** (President &amp; CEO)\nThey're all basically the same ideas. And there is you have chain of thought, you have search. The amount of tokens generated, the amount of inference compute needed is already a hundred times more than the one shot examples and the one shot capabilities of large language models in the beginning. And that's just the beginning. This is just the beginning.\n\n**Jensen Huang** (President &amp; CEO)\nThe idea that, the next generation could have thousands times and even hopefully, extremely thoughtful and simulation based and search based models that could be hundreds of thousands, millions of times, more compute than today, is, is, in our future. And so, so the question is how do you design such an architecture? Some of it some of the models are autoregressive, some of the models are diffusion based, Some of it some of the times you want your, data center to have disaggregated inference, sometimes it's compacted. And so, it's hard to it's hard to figure out, what is the best configuration of a data center, which is the reason why NVIDIA's architecture is so popular. We run every model.\n\n**Jensen Huang** (President &amp; CEO)\nWe are great at training. The vast majority of our compute today is actually inference. And Blackwell takes all of that to a new level. We designed Blackwell with the idea of reasoning models in mind. And when you look at training, it's many times more performant.\n\n**Jensen Huang** (President &amp; CEO)\nBut what's really amazing is for long thinking, test time scaling, reasoning AI models were tens of times faster, 25 times higher throughput. And so, Blackwell is going to be incredible across the board. And when you have a data center that allows you to, configure and use your data center, based on are you doing more pre training now, post training now, or scaling out your inference, our architecture is fungible and easy to use, in all of those different ways. And so, we're seeing in fact much, much more concentration of a unified architecture than ever before.\n\n**Operator**\nYour next question comes from the line of Joe Moore with JPMorgan. Please go ahead.\n\n**Joe Moore** (Analyst)\nGood morning, Stanley, actually. Thank you. I wonder if you could talk about GB200 at CES. You sort of talked about the complexity of the rack level systems and the challenges you have. And then as you said in the prepared remarks, we've seen a lot of general availability.\n\n**Joe Moore** (Analyst)\nWhere are you in terms of that ramp? Are there still bottlenecks to consider at a systems level above and beyond the chip level? And just have you maintained your enthusiasm for the NVL 72 platforms?\n\n**Jensen Huang** (President &amp; CEO)\nWell, I'm more enthusiastic today than I was at CES. And the reason for that is because we've shipped a lot more to CES. We have some three fifty plants, manufacturing the 1,500,000 components that go into each one of the Blackwell racks, Grace Blackwell racks. Yes, it's extremely complicated and, we successfully, and incredibly ramped up Grace Blackwell, delivering some $11,000,000,000 in revenues last quarter. We're going to have to continue to scale as demand is quite high and customers are anxious and impatient to get their Blackwell systems.\n\n**Jensen Huang** (President &amp; CEO)\nYou've probably seen on the web a fair number of celebrations about Grace Blackwell systems coming online. And we have them, of course, we have a fairly large installation of Grace Blackwell's for our own engineering and our own design teams and software teams. CoreWeave has now, been quite public about the successful bring up of theirs. Microsoft has. Of course, OpenAI has.\n\n**Jensen Huang** (President &amp; CEO)\nAnd you're starting to see many, many come online. And, so, I think the answer to your question is, nothing is easy about what we're doing, but we're doing great. And, all of our partners are doing great.\n\n**Operator**\nYour next question comes from the line of Vivek Arya with Bank of America Securities. Please go ahead.\n\n**Vivek Arya** (Managing Director)\nThank you for taking my question. Could I just you wouldn't mind confirming if Q1 is the bottom for gross margin? And then, Jensen, my question is for you. What is on your dashboard to give you the confidence that the strong demand can sustain into next year? And has DeepSeek and whatever innovations they came up with, has that changed that view in any way? Thank you.\n\n**Colette Kress** (EVP &amp; CFO)\nLet me first take the first part of the question, there regarding the gross margin. During our Blackwell ramp, our gross margins, will be in the low 70s. At this point, we are focusing on expediting our manufacturing, expediting our manufacturing to make sure that we can provide the customers as soon as possible. Our Blackwell has fully ramped, and once it does I'm sorry, once our Blackwell fully ramps, we can improve our cost and our gross margin. So we expect to probably be in the mid-70s later this year.\n\n**Colette Kress** (EVP &amp; CFO)\nYou know, walking through what you heard, Johnson speak about the systems and their complexity, they are customizable in some cases. They've got multiple networking options. They have liquid cooled and water cooled. So we know there is an opportunity for us to improve these gross margins going forward. But right now, we are going to focus on getting the manufacturing complete and to our customers as soon as possible.\n\n**Jensen Huang** (President &amp; CEO)\nWe know several things, Vivek. We have a fairly good line of sight of the amount of capital investment that data centers are building out towards. We know that going forward, the vast majority of software is going to be based on machine learning. And so accelerated computing and generative AI, reasoning AI, are going to be the type of architecture you want in your data center. We have, of course, forecasts and plans from our top partners.\n\n**Jensen Huang** (President &amp; CEO)\nAnd, we also know that there are many innovative, really exciting startups that are still coming online as new opportunities for developing the next breakthroughs in AI, whether it's agentic AIs, reasoning AIs, or physical AIs. The number of startups are still quite vibrant and each one of them need a fair amount of computing infrastructure. And so, I think the whether it's the near term signals or the midterm signals near term signals, of course, are POs and forecasts and things like that. Midterm signals would be, the level of infrastructure and CapEx scale out compared to previous years. And then the long term signals has to do with the fact that we know fundamentally software has changed from hand coding that runs on CPUs to machine learning and AI based software that runs on GPUs and accelerated computing systems.\n\n**Jensen Huang** (President &amp; CEO)\nAnd so, we have a fairly good sense that this is the future of software. And then maybe as you roll it out, another way to think about that is, we've really only tapped consumer, AI and search and some amount of consumer generative AI. Advertising, recommenders, kind of the early days of software. The next wave's coming. Agentic AI for enterprise, physical AI for robotics, and, Sovereign AI as different regions build out their AI for their own ecosystems.\n\n**Jensen Huang** (President &amp; CEO)\nAnd so, each one of these are fairly off the ground and we can see them. We can see them because, obviously, we're in the center of much of this development and we can see great activity happening in all these different places and these will happen. So near term, mid term, long term.\n\n**Operator**\nYour next question comes from the line of Harlan Sur with JPMorgan. Please go ahead.\n\n**Harlan Sur** (Executive Director - Equity Research)\nYes, good afternoon. Thanks for taking my question. Your next generation Blackwall Ultra set to launch in the second half of this year, in line with the team's annual product cadence. Jensen, can you help us understand the demand dynamics for Ultra, given that you'll still be ramping the current generation Blackwell solutions? How do your customers and the supply chain also manage the simultaneous ramps of these two products?\n\n**Harlan Sur** (Executive Director - Equity Research)\nAnd is the team still on track to execute Blackwell Ultra in the second half of this year?\n\n**Jensen Huang** (President &amp; CEO)\nYes. Blackwell Ultra is second half. As you know, the first Blackwell was, we had a hiccup that probably cost us a couple of months. We're fully recovered, of course. The team did an amazing job recovering.\n\n**Jensen Huang** (President &amp; CEO)\nAnd all of our supply chain partners and just so many people helped us recover at the speed of light. And so now we've successfully ramped production of Blackwell. But that doesn't stop the next train. The next train is on an annual rhythm and Blackwell Ultra with new networking, new memories and, of course, new processors. And all of that is coming online.\n\n**Jensen Huang** (President &amp; CEO)\nWe've been working with all of our partners and customers laying this out. They have all of the necessary information and we'll work with everybody to do the proper transition. This time between Blackwell and Blackwell Ultra, the system architecture is exactly the same. It's a lot harder going from Hopper to Blackwell because we went from an NVLink eight system to a NVLink 72 based system. So the chassis, the architecture of the system, the hardware, the power delivery, all of that had to change.\n\n**Jensen Huang** (President &amp; CEO)\nThis was quite a challenging transition. But the next transition will slot right in, right? Blackwall Ultra will slot right in. We've also already revealed and been working very closely with all of our partners on the click after that. And the click after that is called Verarubin.\n\n**Jensen Huang** (President &amp; CEO)\nAnd, all of our partners are, getting up to speed on, on the transition of that. And so preparing for that transition. And again, we're going to provide a big, big, huge step up. And so come to GCC and I'll talk to you about Blackwell Ultra, Verarubin and then show you what's the one click after that. Really exciting new products. So come to GTC, please.\n\n**Operator**\nYour next question comes from the line of Timothy Arcuri with UBS. Please go ahead.\n\n**Timothy Arcuri** (Managing Director)\nThanks a lot. Jensen, we hear a lot about custom ASICs. Can you kind of speak to the balance between custom ASIC and merchant GPU? We hear about some of these heterogeneous superclusters to use both GPU and ASIC. Is that something customers are planning on building or will these infrastructures remain fairly distinct? Thanks.\n\n**Jensen Huang** (President &amp; CEO)\nWell, we build very different things than ASICs, in some ways completely different in some areas we intersect. We're different in several ways. One, NVIDIA's architecture is general. Whether you're you've optimized for autoregressive models or diffusion based models or vision based models or multimodal models or text models, we're great at all of it. We're great at all of it because our software stack is so our architecture is flexible, our software stack is ecosystem is so rich that we're the initial target of most exciting innovations and algorithms.\n\n**Jensen Huang** (President &amp; CEO)\nAnd so, by definition, we're much, much more general than narrow. We're also, really good from the end to end, from data processing, the curation of the training data to, the training of the data, of course, to reinforcement learning, used in post training all the way to inference with test time scaling. So, we're general, we're end to end and we're everywhere. And because we're not in just one cloud, we're in every cloud, we could be on prem, we could be in a robot. Our architecture is much more accessible and a great target initial target for anybody who's starting up a new company.\n\n**Jensen Huang** (President &amp; CEO)\nAnd so, we're everywhere. And then the third thing I would say is that our performance and our rhythm is so incredibly fast. Remember that these data centers are always fixed in size. They're fixed in size or they're fixed in power. And if our performance per watt is anywhere from 2x to 4x to 8x, which is not unusual, it translates directly to revenues.\n\n**Jensen Huang** (President &amp; CEO)\nAnd so, if you have a 100 megawatt data center, if the performance or the throughput in that 100 megawatt or that gigawatt data center is four times or eight times higher, your revenues for that gigawatt data center is eight times higher. And the reason that is so different than data centers of the past is because AI factories are directly monetizable through its tokens generated. And so the token throughput of our architecture being so incredibly fast is just incredibly valuable to all of the companies that are building these things for revenue generation reasons and capturing the fast ROIs. And so, I think the third reason is performance. And then, the last thing that I would say is the software stack is incredibly hard.\n\n**Jensen Huang** (President &amp; CEO)\nBuilding an ASIC is no different than what we do. We have to build a new architecture. And the ecosystem that sits on top of our architecture is 10 times more complex today than it was two years ago. And that's fairly obvious because the amount of software that the world is building on top of architecture is growing exponentially and AI is advancing very quickly. So bringing that whole ecosystem on top of multiple chips is hard.\n\n**Jensen Huang** (President &amp; CEO)\nAnd so I would say that those four reasons. And then finally, I will say this, just because the chip is designed doesn't mean it gets deployed. And you've seen this over and over again. There are a lot of chips that gets built. But when the time comes, a business decision has to be made.\n\n**Jensen Huang** (President &amp; CEO)\nAnd that business decision is about deploying a new engine, a new processor into a limited AI factory in size, in power and in time. And our technology, is not only more advanced, more performant, it has much, much better software capability. And very importantly, our ability to deploy is lightning fast. And so, these things are not for the faint of heart as everybody knows now. And so, there's a lot of different reasons why we do well, why we win.\n\n**Operator**\nYour next question comes from the line of Ben Reitz with Melius Research. Please go ahead.\n\n**Ben Reitzes** (Managing Director – Head of Technology Research)\nYes. Hi, Ben Reitz is here. Hey, thanks a lot for the question. Hey, Jensen, it's a geography related question. You did a great job explaining some of the demand underlying factors here on the strength.\n\n**Ben Reitzes** (Managing Director – Head of Technology Research)\nBut U. S. Was up about $5,000,000,000 or so sequentially. And I think there is a concern about whether U. S.\n\n**Ben Reitzes** (Managing Director – Head of Technology Research)\nCan pick up the slack if there's regulations towards other geographies. And I was just wondering as we go throughout the year, if this kind of surge in The U. S. Continues and it's going to be, whether that's okay and if that underlies your growth rate, how can you keep growing so fast with this mix shift towards The U. S?\n\n**Ben Reitzes** (Managing Director – Head of Technology Research)\nYour guidance looks like China is probably up sequentially. So just wondering if you could go through that dynamic and maybe Colette can weigh in? Thanks a lot.\n\n**Jensen Huang** (President &amp; CEO)\nChina is approximately the same percentage as Q4 and as, in previous quarters. It's about half of what it was before the export control. But it's approximately the same in percentage. With respect to geographies, the takeaway is that AI is software. It's modern software.\n\n**Jensen Huang** (President &amp; CEO)\nIt's incredible modern software, but it's modern software and AI has gone mainstream. AI is used in delivery services everywhere, shopping services everywhere. You know, if you were to buy, you know, a quarter of milk is delivered to you, AI was involved. And so, almost everything that a consumer service, provides, AI's at the core of it. Every every student will use AI as a tutor.\n\n**Jensen Huang** (President &amp; CEO)\nHealthcare services use AI. Financial services use AI. No fintech company will not use AI. Every fintech company will. Climate tech company use AI.\n\n**Jensen Huang** (President &amp; CEO)\nMineral discovery now uses AI. The number of the number of at every higher education, every university, uses AI. And so, I think it is fairly safe to say that AI has gone mainstream, that it's being integrated into every application. And our hope is that, of course, the technology continues to advance, safely and advance in a helpful way to society. And with that, we're, I do believe that we're at the beginning of this new transition.\n\n**Jensen Huang** (President &amp; CEO)\nAnd what I mean by that in the beginning is remember behind us has been decades of data centers and decades of computers that have been built. And they've been built for a world of hand coding and general purpose computing and, CPUs and so on and so forth. And going forward, I think it's fairly safe to say that that world is going to be almost all software will be infused with AI. All software and all services will be based on ultimately based on machine learning and the data flywheel is going to be part of improving software and services, and that the future computers will be accelerated. The future computers will be based on AI.\n\n**Jensen Huang** (President &amp; CEO)\nAnd we're really two years into that journey. And in modernizing computers that have taken decades to build out. And so I'm fairly sure that we're in the beginning of this new era. And then lastly, no technology has ever had the opportunity to address a larger part of the world's GDP than AI. No software tool ever has.\n\n**Jensen Huang** (President &amp; CEO)\nAnd so, this is now a software tool that can address a much larger part of the world's GDP more than any time in history. And so, the way we think about growth and the way we think about whether something is big or small has to be in the context of that. And, when you take a step back and look at it from that perspective, we're really just in the beginnings.\n\n**Operator**\nYour next question comes from the line of Aaron Rakers with Wells Fargo. Please go ahead. Aaron, your line is open. Your next question comes from Mark Lupacis with Evercore ISI. Please go ahead.\n\n**Mark Lipacis** (Senior Managing Director)\nHi, that's Mark Matapasas. Thanks for taking the question.\n\n**Mark Lipacis** (Senior Managing Director)\nI had a clarification and a question. Colette, up for the clarification. Did you say that enterprise within the data center grew 2x year on year for the January? And if so, does that would that make it faster going than the hyperscalers? And then, Jensen, for you, the question, hyperscalers are the biggest purchasers of your solutions, but they buy equipment for both internal and external workloads, external workloads being cloud services that enterprises use.\n\n**Mark Lipacis** (Senior Managing Director)\nSo, the question is, can you give us a sense of how that hyperscale expense splits between that external workload and internal? And as these new AI workloads and applications come up, would you expect enterprises to become a larger part of that consumption mix and decide to impact how you develop your, service your ecosystem? Thank you.\n\n**Colette Kress** (EVP &amp; CFO)\nSure. Thanks for the question regarding, our enterprise business. Yes, it grew 2x and very similar to what we were seeing, with our large CSPs. Keep in mind, these are both important, areas to understand. Working with the CSPs can be, working on large language models, can be working on inference in their own work.\n\n**Colette Kress** (EVP &amp; CFO)\nBut keep in mind, that is also where the enterprises are surfacing. Your enterprises are both with your CSPs as well as in terms of, building on their own. They're both growing quite, quite well.\n\n**Jensen Huang** (President &amp; CEO)\nThe CSPs are about half of our business. And, and, the CSPs have internal consumption and external consumption, as you say. And we're using of course, used for internal consumption. We we work very closely with all of them to optimize workloads that are internal to them, because they have a large infrastructure of NVIDIA gear that they could take advantage of. And the fact that we could be used for AI on the one hand, video processing on the other hand, data processing like Spark, were fungible.\n\n**Jensen Huang** (President &amp; CEO)\nAnd so, the useful life of our infrastructure is much better. If the useful life is much longer, then the TCO is also lower. And so, the second part is how do we see the growth of enterprise or not CSPs, if you will, going forward? And the answer is, I believe long term it is by far larger. And the reason for that is because if you look at the computer industry today, and what is not served by the computer industry is largely industrial.\n\n**Jensen Huang** (President &amp; CEO)\nSo, So let me give you an example. When we say enterprise, and let's say let's use a car company as an example because they make both soft things and hard things. And so in the case of a car company, the employees would be what we call enterprise. And agentic AI and software planning systems and tools and we have some really exciting things to share with you guys at GTC, those agentic systems are for employees to make employees more productive, to design, to market, to plan, to operate their company. That's agentic AIs.\n\n**Jensen Huang** (President &amp; CEO)\nOn the other hand, the cars that they manufacture also need AI. They need an AI system that trains the cars, treats this entire giant fleet of cars. And today, there's a billion cars on the road. Someday, there'd be a billion cars on the road and every single one of those cars will be robotic cars. And they'll all be collecting data and we'll be improving them, using an AI factory.\n\n**Jensen Huang** (President &amp; CEO)\nWhereas they have a car factory today, in the future they'll have a car factory and an AI factory. And then inside the car itself is a robotic system. And so, as you can see, there are three computers involved. And there's the computer that helps the people, there's the computer that builds the AI for, the machineries, it could be, of course, it could be a tractor, it could be a lawnmower, it could be a humanoid robot that's being developed today, it could be a building, it could be a warehouse. These physical systems require a new type of AI we call physical AI.\n\n**Jensen Huang** (President &amp; CEO)\nThey can't just understand the meaning of words and languages, but they have to understand the meaning of the world. Friction and inertia, object permanence and cause and effect and all of those type of things that are common sense to you and I, but, you know, AIs have to go learn those physical effects. So, we call that physical AI. That whole part of using Agentic AI to revolutionize the way we work inside companies, that's just starting. This is now the beginning of the agentic AI era and you hear a lot of people talking about it and we've got some really great things going on.\n\n**Jensen Huang** (President &amp; CEO)\nAnd then there's the physical AI after that and then there are robotic systems after that. And so, these three computers are all brand new. And my sense is that long term this will be by far a larger of them all, which kind of makes sense. The world of the world's GDP is representing represented by either heavy industries or industrials and companies that are providing for those.\n\n**Operator**\nYour next question comes from the line of Aaron Rickers with Wells Fargo. Please go ahead.\n\n**Aaron Rakers** (Managing Director &amp; Technology Analyst)\nYes. Thanks for letting me back in. Jensen, I'm curious as we now approach the two year anniversary of really the Hopper inflection that you saw in 2023 and Gen AI in general and we think about the roadmap you have in front of us, how do you think about the infrastructure that's been deployed from a replacement cycle perspective and whether if it's GB300 or if it's the Ruben cycle where we start to see maybe some refresh opportunity. I'm just curious to how you look at that.\n\n**Jensen Huang** (President &amp; CEO)\nI appreciate it. First of all, people are still using Voltas and Pascals and Amperes. And the reason for that is because there are always, things that because CUDA is so programmable, you could use it right well, one of the major use cases right now is data processing and data curation. You find a circumstance that an AI model is not very good at. You present that circumstance to a vision language model, let's say.\n\n**Jensen Huang** (President &amp; CEO)\nLet's say it's a car. You present that circumstance to a vision language model. The vision language model actually looks at the circumstances and says, this is what happened and, I wasn't very good at it. You then take that response, the prompt, and you go and prompt an AI model to go find in your whole link of data, other circumstances like that, whatever that circumstance was. And then you use an AI to do, domain randomization and generate a whole bunch of other examples.\n\n**Jensen Huang** (President &amp; CEO)\nAnd then from that, you can go train the model. And so, you could use the the amperes, to go and do data processing and data curation and machine learning based search. And then you create the training dataset which you then present to your hopper systems for training. And so, each one of these architectures are completely they're all CUDA compatible and so everything runs on everything. But if you have infrastructure in place, then you can put the less intensive workloads onto the installed base of the past. All of our CPUs are very well employed.\n\n**Operator**\nWe have time for one more question. And that question comes from Atif Malik with Citi. Please go ahead.\n\n**Atif Malik** (Analyst)\nHi. Thank you for taking my question. I have a follow-up question on gross margins for Colette. Colette, I understand there are many moving parts, the BlackBull yields and ELink 72 and Ethernet mix. And you kind of tiptoed the earlier question if April is the bottom.\n\n**Atif Malik** (Analyst)\nBut second half would have to ramp like 200 basis points per quarter to get to the mid-70s range that you're giving, for the end of the fiscal year. And we still don't know much about tariffs impact to broader semiconductor. So what kind of gives you the confidence in that trajectory in the back half of this year?\n\n**Colette Kress** (EVP &amp; CFO)\nYeah. Thanks for the question. Our gross margins, they're quite complex in terms of the material and everything that we put together in a Blackwell system. Tremendous amount of opportunity to look at a lot of different pieces of that on how we can better improve our gross margins over time. Remember, we have many different configurations as well, on Blackwell that will be able to help us do that.\n\n**Colette Kress** (EVP &amp; CFO)\nSo, together, working, after we get some of these really strong ramping completed for our customers, we can begin a lot of that work. If not, we're going to probably start as soon as possible if we can. And if we can improve it in the short term, we will also do that. Tariffs, at this point, it's a little bit of an unknown. It's an unknown until we understand further, what the U.\n\n**Colette Kress** (EVP &amp; CFO)\nS. Government's plan is, both its timing, its where and how much. So, at this time, we are awaiting. But again, we would, of course, always follow, export controls and or tariffs in that manner.\n\n**Operator**\nLadies and gentlemen, that does conclude our question and answer session. I'm sorry.\n\n**Jensen Huang** (President &amp; CEO)\nThank you.\n\n**Colette Kress** (EVP &amp; CFO)\nNo, no, we're going to open up to, Jensen.\n\n**Jensen Huang** (President &amp; CEO)\nI just want to thank you.\n\n**Jensen Huang** (President &amp; CEO)\nI just want to thank you. Thank you, Colette. Demand for Blackwall is extraordinary. AI is evolving beyond perception and generative AI into reasoning. With reasoning AI, we're observing another scaling law, inference time or test time scaling.\n\n**Jensen Huang** (President &amp; CEO)\nThe more computation, the more the model thinks, the smarter the answer. Models like OpenAI, Broad3, DeepSeq R1 are reasoning models that apply inference time scaling. Reasoning models can consume 100 times more compute. Future reasoning models can consume much more compute. DeepSeq R1 has ignited global enthusiasm.\n\n**Jensen Huang** (President &amp; CEO)\nIt's an excellent innovation, but even more importantly, it has open sourced a world class reasoning AI model. Nearly every AI developer is applying R1 or chain of thought and reinforcement learning techniques like R1 to scale their model's performance. We now have three scaling laws, as I mentioned earlier, driving the demand for AI computing. The traditional scaling laws of AI remains intact. Foundation models are being enhanced with multi modality and pre training is still growing.\n\n**Jensen Huang** (President &amp; CEO)\nBut it's no longer enough. We have two additional scaling dimensions. Post training scaling, where reinforcement learning, fine tuning, model distillation require orders of magnitude more compute than pre training alone. Inference time scaling and reasoning, where a single query can demand 100 times more compute. We designed Blackwell for this moment, a single platform that can easily transition from pre training, post training and test time scaling.\n\n**Jensen Huang** (President &amp; CEO)\nBlackwell's FP4 transformer engine and NVLink 72 scale up fabric and new software technologies let Blackwell process reasoning AI models 25 times faster than Hopper. Blackwell in all of its configurations is in full production. Each Grace Blackwell NVLink72 rack is an engineering marvel. 1,500,000 components produced across three fifty manufacturing sites by nearly 100,000 factory operators. AI is advancing at light speed.\n\n**Jensen Huang** (President &amp; CEO)\nWe're at the beginning of reasoning AI and inference time scaling. But we're just at the start of the age of AI. Multimodal AI, enterprise AI, sovereign AI and physical AI are right around the corner. We will grow strongly in 2025. Going forward, data centers will dedicate most of CapEx to accelerated computing and AI.\n\n**Jensen Huang** (President &amp; CEO)\nData centers will increasingly become AI factories and every company will have them either rented or self operated. I want to thank all of you for joining us today. Come join us at GTC in a couple of weeks. We're going to be talking about Blackwell Ultra, Rubin and other new computing, networking, reasoning AI, physical AI products and a whole bunch more. Thank you.\n\n**Operator**\nThis concludes today's conference call. You may now disconnect.",
      "fetched_at": "2026-02-04T15:38:27.029Z"
    },
    {
      "ticker": "NVDA",
      "title": "NVIDIA Corporation (NVDA) Q3 FY2025 earnings call transcript",
      "published_date": "Nov 20, 2024, 5:00 PM EST",
      "fiscal_year": "2025",
      "quarter": "Q3",
      "url": "https://finance.yahoo.com/quote/NVDA/earnings/NVDA-Q3-2025-earnings_call-225518.html",
      "content": "**Operator**\nGood afternoon. My name is Jay Lunn, I'll be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's third quarter earnings call. All lines have been placed on mute to prevent any background noise. After the speaker's remarks, there will be a question-and-answer session. If you would like to ask a question during this time, simply press star followed by the number one on your telephone keypad. If you would like to withdraw your question, press the star one again. Thank you. Stewart Stecker, you may begin your conference.\n\n**Stewart Stecker** (Head of Investor Relations)\nThank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2025. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer, and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter of fiscal 2025. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially.\n\n**Stewart Stecker** (Head of Investor Relations)\nFor a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10K and 10Q, and the reports that we may file on form 8K with the Securities and Exchange Commission. All our statements are made as of today, November 20, 2024, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\n\n**Colette Kress** (EVP and CFO)\nThank you, Stewart. Q3 was another record quarter. We continue to deliver incredible growth. Revenue of $35.1 billion was up 17% sequentially and up 94% year on year, and well above our outlook of $32.5 billion. All market platforms posted strong sequential and year-over-year growth fueled by the adoption of NVIDIA Accelerated Computing and AI. Starting with data center, another record was achieved in data center. Revenue of $30.8 billion, up 17% sequential and up 112% year on year. NVIDIA Hopper demand is exceptional, and sequentially, NVIDIA H200 sales increased significantly to double-digit billions, the fastest product ramp in our company's history. The H200 delivers up to 2X faster inference performance and up to 50% improved TCO. Cloud service providers were approximately half of our data center sales, with revenue increasing more than 2X year on year.\n\n**Colette Kress** (EVP and CFO)\nCSPs deployed NVIDIA H200 infrastructure and high-speed networking with installations scaling to tens of thousands of DPUs to grow their business and serve rapidly rising demand for AI training and inference workloads. NVIDIA H200-powered cloud instances are now available from AWS, CoreWeave, and Microsoft Azure, with Google Cloud and OCI coming soon. Alongside significant growth from our large CSPs, NVIDIA GPU regional cloud revenue jumped 2X year on year as North America, India, and Asia-Pacific regions ramped NVIDIA cloud instances and sovereign cloud buildouts. Consumer internet revenue more than doubled year on year as companies scaled their NVIDIA Hopper infrastructure to support next-generation AI models, training, multimodal and agentic AI, deep learning recommender engines, and generative AI inference and content creation workloads. NVIDIA's Ampere and Hopper infrastructures are fueling inference revenue growth for customers. NVIDIA is the largest inference platform in the world.\n\n**Colette Kress** (EVP and CFO)\nOur large install base and rich software ecosystem encourage developers to optimize for NVIDIA and deliver continued performance and TCO improvements. Rapid advancements in NVIDIA software algorithms boosted Hopper inference throughput by an incredible 5X in one year and cut time to first token by 5X. Our upcoming release of NVIDIA NIM will boost Hopper inference performance by an additional 2.4X. Continuous performance optimizations are a hallmark of NVIDIA and drive increasingly economic returns for the entire NVIDIA installed base. Blackwell is in full production after a successfully executed mass change. We shipped 13,000 GPU samples to customers in the third quarter, including one of the first Blackwell DGX engineering samples to OpenAI. Blackwell is a full-stack, full-infrastructure AI data center scale system with customizable configurations needed to address a diverse and growing AI market.\n\n**Colette Kress** (EVP and CFO)\nFrom x86 to ARM, training to inferencing GPUs, InfiniBand to Ethernet switches, and NVLink, and from liquid-cooled to air-cooled. Every customer is racing to be the first to market. Blackwell is now in the hands of all of our major partners, and they are working to bring up their data centers. We are integrating Blackwell systems into the diverse data center configurations of our customers. Blackwell demand is staggering, and we are racing to scale supply to meet the incredible demand customers are placing on us. Customers are gearing up to deploy Blackwell at scale. Oracle announced the world's first ZetaScale AI cloud computing clusters that can scale to over 131,000 Blackwell GPUs to help enterprises train and deploy some of the most demanding next-generation AI models.\n\n**Colette Kress** (EVP and CFO)\nYesterday, Microsoft announced they will be the first CSP to offer in private preview Blackwell-based cloud instances powered by NVIDIA GB200 and Quantum InfiniBand. Last week, Blackwell made its debut on the most recent round of MLPerf training results, sweeping the Perf GPU benchmarks and delivering a 2.2X leap in performance over Hopper. The results also demonstrate our relentless pursuit to drive down the cost of compute. Just 64 Blackwell GPUs are required to run the GPT-3 benchmark compared to 256 H100s for a 4X reduction in cost. NVIDIA Blackwell architecture with NVLink switch enables up to 30X faster inference performance and a new level of inference scaling throughput and response time that is excellent for running new reasoning inference applications like OpenAI's O1 model. With every new platform shift, a wave of startups is created. Hundreds of AI-native companies are already delivering AI services with great success.\n\n**Colette Kress** (EVP and CFO)\nThough Google, Meta, Microsoft, and OpenAI are the headliners, and Anthropic, Perplexity, Mistral, Adobe Firefly, Runway, Midjourney, Lightricks, Harvey, Codium, Cursor, and Bridge are seeing great success while thousands of AI-native startups are building new services. The next waves of AI are enterprise AI and industrial AI. Enterprise AI is in full throttle. NVIDIA AI Enterprise, which includes NVIDIA NeMo and NIM microservices, is an operating platform of agentic AI. Industry leaders are using NVIDIA AI to build copilots and agents. Working with NVIDIA, Cadence, Cloudera, Cohesity, NetApp, Nutanix, Salesforce, SAP, and ServiceNow are racing to accelerate development of these applications with the potential for billions of agents to be deployed in the coming years.\n\n**Colette Kress** (EVP and CFO)\nConsulting leaders like Accenture and Deloitte are taking NVIDIA AI to the world's enterprises. Accenture launched a new business group with 30,000 professionals trained on NVIDIA AI technology to help facilitate this global buildout.\n\n**Colette Kress** (EVP and CFO)\nAdditionally, Accenture, with over 770,000 employees, is leveraging NVIDIA-powered agentic AI applications internally, including one case that cuts manual steps in marketing campaigns by 25% to 35%. Nearly 1,000 companies are using NVIDIA NIM, and the speed of its uptake is evident in NVIDIA AI Enterprise monetization. We expect NVIDIA AI Enterprise full-year revenue to increase over 2X from last year, and our pipeline continues to build. Overall, our software, service, and support revenue is annualizing at $1.5 billion, and we expect to exit this year annualizing at over $2 billion. Industrial AI and robotics are accelerating. This is triggered by breakthroughs in physical AI foundation models that understand the physical world, like NVIDIA NeMo for enterprise AI agents. We built NVIDIA Omniverse for developers to build, train, and operate industrial AI and robotics.\n\n**Colette Kress** (EVP and CFO)\nSome of the largest industrial manufacturers in the world are adopting NVIDIA Omniverse to accelerate their businesses, automate their workflows, and to achieve new levels of operating efficiency. Foxconn, the world's largest electronics manufacturer, is using digital twins and industrial AI built on NVIDIA Omniverse to speed the bring-up of its Blackwell factories and drive new levels of efficiency. In its Mexico facility alone, Foxconn expects a reduction of over 30% in annual kilowatt-hour usage. From a geographic perspective, our data center revenue in China grew sequentially due to shipments of export-compliant Hopper products to industries. As a percentage of total data center revenue, it remains well below levels prior to the onset of export controls. We expect the market in China to remain very competitive going forward. We will continue to comply with export controls while serving our customers.\n\n**Colette Kress** (EVP and CFO)\nOur sovereign AI initiatives continue to gather momentum as countries embrace NVIDIA Accelerated Computing for a new industrial revolution powered by AI. India's leading CSPs, including Tata Communications and Yotta Data Services, are building AI factories for tens of thousands of NVIDIA GPUs. By year-end, they will have boosted NVIDIA GPU deployments in the country by nearly 10X. Infosys, TCS, Wipro are adopting NVIDIA AI Enterprise and upskilling nearly 500,000 developers and consultants to help clients build and run AI agents on our platform. In Japan, SoftBank is building the nation's most powerful AI supercomputer with NVIDIA DGX Blackwell and Quantum InfiniBand. SoftBank is also partnering with NVIDIA to transform the telecommunications network into a distributed AI network with NVIDIA AI Aerial and ARAN platform that can process 5G RAN on AI and CUDA. We are launching the same in the U.S. with T-Mobile.\n\n**Colette Kress** (EVP and CFO)\nLeaders across Japan, including Fujitsu, NEC, and NTT, are adopting NVIDIA AI Enterprise and major consulting companies, including EY Strategy and Consulting, to help bring NVIDIA AI technology to Japan's industries. Networking revenue increased 20% year on year. Areas of sequential revenue growth include InfiniBand and Ethernet switches, SmartNICs, and BlueField DPUs. Though networking revenue was sequentially down, networking demand is strong and growing, and we anticipate sequential growth in Q4. CSPs and supercomputing centers are using and adopting the NVIDIA InfiniBand platform to power new H200 clusters. NVIDIA Spectrum-X Ethernet for AI revenue increased over 3X year on year, and our pipeline continues to build with multiple CSPs and consumer internet companies planning large cluster deployments. Traditional Ethernet was not designed for AI. NVIDIA Spectrum-X uniquely leverages technology previously exclusive to InfiniBand to enable customers to achieve massive scale of their GPU compute.\n\n**Colette Kress** (EVP and CFO)\nUtilizing Spectrum-X, xAI's Colossus 100,000 Hopper supercomputer experienced zero application latency degradation and maintained 95% data throughput versus 60% for traditional Ethernet. Now moving to gaming and AI PCs. Gaming revenue of $3.3 billion increased 14% sequentially and 15% year on year. Q3 was a great quarter for gaming with notebook, console, and desktop revenue, all growing sequentially and year on year. RTX 40 demand was fueled by strong back-to-school sales as consumers continued to choose GeForce RTX GPUs and devices to power gaming, creative, and AI applications. Channel inventory remains healthy, and we are gearing up for the holiday season.\n\n**Colette Kress** (EVP and CFO)\nWe began shipping new GeForce RTX AI PCs with up to 321 AI TOPS from ASUS and MSI, with Microsoft's Copilot+ capabilities anticipated in Q4. These machines harness the power of RTX ray tracing and AI technologies to supercharge gaming, photo and video editing, image generation, and coding.\n\n**Colette Kress** (EVP and CFO)\nThis past quarter, we celebrated the 25th anniversary of the GeForce 256, the world's first GPU. From transforming computing graphics to igniting the AI revolution. NVIDIA's GPUs have been the driving force behind some of the most consequential technologies of our time. Moving to ProViz. Revenue of $486 million was up 7% sequentially and 17% year on year. NVIDIA RTX workstations continue to be the preferred choice to power professional graphics, design, and engineering-related workloads. Additionally, AI is emerging as a powerful demand driver, including autonomous vehicle simulation, generative AI model prototyping for productivity-related use cases, and generative AI content creation in media and entertainment. Moving to automotive. Revenue was a record $449 million, up 30% sequentially and up 72% year on year. Strong growth was driven by self-driving ramps of NVIDIA Orin and robust end-market demand for NEVs.\n\n**Colette Kress** (EVP and CFO)\nVolvo Cars is rolling out its fully electric SUV built on NVIDIA Orin and Drive OS. Okay, moving to the rest of the P&L. GAAP gross margin was 74.6% and non-GAAP gross margin was 75%. Down sequentially, primarily driven by a mix shift of the H100 systems to more complex and higher-cost systems within data center. Sequentially, GAAP operating expenses and non-GAAP operating expenses were up 9% due to higher compute, infrastructure, and engineering development costs for new product introductions. In Q3, we returned $11.2 billion to shareholders in the form of share repurchases and cash dividends. Well, let me turn to the outlook for the fourth quarter. Total revenue is expected to be $37.5 billion, plus or minus 2%, which incorporates continued demand for Hopper architecture and the initial ramp of our Blackwell products.\n\n**Colette Kress** (EVP and CFO)\nWhile demand is greatly exceeding supply, we are on track to exceed our previous Blackwell revenue estimate of several billion dollars as our visibility into supply continues to increase. On gaming, although sell-through was strong in Q3, we expect fourth quarter revenue to decline sequentially due to supply constraints. GAAP and non-GAAP gross margins are expected to be 73% and 73.5% respectively, plus or minus 50 basis points. Blackwell is a customizable AI infrastructure with seven different types of NVIDIA-built chips, multiple networking options, and for air and liquid-cooled data centers. Our current focus is on ramping to strong demand, increasing system availability, and providing the optimal mix of configurations to our customer. As Blackwell ramps, we expect gross margins to moderate to the low 70s. When fully ramped, we expect Blackwell margins to be in the mid-70s.\n\n**Colette Kress** (EVP and CFO)\nGAAP and non-GAAP operating expenses are expected to be approximately $4.8 billion and $3.4 billion respectively. We are a data center-scale AI infrastructure company. Our investments include building data centers for the development of our hardware and software stacks and to support new introductions. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $400 million, excluding gains and losses from non-affiliated investments. GAAP and non-GAAP tax rates are expected to be 16.5%, plus or minus 1%, excluding any discrete items. Further financial details are included in the CFO commentary and other information available on our IR websites. In closing, let me highlight upcoming events for the financial community. We will be attending the UBS Global Technology and AI Conference on December 3rd in Scottsdale.\n\n**Colette Kress** (EVP and CFO)\nPlease join us at CES in Las Vegas, where Jensen will deliver a keynote on January 6th, and we will host a Q&A session for financial analysts the next day on January 7th. Our earnings call to discuss results for the fourth quarter of fiscal 2025 is scheduled for February 26, 2025. We will now open the call for questions. Operator, can you poll for questions, please?\n\n**Operator**\nAt this time, I would like to remind everyone in order to ask a question, press star, then the number one on your telephone keypad. We'll pause for just a moment to compile the Q&A roster. As a reminder, please limit yourself to one question. Your first question comes from the line of CJ Muse of Cantor Fitzgerald. Your line is open.\n\n**C.J. Muse** (Senior Managing Director)\nYeah, good afternoon. Thank you for taking the question. I guess just a question for you on the debate around whether scaling for large language models have stalled. Obviously, we're very early here, but we'd love to hear your thoughts on this front. How are you helping your customers as they work through these issues? And then obviously, part of the context here is we're discussing clusters that have yet to benefit from Blackwell. So is this driving even greater demand for Blackwell? Thank you.\n\n**Jensen Huang** (President and CEO)\nFoundation model pre-training scaling is intact, and it's continuing. As you know, this is an empirical law, not a fundamental physical law, but the evidence is that it continues to scale. What we're learning, however, is that it's not enough, that we've now discovered two other ways to scale. One is post-training scaling.\n\n**Jensen Huang** (President and CEO)\nOf course, the first generation of post-training was reinforcement learning human feedback, but now we have reinforcement learning AI feedback and all forms of synthetic data-generated data that assists in post-training scaling. And one of the biggest events and one of the most exciting developments is Strawberry, ChatGPT o1, OpenAI's o1, which does inference time scaling, or what's called test time scaling.\n\n**Jensen Huang** (President and CEO)\nThe longer it thinks, the better and higher quality answer it produces. And it considers approaches like chain of thought and multi-path planning and all kinds of techniques necessary to reflect and so on and so forth. And intuitively, it's a little bit like us doing thinking in our head before we answer a question. And so we now have three ways of scaling, and we're seeing all three ways of scaling. And as a result of that, the demand for our infrastructures is really great.\n\n**Jensen Huang** (President and CEO)\nYou see now that at the tail end of the last generation of foundation models, we're at about 100,000 Hoppers. The next generation starts at 100,000 Blackwells. And so that kind of gives you a sense of where the industry is moving with respect to pre-training scaling, post-training scaling, and then now, very importantly, inference time scaling. And so the demand is really great for all of those reasons. But remember, simultaneously, we're seeing inference really starting to scale up for our company.\n\n**Jensen Huang** (President and CEO)\nWe are the largest inference platform in the world today because our installed base is so large, and everything that was trained on Ampere's and Hopper's inference incredibly on Ampere's and Hopper's. And as we move to Blackwells for training foundation models, it leaves behind it a large installed base of extraordinary infrastructure for inference. And so we're seeing inference demand go up. We're seeing inference time scaling go up. We see the number of AI-native companies continue to grow. And of course, we're starting to see enterprise adoption of agentic AI really is the latest rage. And so we're seeing a lot of demand coming from a lot of different places.\n\n**Operator**\nYour next question comes from the line of Toshiya Hari of Goldman Sachs. Your line is open.\n\n**Toshiya Hari** (Managing Director)\nHi, good afternoon. Thank you so much for taking the question. Jensen, you executed the mask change earlier this year. There were some reports over the weekend about some heating issues. On the back of this, we've had investors ask about your ability to execute to the roadmap you presented at GTC this year with Ultra coming out next year and the transition to Rubin in 2026. Can you sort of speak to that? And some investors are questioning that.\n\n**Toshiya Hari** (Managing Director)\nSo if you can sort of speak to your ability to execute on time, that would be super helpful. And then a quick Part B on supply constraints. Is it a multitude of componentry that's causing this, or is it specifically CoWoS, HBM? Is the supply constraints—are the supply constraints getting better? Are they worsening? Any sort of color on that would be super helpful as well. Thank you.\n\n**Jensen Huang** (President and CEO)\nYeah, thanks. Thanks. So let's see. Back to the first question. Blackwell production is in full steam. In fact, as Colette mentioned earlier, we will deliver this quarter more Blackwells than we had previously estimated. And so the supply chain team is doing an incredible job working with our supply partners to increase Blackwell. And we're going to continue to work hard to increase Blackwell through next year. It is the case that demand exceeds our supply.\n\n**Jensen Huang** (President and CEO)\nThat's expected as we're in the beginnings of this generative AI revolution, as we all know. We're at the beginning of a new generation of foundation models that are able to do reasoning and able to do long thinking. Of course, one of the really exciting areas is physical AI, AI that now understands the structure of the physical world. Blackwell demand is very strong. Our execution is going well. There's obviously a lot of engineering that we're doing across the world. You see now systems that are being stood up by Dell and CoreWeave. I think you saw systems from Oracle stood up. You have systems from Microsoft, and they're about to preview their Grace Blackwell systems. You have systems that are at Google. All of these CSPs are racing to be first.\n\n**Jensen Huang** (President and CEO)\nThe engineering that we do with them is, as you know, rather complicated. The reason for that is because although we build full stack and full infrastructure, we disaggregate all of this AI supercomputer, and we integrate it into all of the custom data centers and architectures around the world. That integration process is something we've done several generations now. We're very good at it, but still, there's a lot of engineering that happens at this point. As you see from all of the systems that are being stood up, Blackwell's in great shape. As we mentioned earlier, the supply and what we're planning to ship this quarter is greater than our previous estimates. With respect to the supply chain, there are seven different chips, seven custom chips that we built in order for us to deliver the Blackwell systems.\n\n**Jensen Huang** (President and CEO)\nThe Blackwell systems go in air-cooled or liquid-cooled, NVLink 8 or NVLink 72, or NVLink 8, NVLink 36, NVLink 72. We have x86 or Grace, and the integration of all of those systems into the world's data centers is nothing short of a miracle, and so the component supply chain necessary to ramp at the scale, you have to go back and take a look at how much Blackwell was shipped last quarter, which was zero, and in terms of how much Blackwell total systems were shipped this quarter, which is measured in billions, the ramp is incredible, and so almost every company in the world seems to be involved in our supply chain, and we've got great partners, everybody from, of course, TSMC and Amphenol, the connector company, incredible company, Vertiv and SK Hynix and Micron, and Amkor, and KYEC.\n\n**Jensen Huang** (President and CEO)\nAnd there's Foxconn and the factories that they've built and Quanta and Wiwynn and, gosh, Dell and HP and Supermicro, Lenovo. And the number of companies is just really quite incredible. Quanta. And I'm sure I've missed partners that are involved in the ramping of Blackwell, which I really appreciate. And so anyways, I think we're in great shape with respect to the Blackwell ramp at this point. And then lastly, your question about our execution of our roadmap. We're on an annual roadmap, and we're expecting to continue to execute on our annual roadmap. And by doing so, we increase the performance, of course, of our platform. But it's also really important to realize that when we're able to increase performance and do so at X factors at a time, we're reducing the cost of training. We're reducing the cost of inferencing.\n\n**Jensen Huang** (President and CEO)\nWe're reducing the cost of AI so that it could be much more accessible. But the other factor that's very important to note is that when there's a data center of some fixed size, and a data center always is of some fixed size. It could be, of course, tens of megawatts in the past, and now most data centers are now 100 megawatts to several hundred megawatts, and we're planning on gigawatt data centers. It doesn't really matter how large the data centers are. The power's limited. And when you're in the power-limited data center, the highest performance per watt translates directly into the highest revenues for our partners. And so on the one hand, our annual roadmap reduces cost.\n\n**Jensen Huang** (President and CEO)\nBut on the other hand, because our per watt is so good compared to anything out there, we generate for our customers the greatest possible revenues. And so that annual rhythm is really important to us, and we have every intention of continuing to do that. And everything's on track as far as I know.\n\n**Operator**\nYour next question comes from the line of Timothy Arcuri of UBS. Your line is open.\n\n**Timothy Arcuri** (Managing Director)\nThanks a lot. I'm wondering if you can talk about the trajectory of how Blackwell is going to ramp this year. I know, Jensen, you did just talk about Blackwell being better than I think you had said several billions of dollars in January. It sounds like you're going to do more than that. But I think in recent months, also, you said that Blackwell crosses over Hopper in the April quarter. So I guess I had two questions. First of all, is that still the right way to think about it, that Blackwell will cross over Hopper in April?\n\n**Timothy Arcuri** (Managing Director)\nAnd then, Colette, you kind of talked about Blackwell bringing down gross margin to the low 70s% as it ramps. So I guess if April is the crossover, is that the worst of the pressure on gross margin? So you're going to be kind of in the low 70s% as soon as April. I'm just wondering if you can sort of shape that for us. Thanks.\n\n**Jensen Huang** (President and CEO)\nColette, why don't you start?\n\n**Colette Kress** (EVP and CFO)\nSure. Let me first start with your question, Tim. Thank you regarding our gross margins. And we discussed that our gross margins, as we are ramping Blackwell in the very beginning, and the many different configurations, the many different chips that we are bringing to market, we are going to focus on making sure we have the best experience for our customers as they stand that up.\n\n**Colette Kress** (EVP and CFO)\nWe will start growing in for our gross margins, but we do believe those will be in the low 70s in that first part of the ramp. So you're correct. As you look at the quarters following after that, we will start increasing our gross margins, and we hope to get to the mid-70s quite quickly as part of that ramp.\n\n**Jensen Huang** (President and CEO)\nHopper demand will continue through next year, surely the first several quarters of the next year. And meanwhile, we'll ship more Blackwells next quarter than this, and we'll ship more Blackwells the quarter after that than our first quarter. And so that kind of puts it in perspective. We are really at the beginnings of two fundamental shifts in computing that is really quite significant. The first is moving from coding that runs on CPUs to machine learning that creates neural networks that runs on GPUs.\n\n**Jensen Huang** (President and CEO)\nThat fundamental shift from coding to machine learning is widespread at this point. There are no companies who are not going to do machine learning. And so machine learning is also what enables generative AI. And so on the one hand, the first thing that's happening is $1 trillion worth of computing systems and data centers around the world is now being modernized for machine learning. On the other hand, secondarily, I guess, is that on top of these systems, we're going to be creating a new type of capability called AI. And when we say generative AI, we're essentially saying that these data centers are really AI factories. They're generating something. Just like we generate electricity, we're now going to be generating AI.\n\n**Jensen Huang** (President and CEO)\nIf the number of customers is large, just as the number of consumers of electricity is large, these generators are going to be running 24/7. Today, many AI services are running 24/7, just like an AI factory. We're going to see this new type of system come online. I call it an AI factory because that's really as close to what it is. It's unlike a data center of the past. These two fundamental trends are really just beginning. We expect this to happen, this growth, this modernization, and the creation of a new industry to go on for several years.\n\n**Operator**\nYour next question comes from the line of Vivek Arya of Bank of America Securities. Your line is open. Thanks for taking my question.\n\n**Vivek Arya** (Managing Director)\nColette, just to clarify, do you think it's a fair assumption to think NVIDIA could recover to kind of mid-70s gross margin in the back half of calendar 2025? Just wanted to clarify that. And then, Jensen, my main question, historically, when we have seen hardware deployment cycles, they have inevitably included some digestion along the way. When do you think we get to that phase, or is it just too premature to discuss that because you're just at the start of Blackwell? So how many quarters of shipments do you think is required to kind of satisfy this first wave? Can you continue to grow this into calendar 2026? Just how should we be prepared to see what we have seen historically, right, the periods of digestion along the way of long-term kind of secular hardware deployment?\n\n**Colette Kress** (EVP and CFO)\nOkay. Vivek, thank you for the question. Let me clarify your question regarding gross margins. Could we reach the mid-70s in the second half of next year, and yes, I think it is a reasonable assumption or a goal for us to do, but we'll just have to see how that mix of ramp goes, but yes, it is definitely possible.\n\n**Jensen Huang** (President and CEO)\nThe way to think through that, Vivek, is I believe that there will be no digestion until we modernize $1 trillion worth of data centers. If you just look at the world's data centers, the vast majority of it is built for a time when we wrote applications by hand, and we ran them on CPUs. It's just not a sensible thing to do anymore.\n\n**Jensen Huang** (President and CEO)\nIf every company's CapEx, if they're ready to build a data center tomorrow, they ought to build it for a future of machine learning and generative AI because they have plenty of old data centers, and so what's going to happen over the course of next X number of years, and let's assume that over the course of four years, the world's data centers could be modernized as we grow into IT. As you know, IT continues to grow about 20%-30% a year, let's say, and so let's say by 2030, the world's data centers for computing is, call it a couple trillion dollars. We have to grow into that. We have to modernize the data center from coding to machine learning. That's number one. The second part of it is generative AI.\n\n**Jensen Huang** (President and CEO)\nWe're now producing a new type of capability that the world's never known, a new market segment that the world's never had. If you look at OpenAI, it didn't replace anything. It's something that's completely brand new. In a lot of ways, as when the iPhone came, it was completely brand new. It wasn't really replacing anything. We're going to see more and more companies like that. They're going to create and generate out of their services, essentially, intelligence. Some of it would be digital artist intelligence like Runway. Some of it would be basic intelligence like OpenAI. Some of it would be legal intelligence like Harvey. Digital marketing intelligence like Writer. So on and so forth. The number of these companies, what are they called, AI-native companies, are just in hundreds. Almost every platform shift, there were internet companies, as you recall.\n\n**Jensen Huang** (President and CEO)\nThere were cloud-first companies. There were mobile-first companies. Now they're AI natives. And so these companies are being created because people see that there's a platform shift, and there's a brand new opportunity to do something completely new. And so my sense is that we're going to continue to build out, to modernize IT, modernize computing, number one. And then number two, create these AI factories that are going to be for a new industry for the production of artificial intelligence.\n\n**Operator**\nYour next question comes from the line of Stacy Rasgon of Bernstein Research. Your line is open.\n\n**Stacy Rasgon** (Senior Analyst)\nHi, guys. Thanks for taking my questions. Colette, I had a clarification and a question for you. The clarification is when you say low 70s% gross margins, is 73.5% count as low 70s%, or do you have something else in mind? And for my question, you're guiding total revenues.\n\n**Stacy Rasgon** (Senior Analyst)\nI mean, total data center revenues in the next quarter must be up, quote-unquote, \"several billion dollars,\" but it sounds like Blackwell now should be up more than that. But you also said Hopper was still strong. So is Hopper down sequentially next quarter? And if it is, why? Is it because of the supply constraints? China has been pretty strong. Is China kind of rolling off a bit into Q4? So any color you can give us on sort of the Blackwell ramp and the Blackwell versus Hopper behavior into Q4 would be really helpful. Thank you.\n\n**Colette Kress** (EVP and CFO)\nFirst, starting on your first question there, Stacy, regarding our gross margin and defining low. Low, of course, is below the mids. And let's say we might be at 71%, maybe about 72%, 72 and a half. We're going to be in that range.\n\n**Colette Kress** (EVP and CFO)\nWe could be higher than that as well. We're just going to have to see how it comes through. We do want to make sure that we are ramping and continuing that improvement, the improvement in terms of our yields, the improvement in terms of the product as we go through the rest of the year. So we'll get up to the mid-70s by that point. The second statement was a question regarding our Hopper. And what is our Hopper doing? We have seen substantial growth for H200, not only in terms of orders, but the quickness in terms of those that are standing that up. It is an amazing product, and it's the fastest growing and ramping that we've seen. We will continue to be selling Hopper in this quarter in Q4 for sure.\n\n**Colette Kress** (EVP and CFO)\nThat is across the board in terms of all of our different configurations, and our configurations include what we may do in terms of China. But keep that in mind that folks are also at the same time looking to build out their Blackwell. So we've got a little bit of both happening in Q4. But yes, is it possible for Hopper to grow between Q3 and Q4? It's possible, but we'll just have to see.\n\n**Operator**\nYour next question comes from the line of Joseph Moore of Morgan Stanley. Your line is open.\n\n**Joseph Moore** (Managing Director)\nGreat. Thank you. I wonder if you could talk a little bit about what you're seeing in the inference market. You've talked about Strawberry and some of the ramifications of longer scaling inference projects.\n\n**Joseph Moore** (Managing Director)\nBut you've also talked about the possibility that as some of these Hopper clusters age, that you could use some of the Hopper latent chips for inference. So I guess, do you expect inference to outgrow training in the next kind of 12-month timeframe? And just generally, your thoughts there.\n\n**Jensen Huang** (President and CEO)\nOur hopes and dreams is that someday the world does a ton of inference. And that's when AI has really succeeded. It's when every single company is doing inference inside their companies for the marketing department and forecasting department and supply chain group and their legal department and engineering, of course, and coding, of course. And so we hope that every company is doing inference 24/7 and that there will be a whole bunch of AI-native startups, thousands of AI-native startups that are generating tokens and generating AI.\n\n**Jensen Huang** (President and CEO)\nEvery aspect of your computer experience, from using Outlook to PowerPointing or when you're sitting there with Excel, you're constantly generating tokens. Every time you read a PDF, open a PDF, it generated a whole bunch of tokens. One of my favorite applications is NotebookLM, this Google application that came out. I used the living daylights out of it just because it's fun. I put every PDF, every archive paper into it just to listen to it as well as scanning through it. So I think that's the goal, is to train these models so that people use it. There's now a whole new era of AI, if you will, a whole new genre of AI called physical AI. Just those large language models understand the human language and the thinking process, if you will. Physical AI understands the physical world.\n\n**Jensen Huang** (President and CEO)\nIt understands the meaning of the structure and understands what's sensible and what's not, and what could happen and what wouldn't. Not only does it understand, but it can predict and roll out a short future. That capability is incredibly valuable for industrial AI and robotics. That's fired up so many AI-native companies, robotics companies, and physical AI companies that you're probably hearing about. It's really the reason why we built Omniverse. Omniverse is so that we can enable these AIs to be created and learn in Omniverse and learn from synthetic data generation and reinforcement learning physics feedback instead of human feedback. It's now physics feedback. To have these capabilities, Omniverse was created so that we can enable physical AI. The goal is to generate tokens. The goal is to inference. We're starting to see that growth happening.\n\n**Jensen Huang** (President and CEO)\nSo I'm super excited about that. Now, let me just say one more thing. Inference is super hard. And the reason why inference is super hard is because you need the accuracy to be high on the one hand. You need the throughput to be high so that the cost could be as low as possible. But you also need the latency to be low. And computers that are high throughput as well as low latency is incredibly hard to build. And these applications have long context lengths because they want to understand. They want to be able to inference within understanding the context of what they're being asked to do. And so the context length is growing larger and larger. On the other hand, the models are getting larger. They're multi-modality. Just the number of dimensions that inference is innovating is incredible.\n\n**Jensen Huang** (President and CEO)\nThis innovation rate is what makes NVIDIA's architecture so great because our ecosystem is fantastic. Everybody knows that if they innovate on top of CUDA and on top of NVIDIA's architecture, they can innovate more quickly, and they know that everything should work. And if something were to happen, it's probably likely their code and not ours. And so that ability to innovate in every single direction at the same time, having a large install base so that whatever you create could land on an NVIDIA computer and be deployed broadly all around the world in every single data center, all the way out to the edge into robotic systems, that capability is really quite phenomenal.\n\n**Operator**\nYour next question comes from the line of Aaron Rakers. Sorry, Aaron Rakers of Wells Fargo. Your line is open.\n\n**Aaron Rakers** (Managing Director)\nYeah. Thanks for taking the question. I wanted to ask you, as we kind of focus on the Blackwell cycle and think about the data center business, when I look at the results this last quarter, Colette, you mentioned that obviously the networking business was down about 15% sequentially. But then your comments were that you were seeing very strong demand. You mentioned also that you had multiple cloud CSP design wins for these large-scale clusters. So I'm curious if you could unpack what's going on in the networking business and where maybe you've seen some constraints and just your confidence in the pace of Spectrum-X progressing to that multiple billions of dollars that you previously had talked about. Thank you.\n\n**Colette Kress** (EVP and CFO)\nLet's first start with the networking. The growth year over year is tremendous.Our focus since the beginning of our acquisition of Mellanox has really been about building together the work that we do in terms of in the data center. The networking is such a critical part of that. Our ability to sell our networking with many of our systems that we are doing in data center is continuing to grow and do quite well. So this quarter is just a slight dip down, and we're going to be right back up in terms of growing. They're getting ready for Blackwell and more and more systems that will be using not only our existing networking, but also the networking that is going to be incorporated in a lot of these large systems that we are providing them to.\n\n**Operator**\nYour next question comes from the line of Atif Malik of Citi. Your line is open.\n\n**Atif Malik** (Managing Director)\nThank you for taking my question.I have two quick ones for Colette. Colette, on the last earnings call, you mentioned that sovereign demand is in low double-digit billions. Can you provide an update on that? And then can you explain the supply constraint situation in gaming? Is that because you're shifting your supply towards data center?\n\n**Colette Kress** (EVP and CFO)\nSo first, starting in terms of sovereign AI, such an important part of growth, something that has really surfaced with the onset of generative AI and building models in the individual countries around the world. And we see a lot of them, and we talked about a lot of them in the call today and the work that they are doing.\n\n**Colette Kress** (EVP and CFO)\nSo our sovereign AI and our pipeline going forward is still absolutely intact as those are working to build these foundational models in their own language, in their own culture, and working in terms of the enterprises within those countries. And I think you'll continue to see this be a growth opportunity that you may see with our regional clouds that are being stood up and/or those that are focusing in terms of AI factories for many parts of the sovereign AI. This is areas where this is growing not only in terms of in Europe, but you're also seeing this in terms of growth in terms of in the Asia-Pacific as well.\n\n**Colette Kress** (EVP and CFO)\nLet me flip to your second question that you asked regarding gaming. So our gaming right now from a supply, we're busy trying to make sure that we can ramp all of our different products.\n\n**Colette Kress** (EVP and CFO)\nAnd in this case, our gaming supply, given what we saw selling through, was moving quite fast. Now, the challenge that we have is how fast could we get that supply getting ready into the market for this quarter? Not to worry, I think we'll be back on track with more supply as we turn the corner into the new calendar year. We're just going to be tight for this quarter.\n\n**Operator**\nYour next question comes from the line of Ben Reitzes of Melius Research. Your line is open.\n\n**Ben Reitzes** (Managing Director)\nYeah. Hi. Thanks a lot for the question. I wanted to ask Colette and Jensen with regard to sequential growth. So very strong sequential growth this quarter, and you're guiding to about 7%. Do your comments on Blackwell imply that we re-accelerate from there as you get more supply?\n\n**Ben Reitzes** (Managing Director)\nJust in the first half, it would seem that there would be some catch-up. So I was wondering how prescriptive you could be there. And then, Jensen, just overall, with the change in administration that's going to take place here in the U.S. and the China situation, have you gotten any sense or any conversations about tariffs or anything with regard to your China business? Any sense of what may or may not go on? It's probably too early, but wondering if you had any thoughts there. Thanks so much.\n\n**Jensen Huang** (President and CEO)\nWe got one quarter at a time.\n\n**Colette Kress** (EVP and CFO)\nWe are working right now on the quarter that we're in and building what we need to ship in terms of Blackwell. We have every supplier on the planet working seamlessly with us to do that. And once we get to next quarter, we'll help you understand in terms of that ramp that we'll see to the next quarter going after that.\n\n**Colette Kress** (EVP and CFO)\nWhatever the new administration decides, we will, of course, support the administration. And that's the highest mandate. And then after that, do the best we can, just as we always do. And so we have to simultaneously, and we will, comply with any regulation that comes along fully and support our customers to the best of our abilities and compete in the marketplace. We'll do all of these three things simultaneously.\n\n**Operator**\nYour final question comes from the line of Pierre Ferragu of New Street Research. Your line is open.\n\n**Pierre Ferragu** (Managing Partner)\nHey, thanks for taking my question. Jensen, you mentioned in your comments you have the pre-training, the actual language models, and you have reinforcement learning that becomes more and more important in training and in inference as well. And then you have inference itself. And I was wondering if you have a sense, a high-level typical sense of out of an overall AI ecosystem, maybe one of your clients or one of the large models that are out there. Today, how much of the compute goes into each of these buckets? How much for the pre-training, how much for the reinforcement, and how much into inference today? Do you have any sense for how it's splitting and where the growth is the most important as well?\n\n**Jensen Huang** (President and CEO)\nWell, today, it's vastly in pre-training of foundation model because, as you know, post-training, the new technologies are just coming online. And whatever you could do in pre-training and post-training, you would try to do so that the inference cost could be as low as possible for everyone.\n\n**Jensen Huang** (President and CEO)\nHowever, there are only so many things that you could do a priori. And so you'll always have to do on-the-spot thinking and in-context thinking and reflection. And so I think that the fact that all three are scaling is actually very sensible based on where we are. And in the area of foundation model, now we have multimodal foundation models. And the amount of petabytes of video that these foundation models are going to be trained on is incredible. And so my expectation is that for the foreseeable future, we're going to be scaling pre-training, post-training, as well as inference time scaling, which is the reason why I think we're going to need more and more compute.\n\n**Jensen Huang** (President and CEO)\nAnd we're going to have to drive as hard as we can to keep increasing the performance by X factors at a time so that we can continue to drive down the cost and continue to increase the revenues and get the AI revolution going.\n\n**Pierre Ferragu** (Managing Partner)\nThank you.\n\n**Operator**\nThank you. I'll turn the call back over to Jensen Huang for a closing remark.\n\n**Jensen Huang** (President and CEO)\nThank you. The tremendous growth in our business is being fueled by two fundamental trends that are driving global adoption of NVIDIA computing. First, the computing stack is undergoing a reinvention, a platform shift from coding to machine learning, from executing code on CPUs to processing neural networks on GPUs. The trillion-dollar install base of traditional data center infrastructure is being rebuilt for software 2.0, which applies machine learning to produce AI. Second, the age of AI is in full swing.\n\n**Jensen Huang** (President and CEO)\nGenerative AI is not just a new software capability, but a new industry with AI factories manufacturing digital intelligence, a new industrial revolution that can create a multi-trillion-dollar AI industry. Demand for Hopper and anticipation for Blackwell, which is now in full production, are incredible for several reasons. There are more foundation model makers now than there were a year ago. The computing scale of pre-training and post-training continues to grow exponentially. There are more AI-native startups than ever, and the number of successful inference services is rising. With the introduction of ChatGPT o1, OpenAI o1, a new scaling law called test-time scaling has emerged. All of these consume a great deal of computing. AI is transforming every industry, company, and country. Enterprises are adopting agentic AI to revolutionize workflows. Over time, AI coworkers will assist employees in performing their jobs faster and better.\n\n**Jensen Huang** (President and CEO)\nInvestments in industrial robotics are surging due to breakthroughs in physical AI, driving new training infrastructure demand as researchers train world foundation models on petabytes of video and Omniverse synthetically generated data. The age of robotics is coming. Countries across the world recognize the fundamental AI trends we are seeing and have awakened to the importance of developing their national AI infrastructure. The age of AI is upon us, and it's large and diverse. NVIDIA's expertise, scale, and ability to deliver full stack and full infrastructure let us serve the entire multi-trillion-dollar AI and robotics opportunities ahead. From every hyperscale cloud, enterprise private cloud, to sovereign regional AI clouds, on-prem, to industrial edge, and robotics. Thanks for joining us today, and catch up next time.\n\n**Operator**\nThis concludes today's conference call. You may now disconnect.",
      "fetched_at": "2026-02-04T15:38:30.972Z"
    }
  ]
}