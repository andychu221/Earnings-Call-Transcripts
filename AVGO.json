{
  "ticker": "AVGO",
  "last_updated": "2026-02-04T15:40:17.275Z",
  "total_transcripts": 4,
  "transcripts": [
    {
      "ticker": "AVGO",
      "title": "Broadcom Inc. (AVGO) Q4 FY2025 earnings call transcript",
      "published_date": "Dec 11, 2025, 5:00 PM EST",
      "fiscal_year": "2025",
      "quarter": "Q4",
      "url": "https://finance.yahoo.com/quote/AVGO/earnings/AVGO-Q4-2025-earnings_call-382934.html",
      "content": "**Operator**\nWelcome to Broadcom Inc.'s Fourth Quarter and Fiscal Year 2025 Financial Results Conference Call. At this time, for opening remarks and introductions, I would like to turn the call over to Ji Yoo, Head of Investor Relations of Broadcom Inc.\n\n**Ji Yoo** (Head of Investor Relations)\nThank you, Cherie, and good afternoon, everyone. Joining me on today's call are Hock Tan, President and CEO, Kirsten Spears, Chief Financial Officer, and Charlie Kawwas, President, Semiconductor Solutions Group. Broadcom distributed a press release and financial tables after the market closed, describing our financial performance for the fourth quarter and fiscal year 2025. If you did not receive a copy, you may obtain the information from the investor section of Broadcom's website at broadcom.com. This conference call is being webcast live, and an audio replay of the call can be accessed for one year through the investor section of Broadcom's website. During the prepared remarks, Hock and Kirsten will be providing details of our fourth quarter and fiscal year 2025 results, guidance for our first quarter of fiscal year 2026, as well as commentary regarding the business environment. We'll take questions after the end of our prepared comments.\n\n**Ji Yoo** (Head of Investor Relations)\nPlease refer to our press release today and our recent filings with the SEC for information on the specific risk factors that could cause our actual results to differ materially from the forward-looking statements made on this call. In addition to U.S. GAAP reporting, Broadcom reports certain financial measures on a non-GAAP basis. A reconciliation between GAAP and non-GAAP measures is included in the tables attached to today's press release. Comments made during today's call will primarily refer to our non-GAAP financial results. I'll now turn the call over to Hock.\n\n**Hock Tan** (President and CEO)\nThank you, Ji. And thank you, everyone, for joining us today. We just ended our Q4, fiscal 2025, and before I get into details of that quarter, let me recap the year. In our fiscal 2025, consolidated revenue grew 24% year over year to a record $64 billion, and it's driven by AI semiconductors and VMware. AI revenue grew 65% year over year to $20 billion, driving the semiconductor revenue for this company to a record $37 billion for the year. In our infrastructure software business, strong adoption of VMware Cloud Foundation, or VCF, as we call it, drove revenue growth of 26% year on year to $27 billion. In summary, 2025 was another strong year for Broadcom, and we see the spending momentum by our customers for AI continuing to accelerate in 2026. Now, let's move on to the results of our fourth quarter 2025.\n\n**Hock Tan** (President and CEO)\nTotal revenue was a record $18 billion, up 28% year on year, and above our guidance on better-than-expected growth in AI semiconductors, as well as infrastructure software. Q4 consolidated adjusted EBITDA was a record $12.12 billion, up 34% year on year. So let me give you more color on our two segments. In semiconductors, revenue was $11.1 billion, as year-on-year growth accelerated to 35%. And this robust growth was driven by the AI semiconductor revenue of $6.5 billion, which was up 74% year on year. And this represents a growth trajectory exceeding 10 times over the 11 quarters we have reported this line of business. Our custom-accelerated business more than doubled year over year, as we see our customers increase adoption of XPUs, as we call those custom accelerators, in training their LLMs and monetizing their platforms through inferencing APIs and applications.\n\n**Hock Tan** (President and CEO)\nThese XPUs, I might add, are not only being used to train and inference internal workloads by our customers. The same XPUs, in some situations, have been extended externally to other LLM peers. Best exemplified at Google, where the TPUs used in creating Gemini are also being used for AI cloud computing by Apple, Cohere, and SSI as a sample, and the scale at which we see this happening could be significant, and as you are aware, last quarter, Q3 2025, we received a $10 billion order to sell the latest TPU, Ironwood Rex, to Anthropic, and in this quarter, Q4, we received an additional $11 billion order from this same customer for delivery in late 2026, but that does not mean our other two customers are using TPUs.\n\n**Hock Tan** (President and CEO)\nIn fact, they prefer to control their own destiny by continuing to drive their multi-year journey to create their own custom AI accelerators, or XPU Rex, as we call them. I'm pleased today to report that during this quarter, we acquired a fifth XPU customer through a $1 billion order placed for delivery in late 2026. Now, moving on to AI networking. Demand here has even been stronger as we see customers build out their data center infrastructure ahead of deploying AI accelerators. Our current order backlog for AI switches exceeds $10 billion, as our latest 102 terabit per second Tomahawk 6 switch, the first and only one of its capability out there, continues to book at record rates. This is just a subset of what we have.\n\n**Hock Tan** (President and CEO)\nWe have also secured record orders on DSPs, optical components like lasers, and PCI Express switches to be deployed in AI data centers, and all these components, combined with our XPUs, bring our total order on hand in excess of $73 billion today, which is almost half Broadcom's consolidated backlog of $162 billion. We expect these $73 billion in AI backlog to be delivered over the next 18 months, and in Q1, fiscal 2026, we expect our AI revenue to double year on year to $8.2 billion. Turning to non-AI semiconductors, Q4 revenue of $4.6 billion was up 2% year on year and up 16% sequentially based on favorable wireless seasonality. Year on year, broadband showed solid recovery. Wireless was flat, and all the other end markets were down as enterprise spending continued to show limited signs of recovery.\n\n**Hock Tan** (President and CEO)\nAccordingly, in Q1, we forecast non-AI semiconductor revenue to be approximately $4.1 billion, flat from a year ago, down sequentially due to wireless seasonality. Let me now talk about our infrastructure software segment. Q4 infrastructure software revenue of $6.9 billion was up 19% year on year, and above our outlook of $6.7 billion. Bookings continued to be strong as total contract value booked in Q4 exceeded $10.4 billion versus $8.2 billion a year ago. We ended the year with $73 billion of infrastructure software backlog, up from $49 billion a year ago. We expect renewals to be seasonal in Q1 and forecast infrastructure software revenue to be approximately $6.8 billion. We still expect, however, that for fiscal 2026, infrastructure software revenue to grow low double-digit percentage. Here's what we see in 2026.\n\n**Hock Tan** (President and CEO)\nDirectionally, we expect AI revenue to continue to accelerate and drive most of our growth, and non-AI semiconductor revenue to be stable. Infrastructure software revenue will continue to be driven by VMware growth at low double digits. And for Q1 2026, we expect consolidated revenue of approximately $19.1 billion, up 28% year on year. And we expect adjusted EBITDA to be approximately 67% of revenue. And with that, let me turn the call over to Kirsten.\n\n**Kirsten Spears** (CFO)\nThank you, Hock. Let me now provide additional detail on our Q4 financial performance. Consolidated revenue was a record $18 billion for the quarter, up 28% from a year ago. Gross margin was 77.9% of revenue in the quarter, better than we originally guided on higher software revenues and product mix within semiconductors. Consolidated operating expenses were $2.1 billion, of which $1.5 billion was research and development. Q4 operating income was a record $11.9 billion, up 35% from a year ago. Now, on a sequential basis, even as gross margin was down 50 basis points on semiconductor product mix, operating margin increased 70 basis points sequentially to 66.2% on favorable operating leverage. Adjusted EBITDA of $12.12 billion, or 68% of revenue, was above our guidance of 67%. This figure excludes $148 million of depreciation. Now, a review of the P&L for our two segments, starting with semiconductors.\n\n**Kirsten Spears** (CFO)\nRevenue for our semiconductor solution segment was a record $11.1 billion, with growth accelerating to 35% year on year, driven by AI. Semiconductor revenue represented 61% of total revenue in the quarter. Gross margin for our semiconductor solution segment was approximately 68%. Operating expenses increased 16% year on year to $1.1 billion on increased investment in R&D for leading-edge AI semiconductors. Semiconductor operating margin of 59% was up 250 basis points year on year. Now, moving to infrastructure software. Revenue for infrastructure software of $6.9 billion was up 19% year on year and represented 39% of total revenue. Gross margin for infrastructure software was 93% in the quarter, compared to 91% a year ago. Operating expenses were $1.1 billion in the quarter, resulting in infrastructure software operating margin of 78%. This compares to operating margin of 72% a year ago, reflecting the completion of the integration of VMware.\n\n**Kirsten Spears** (CFO)\nMoving on to cash flow. Free cash flow in the quarter was $7.5 billion and represented 41% of revenue. We spent $237 million on capital expenditures. Day sales outstanding were 36 days in the fourth quarter, compared to 29 days a year ago. We ended the fourth quarter with inventory of $2.3 billion, up 4% sequentially. Our days of inventory on hand were 58 days in Q4, compared to 66 days in Q3, as we continue to remain disciplined on how we manage inventory across the ecosystem. We ended the fourth quarter with $16.2 billion of cash, up $5.5 billion sequentially on strong cash flow generation. The weighted average coupon rate in years to maturity of our gross principal fixed rate debt of $67.1 billion is 4% at 7.2 years, respectively. Turning to capital allocation.\n\n**Kirsten Spears** (CFO)\nIn Q4, we paid stockholders $2.8 billion of cash dividends based on a quarterly common stock cash dividend of $0.59 per share. In Q1, we expect the non-GAAP diluted share count to be approximately 4.97 billion shares, excluding the potential impact of any share repurchases. Now, let me recap our financial performance for fiscal year 2025. Our revenue hit a record $63.9 billion, with organic growth accelerating to 24% year on year. Semiconductor revenue was $36.9 billion, up 22% year over year. Infrastructure software revenue was $27 billion, up 26% year on year. Fiscal 2025 adjusted EBITDA was $43 billion and represented 67% of revenue. Free cash flow grew 39% year on year to $26.9 billion. For fiscal 2025, we returned $17.5 billion of cash to shareholders in the form of $11.1 billion of dividends and $6.4 billion in share repurchases and elimination.\n\n**Kirsten Spears** (CFO)\nAligned with our ability to generate increased cash flows in the preceding year, we are announcing an increase in our quarterly common stock cash dividend in Q1 fiscal 2026 to $0.65 per share, an increase of 10% from the prior quarter. We intend to maintain this target quarterly dividend throughout fiscal 26, subject to quarterly board approval. This implies our fiscal 2026 annual common stock dividend to be a record $2.60 per share, an increase of 10% year on year. I would like to highlight that this represents the 15th consecutive increase in annual dividends since we initiated dividends in fiscal 2011. The board also approved an extension of our share repurchase program, of which $7.5 billion remains through the end of calendar year 2026. Now, moving to guidance. Our guidance for Q1 is for consolidated revenue of $19.1 billion, up 28% year on year.\n\n**Kirsten Spears** (CFO)\nWe forecast semiconductor revenue of approximately $12.3 billion, up 50% year on year. Within this, we expect Q1 AI semiconductor revenue of $8.2 billion, up approximately 100% year on year. We expect infrastructure software revenue of approximately $6.8 billion, up 2% year on year. For your modeling purposes, we expect Q1 consolidated gross margin to be down approximately 100 basis points sequentially, primarily reflecting a higher mix of AI revenue. As a reminder, consolidated gross margins through the year will be impacted by the revenue mix of infrastructure software and semiconductors, and also product mix within semiconductors. We expect Q1 adjusted EBITDA to be approximately 67%. We expect the non-GAAP tax rate for Q1 and fiscal year 2026 to increase from 14% to approximately 16.5% due to the impact of the global minimum tax and shift in geographic mix of income compared to that of fiscal year 2025.\n\n**Kirsten Spears** (CFO)\nThat concludes my prepared remarks. Operator, please open up the call for questions.\n\n**Operator**\nThank you. To ask a question, you will need to press star 11 on your telephone. To withdraw your question, press star 11 again. Due to time restraints, we ask that you please limit yourself to one question. Please stand by while we compile the Q&A roster. Our first question will come from the line of Vivek Arya with Bank of America. Your line is open.\n\n**Vivek Arya** (Managing Director and Senior Analyst)\nThank you. Just wanted to clarify. Hock, you said $73 billion over 18 months for AI. That's roughly $50-ish billion plus for fiscal 26 for AI. I just wanted to make sure I got that right. And then the main question, Hock, is that there is sort of this emerging debate about customer-owned tooling, your hyperscale customers potentially wanting to do more things on their own. How do you see your XPU content and share at your largest customer evolve over the next one or two years? Thank you.\n\n**Hock Tan** (President and CEO)\nTo answer your first question, what we said is correct that as of now, we have $73 billion of backlog in place of XPUs, switches, DSPs, lasers for AI data centers that we anticipate shipping over the next 18 months. Obviously, this is as of now. I mean, we fully expect more bookings to come in over that period of time. So don't take that $73 billion as that's the revenue we ship over the next 18 months. We're just saying we have that now, and the bookings have been accelerating. Frankly, we see that bookings not just in XPUs, but in switches, DSPs, all the other components that go into AI data centers. We have never seen bookings of the nature that what we have seen over the past three months, particularly with respect to Tomahawk 6 switches.\n\n**Hock Tan** (President and CEO)\nThis is one of the fastest-growing products in terms of deployment that we have ever seen of any switch product that we put out there. It is pretty interesting, and partly because it's the only one of its kind out there at this point at 102 terabits per second, and that's the exact product needed to expand the clusters of the latest GPU and XPUs out there. Oh, that's great, but as far as what is the future, is XPU your broader question? My answer to you is don't follow what you hear out there as gospel. It's a trajectory. It's a multi-year journey, and many of the players, and not too many players doing LLMs, want to do their own custom AI accelerator for very good reasons. You can put in hardware what, if you use a general-purpose GPU, you can only do in software and kernels and software.\n\n**Hock Tan** (President and CEO)\nYou can achieve performance-wise so much better in the custom-purpose designed hardware-driven XPU. And we see that in the TPU, and we see that in all the accelerators we are doing for our other customers. Much, much better in areas of Sparse Core, training, inference, reasoning, all that stuff. Now, will that mean that over time they all want to go do it themselves? Not necessarily. And in fact, because the technology in silicon keeps updating, keeps evolving. And if you are an LLM player, where do you put your resources in order to compete in this space, especially when you have to compete at the end of the day against merchant GPUs who are not slowing down in their rate of evolution? So I see that as this concept of customer tooling is an overblown hypothesis, which frankly, I don't think will happen.\n\n**Vivek Arya** (Managing Director and Senior Analyst)\nThank you.\n\n**Operator**\nMoment for our next question, and that will come from the line of Ross Seymour with Deutsche Bank. Your line is open.\n\n**Ross Seymore** (Managing Director)\nHi. Thanks for asking the question. Hock, I wanted to go to something you touched on earlier about the TPUs going a little bit more to a merchant go-to-market to other customers. Do you believe that's a substitution effect for customers who otherwise would have done ASICs with you, or do you think it's actually broadening the market? And so what are kind of the financial implications of that from your perspective?\n\n**Hock Tan** (President and CEO)\nThat's a very good question, Ross. What we see right now is the most obvious move it does is it goes to the people who use TPUs. The alternative is GPUs on a merchant basis. That's the most common thing that happens. Because to do that substitution for another custom, it's different. To make an investment in custom accelerator is a multi-year journey. It's a strategic directional thing. It's not necessarily a very transactional or short-term move. Moving from GPU to TPU is a transactional move. Going into AI accelerator of your own is a long-term strategic move, and nothing will deter you from that to continue to make that investment towards that end goal of successfully creating and deploying your own custom AI accelerator. That's the motion we see.\n\n**Ross Seymore** (Managing Director)\nThank you.\n\n**Operator**\nAnd that will come from the line of Harlan Sur with J.P. Morgan.\n\n**Harlan Sur** (Executive Director of Equity Research)\nYeah. Good afternoon. Thanks for taking my question and congratulations on the strong results, guidance, and execution, Hock. Again, I just want to reiterate I just want to sort of verify this, right? So you talked about total AI backlog of $73 billion over the next six quarters, right? This is just a snapshot of your order book right now. But given your lead times, I think customers can and still will place orders for AI in quarters four, five, and six. So as time moves forward, that backlog number for more shipments in the second half of 2026 will probably still go up, right? Is that the correct interpretation? And then given the strong and growing backlog, right, the question is, does the team have 3-nanometer, 2-nanometer wafer supply, CoWoS, substrate, HBM supply commitments to support all of the demand in your order book?\n\n**Harlan Sur** (Executive Director of Equity Research)\nAnd I know one of the areas where you are trying to mitigate this is in advanced packaging, right? You're bringing up your Singapore facility. Can you guys just remind us what part of the advanced packaging process the team is focusing on with the Singapore facility? Thanks.\n\n**Hock Tan** (President and CEO)\nThanks. Well, to answer your first simpler question, you're right. You can say that $73 billion is the backlog we have today to ship over the next six quarters. You might also say that, and given our lead time, we expect more orders to be able to be absorbed into our backlog for shipments over the next six quarters. So take it that we expect revenue, a minimum revenue, one way to look at it, of $73 billion over the next six quarters, but we do expect much more as more orders come in for shipments within that next six quarters. Our lead time, depending on the particular product it is, can be anywhere from six months to a year. With respect to supply chain, is what you're asking, critical supply chain on silicon and packaging?\n\n**Harlan Sur** (Executive Director of Equity Research)\nYes.\n\n**Hock Tan** (President and CEO)\nYeah. That's an interesting challenge that we have been addressing constantly and continue to, and with the strength of the demand and the need for more innovative packaging, advanced packaging, because you're talking about multi-chips, multi-chips in creating every custom accelerator now, the packaging becomes a very interesting and technical challenge. Building our Singapore fab is to really talk about partially insourcing those advanced packaging. We believe that we have enough demand. We can literally insource not from the viewpoint of not just cost, but in the viewpoint of supply chain security and delivery. We're building up a fairly substantial facility for packaging, advanced packaging in Singapore, as you indicated, purely for that purpose to address the advanced packaging side. Silicon-wise, no, we go back to the same process source in TSMC, and so we keep going for more and more capacity in 2 nanometers, 3 nanometers.\n\n**Hock Tan** (President and CEO)\nAnd so far, we do not have that constraint. But again, time will tell as we progress and as our backlog builds up.\n\n**Harlan Sur** (Executive Director of Equity Research)\nThank you, Hock.\n\n**Operator**\nOne moment for our next question. The next question will come from the line of Blaine Curtis with Jefferies. Your line is open.\n\n**Blayne Curtis** (Managing Director)\nHey, good afternoon. Thanks for taking my question. I wanted to ask, with the original $10 billion deal, you talked about a rack sale. I just wanted to, with the follow-on order as well as the fifth customer, can you just maybe describe how you're going to deliver those? Is it an XPU, or is it a rack? And then maybe you can kind of just walk us through the math and kind of what the deliverable is. Obviously, Google uses its own networking. So I'm kind of curious too, would it be a copy exact of what Google does now that you could talk to it to name? Or would you have your own networking in there as well? Thanks.\n\n**Hock Tan** (President and CEO)\nThat's a very complicated question, Blaine. Let me tell you what it is. It's a system sale. Hock, about that. It's a real system sale. We have so many components beyond XPUs, custom accelerators in any system, in AI system, any AI system used by hyperscalers that, yeah, we believe it began to make sense to do it as a system sales and be responsible, be fully responsible for the entire system or rack, as you call it. I think people understand it as a system sale better. And so on this customer number four, we are selling it as a system with our key components in it. And that's no different than selling a chip. We certify and final ability to run as part of the whole selling process.\n\n**Blayne Curtis** (Managing Director)\nOkay. Thanks, Hock.\n\n**Operator**\nOne moment for our next question, and that will come from the line of Stacy Rasgon with Bernstein. Your line is open.\n\n**Stacy Rasgon** (Managing Director and Senior Analyst)\nHi, guys. Thanks for taking my question. I wanted to touch on gross margins, and maybe it feeds into a little bit the prior question. So I understand why the AI business is somewhat diluted to gross margins. We have the HBM pass-through, and then presumably with the system sales, that will be more diluted. And you've hinted at this in the past, but I was wondering if you could be a little more explicit. As this AI revenue starts to ramp, as we start to get system sales, how should we be thinking about that gross margin number, say if we're looking out four quarters or six quarters? Is it low 70s? I mean, could it start with a 6 at the corporate level? And I guess I'm also wondering, I understand how that comes down, but what about the operating margins?\n\n**Stacy Rasgon** (Managing Director and Senior Analyst)\nDo you think you get enough operating leverage on the OpEx side to keep operating margins flat, or do they need to come down as well?\n\n**Hock Tan** (President and CEO)\nI'll let Kirsten give you the details, but enough for me to broadly high-level explain to you, Stacy, and good question, phenomenal. You don't see that impacting us right now, and we have already started that process of some system sales. You don't see that in our numbers, but it will, and we have said that openly. The AI revenue has a lower gross margin than our, obviously, the rest of our business, including software, of course, but we expect the rate of growth as we do more and more AI revenue to be so much that we get the operating leverage on our operating spending, that operating margin will deliver dollars that are still a high level of growth from what it has been. So we expect operating leverage to benefit us at the operating margin level, even as gross margin will start to deteriorate. High level.\n\n**Kirsten Spears** (CFO)\nYeah. I think Hock said that fairly, and the second half of the year, when we do start shipping more systems, the situation is straightforward. We'll be passing through more components that are not ours, so think of it similar to the XPUs where we have memory on those XPUs, and we're passing through those costs. We'll be passing through more costs within the rack, and so those gross margins will be lower. However, overall, the way Hock said it, gross margin dollars will go up, margins will go down. Operating margins, because we have leverage, operating margin dollars will go up, but the margin itself as a percentage of revenues will come down a bit, but we're not, I mean, we'll guide closer to the end of the year for that.\n\n**Stacy Rasgon** (Managing Director and Senior Analyst)\nGot it. Thank you, guys.\n\n**Operator**\nOne moment for our next question. That will come from the line of Jim Schneider with Goldman Sachs. Your line is open.\n\n**Jim Schneider** (Senior Equity Analyst)\nGood afternoon. Thanks for taking my question. Hock was wondering if you might care to calibrate your expectations for AI revenue in fiscal 2026 a little bit more closely. I believe you talked about acceleration in fiscal 2026 off of the 65% growth rate you did in fiscal 2025, and then you're guiding to 100% growth for Q1. So I'm just wondering if the Q1 is a good jumping-off point for the growth rate you expect for the full year or something maybe a little bit less than that. And then maybe if you could separately clarify whether your $1 billion of orders for the fifth customer is indeed OpenAI, which you made a separate announcement about. Thank you.\n\n**Hock Tan** (President and CEO)\nWow. There's a lot of questions here. But let me start off with 2026. Our backlog is very dynamic these days, as I said. It's continuing to ramp up. And you're right. We originally, six months ago, said maybe year on year, AI revenues would grow in 2026 60%-70%. Q1, we doubled. And Q1 2026, today, we're saying it doubled. And we're looking at it because all the fresh orders keep coming in, and we give you a milestone of where we are today, which is $73 billion of backlog to be shipped over the next 18 months. And we do fully expect, as I answered the earlier question, for that $73 billion over the 18 months to keep growing. Now, it's a moving target. It's a moving number as we move in time, but it will grow.\n\n**Hock Tan** (President and CEO)\nIt's hard for me to pinpoint what 2026 is going to look like precisely. So I'd rather not give you guys any guidance. That's why we don't give you guidance, but we do give it for Q1. Give it time. We'll give it for Q2. You're right. It's in that to us, is it an accelerating trend? My answer is it's likely to be an accelerating trend as we progress through 2026. Hope that answers your question.\n\n**Jim Schneider** (Senior Equity Analyst)\nYes. Thank you.\n\n**Operator**\nOne moment for our next question, and that will come from the line of Ben Reitzes with Melius Research. Your line is open.\n\n**Ben Reitzes** (Managing Director)\nYeah. Hey, guys. Thanks a lot. Hey, Hock. I wanted to ask. I'm not sure if the last caller said something on it, but I didn't hear it in the answer. What I wanted to ask about the OpenAI contract that it's supposed to start in the second half of the year and go through 2029 for 10 gigawatts. I'm going to assume that that's the fifth customer order there. And I was just wondering if you're still confident in that being a driver. Are there any obstacles to making that a major driver? And when you expect that to contribute and your confidence in it? Thanks so much, Hock.\n\n**Hock Tan** (President and CEO)\nYou didn't hear that answer from my last caller, Jim's questions, because I didn't answer it. I did not answer it, and I'm not answering it either. It's the fifth customer, and it's a real customer, and it will grow. They are on their multi-year journey to their own XPUs. And let's leave it at that. As far as the OpenAI view that you have, we appreciate the fact that it is a multi-year journey that will run through 2029, as our press release with OpenAI showed. 10 gigawatts between 2026, more like 2027, 2028, 2029, Ben, not 2026. It's more like 2027, 2028, 2029, 10 gigawatts. That was the OpenAI discussion. And I call it an agreement, an alignment of where we're headed with respect to a very respected and valued customer, OpenAI. But we do not.\n\n**Ben Reitzes** (Managing Director)\nOkay. That's real interesting.\n\n**Hock Tan** (President and CEO)\nWe do not expect much in 2026.\n\n**Ben Reitzes** (Managing Director)\nOkay. Thanks for clarifying that. That's real interesting. Appreciate it.\n\n**Operator**\nOne moment for our next question. That will come from the line of C.J. Muse with Cantor Fitzgerald. Your line is open.\n\n**CJ Muse Senior** (Managing Director)\nYeah. Good afternoon. Thank you for taking the question. I guess, Hock, I wanted to talk about custom silicon and maybe speak to how you expect compute to grow for Broadcom generation to generation. And as part of that, your competitor announced XPU offering essentially accelerator for an accelerator for massive context windows. I'm curious if you see it broadening opportunity for your existing five customers to have multiple XPU offerings. Thanks so much.\n\n**Hock Tan** (President and CEO)\nThank you. No. Yeah. You hit it right on. I mean, the nice thing about a custom accelerator is you try not to do one size fits all and generationally. Each of these five customers now can create their version of an XPU custom accelerator for training and inference. And basically, it's almost two parallel tracks going on almost simultaneously for each of them. So I would have plenty of versions to deal with. I don't need to create any more versions. We got plenty of different content out there just on the basis of creating these custom accelerators. And by the way, when you do custom accelerators, you tend to put more hardware in that are unique, differentiated versus trying to make it work on software and creating kernels into software.\n\n**Hock Tan** (President and CEO)\nI know that's very tricky too, but thinking about the difference where you can create in hardware those Sparse Core data routers versus the dense matrix multipliers, all in one same chip. And that's just one example of what creating custom accelerators is letting us do. Or for that matter, a variation in how much memory capacity or memory bandwidth for the same customer from chip to chip, just because even in inference, you want to do more reasoning versus decoding versus something else like prefill. So you literally start to create different hardware for different aspects of how you want to train or inference and run your workloads. It's a very fascinating area, and we are seeing a lot of variations and multiple chips for each of our customers.\n\n**CJ Muse Senior** (Managing Director)\nThank you.\n\n**Operator**\nOne moment for our next question, and that will come from the line of Harsh Kumar with Piper Sandler. Your line is open.\n\n**Harsh Kumar** (Managing Director and Senior Research Analyst)\nYeah. Hock and team, first of all, congratulations on some pretty stunning numbers. I've got an easy one and a more strategic one. The easy one is your guide in AI, Hock and Kirsten, is calling for almost $1.7 billion of sequential growth. I was curious, maybe you could talk about the diversity of the growth between the three existing customers. Is it pretty well spread out, all of them growing, or is one sort of driving much of the growth? And then, Hock, strategically, one of your competitors bought a photonic fabric company recently. I was curious about your take on that technology and if you think it's disruptive or you think it's just gimmickery at this point in time.\n\n**Hock Tan** (President and CEO)\nI like the way you address this question, the way that you address the question to me. It's almost hesitant. Thank you. I appreciate that. But on your first part, yeah, we are driving growth, and it began to feel like this thing never ends. And it's a real mixed bag of existing customers and on existing XPUs. And a big part of it is XPUs that we're seeing. And that's not to slow down the fact that, as I indicated in my remarks and commented on, the demand for switches, not just Tomahawk 6, Tomahawk 5 switches, the demand for our latest 1.6 terabit per second DSPs that enables optical interconnects for scale-out, particularly. It's just very, very strong. And by extension, demand for the optical components like lasers, PIN diodes, just going nuts. All that come together.\n\n**Hock Tan** (President and CEO)\nNow, all that is smaller on relatively lesser dollars when it comes to XPUs, as you probably guess. I mean, to give you a sense, maybe let me look at it on a backlog side. Of the $73 billion of AI revenue backlog over the next 18 months I talked about, maybe $20 billion of it is everything else. The rest is XPUs. Hope that gives you a sense of what the mix is. But that's not to say that the rest is still $20 billion. That's not small by any means. So we value that. So when you talk about your next question of silicon photonics as a means to create basically much better, more efficient, lower power interconnects in not just scale-out, but hopefully scale-up, yeah, I could see a point in time in the future when silicon photonics matters as the only way to do it.\n\n**Hock Tan** (President and CEO)\nWe're not quite there yet, but we have the technology, and we continue to develop the technology. Even at each time, we develop it first for 400 gigabit bandwidth, going on to 800 gigabit bandwidth. Not ready for it yet. And even we have the product, and we're now doing it for 1.6 terabit bandwidth to create silicon photonics switches, silicon photonics interconnects. Not even sure it will get fully deployed because our engineers, our peers, and the peers we have out there will somehow try to find a way to still try to do scale-up within a rack in copper as long as possible and in scale-up in pluggable optics. The final, final straw is when you can't do it well in pluggable optics. And of course, when you can't do it even in copper, then you're right. You go to silicon photonics, and it will happen.\n\n**Hock Tan** (President and CEO)\nWe're ready for it. Just saying not anytime soon.\n\n**Harsh Kumar** (Managing Director and Senior Research Analyst)\nThank you, Hock.\n\n**Operator**\nOne moment for our next question. That will come from the line of Carl Ackerman with BNP Paribas. Your line is open.\n\n**Karl Ackerman** (Managing Director)\nYes. Thank you. Hock, could you speak to the supply chain resiliency and visibility you have with your key materials suppliers, particularly CoWoS, as you not only support your existing customer programs but the two new custom compute processors that you announced this quarter? I guess what I'm getting at is you also happen to address the very large subset of networking and compute AI supply chains. You talked about record backlog. If you were to pinpoint some of the bottlenecks that you have and areas that you're aiming to address and mitigate from supply chain bottlenecks, what would they be, and how do you see that ameliorating into 2026? Thank you.\n\n**Hock Tan** (President and CEO)\nIt's across the board, typically. I mean, we are very fortunate in some ways that we have the product technology and the operating business lines to create multiple key leading-edge components that enables today's state-of-the-art AI data centers. I mean, our DSP, as I said earlier, is now at 1.6 terabit per second. That's the leading-edge connectivity for bandwidth for the top of the heap XPU and even GPU. And we intend to be that way. And we have the lasers, EMLs, VCSELs, CW lasers that goes with it. So it's fortunate that we have all this and the key active components that go with it. And we see it very early, and we expand the capacity as we do the design to match it.\n\n**Hock Tan** (President and CEO)\nThis is a long answer to what I'm trying to get at, which is I think we are of any of these data center suppliers of the system racks, not counting the power shell and all that. Now, that starts to get beyond us on the power shell and the transformers and the gas turbines. If you just look at the racks, the systems on AI, we probably have a good handle on where the bottlenecks are because sometimes we are part of the bottlenecks, which we then work to get to resolve. We feel pretty good about that through 2026.\n\n**Karl Ackerman** (Managing Director)\nThank you.\n\n**Operator**\nOne moment for our next question. That will come from the line of Christopher Rolland with Susquehanna. Your line is open.\n\n**Christopher Rolland** (Semiconductor Analyst)\nHi. Thanks for the question. Just first a clarification and then my question. And sorry to come back to this issue, but if I understand you correctly, Hock, I think you were saying that OpenAI would be a general agreement, so it's not binding, maybe similar to the agreements with both NVIDIA and AMD. And then secondly, you talked about flat non-AI semiconductor revenue. Maybe what's going on there is there still an inventory overhang, and what do we need to get that going again? Do you see growth eventually in that business? Thank you.\n\n**Hock Tan** (President and CEO)\nOn the non-AI semiconductor, we see broadband literally recovering very well. We don't see the others. No, we see stability. We don't see a sharp recovery that is sustainable yet. I guess give it a couple more quarters. We don't see any further deterioration in demand. It's more, I think, maybe the AI is sucking the oxygen a lot out of enterprise spending elsewhere and hyperscaler spending elsewhere. We don't see it getting any worse. We don't see it recovering very quickly with the exception of broadband. That's a simple summary of non-AI. With respect to OpenAI, without diving into depth, I'm just telling you what that 10-gigawatt announcement is all about. Separately, the journey with them on the custom accelerator progresses at a very advanced stage and will happen very, very quickly. We will have a committed element to this whole thing.\n\n**Hock Tan** (President and CEO)\nAnd that will. But what I was articulating earlier was the 10-gigawatt announcement. And that 10-gigawatt announcement is an agreement to be aligned on developing 10 gigawatts for OpenAI over 2027 to 2029 timeframe. That's it. That's different from the XPU program we're developing with them.\n\n**Christopher Rolland** (Semiconductor Analyst)\nI see. Thank you very much.\n\n**Operator**\nThank you. And we do have time for one final question. And that will come from the line of Joe Moore with Morgan Stanley. Your line is open.\n\n**Joe Moore** (Semiconductor Industry Analyst)\nGreat. Thank you very much. So if you have $21 million of rack revenue in the second half of 2026, I guess, do we stay at that run rate? Beyond that, are you going to continue to sell racks, or does that sort of that type of business make shift over time? And I'm really just trying to figure out the percentage of your 18-month backlog that's actually full systems at this point.\n\n**Hock Tan** (President and CEO)\nWell, it's an interesting question. And that question basically comes to how much compute capacity is needed by our customers over the next, as I say, over the period beyond 18 months. And your guess is probably as good as mine based on what we all know out there, which is really what it relates to. But if they need more, then you see that continuing even larger. If they don't need it, then probably it won't. But what we're trying to indicate is that's the demand we're seeing over that period of time right now.\n\n**Joe Moore** (Semiconductor Industry Analyst)\nThank you.\n\n**Operator**\nI would now like to turn the call back over to Ji Yoo for any closing remarks.\n\n**Ji Yoo** (Head of Investor Relations)\nThank you, operator. This quarter, Broadcom will be presenting at the New Street Research Virtual AI Big Ideas Conference on Monday, December 15th, 2025. Broadcom currently plans to report its earnings for the first quarter of fiscal year 2026 after close of market on Wednesday, March 4th, 2026. A public webcast of Broadcom's earnings conference call will follow at 2:00 P.M. Pacific. That will conclude our earnings call today. Thank you all for joining. Operator, you may end the call.\n\n**Operator**\nThis concludes today's program. Thank you all for participating. You may now disconnect.",
      "fetched_at": "2026-02-04T15:39:56.502Z"
    },
    {
      "ticker": "AVGO",
      "title": "Broadcom Inc. (AVGO) Q3 FY2025 earnings call transcript",
      "published_date": "Sep 4, 2025, 5:00 PM EDT",
      "fiscal_year": "2025",
      "quarter": "Q3",
      "url": "https://finance.yahoo.com/quote/AVGO/earnings/AVGO-Q3-2025-earnings_call-353878.html",
      "content": "**Operator**\nWelcome to Broadcom Inc.'s third quarter, fiscal year 2025 financial results conference call. At this time, for opening remarks and introductions, I would like to turn the call over to Ji Yoo, Head of Investor Relations of Broadcom Inc. Please go ahead.\n\n**Ji Yoo** (Investor Relations)\nThank you, Cheri, and good afternoon, everyone. Joining me on today's call are Hock Tan, President and CEO; Kirsten Spears, Chief Financial Officer; and Charlie Kawwas, President, Semiconductor Solutions Group. Broadcom distributed a press release and financial tables after the market close, describing our financial performance for the third quarter of fiscal year 2025. If you did not receive a copy, you may obtain the information from the Investor section of Broadcom's website at broadcom.com. This conference call is being webcast live, and an audio replay of the call can be accessed for one year through the Investor section of Broadcom's website. During the prepared comments, Hock and Kirsten will be providing details of our third quarter fiscal year 2025 results, guidance for our fourth quarter of fiscal year 2025, as well as commentary regarding the business environment. We'll take questions after the end of our prepared comments.\n\n**Ji Yoo** (Investor Relations)\nPlease refer to our press release today and our recent filings with the SEC for information on the specific risk factors that could cause our actual results to differ materially from the forward-looking statements made on this call. In addition to U.S. GAAP reporting, Broadcom reports certain financial measures on a non-GAAP basis. A reconciliation between GAAP and non-GAAP measures is included in the tables attached to today's press release. Comments made during today's call will primarily refer to our non-GAAP financial results. I will now turn the call over to Hock.\n\n**Hock Tan** (President, CEO &amp; Director)\nThank you, Ji, and thank you, everyone, for joining us today. In our fiscal Q3 2025, total revenue was a record $16 billion, up 22% year-on-year. Revenue growth was driven by better-than-expected strength in AI semiconductors and our continued growth in VMware. Q3 consolidated adjusted EBITDA was a record $10.7 billion, up 30% year-on-year. Looking beyond what we're just reporting this quarter, with robust demand from AI, bookings were extremely strong, and our current consolidated backlog for the company hit a record of $110 billion. Q3 semiconductor revenue was $9.2 billion, as year-on-year growth accelerated to 26% year-on-year. This accelerated growth was driven by AI semiconductor revenue of $5.2 billion, which was up 63% year-on-year, and extended the trajectory of robust growth to 10 consecutive quarters. Let me give you more color on our XPU business, which accelerated to 65% of our AI revenue this quarter.\n\n**Hock Tan** (President, CEO &amp; Director)\nDemand for custom AI accelerators from our three customers continued to grow, as each of them journeys at their own pace towards compute self-sufficiency. Progressively, we continue to gain share with these customers. Further to these three customers, as we had previously mentioned, we have been working with other prospects on their own AI accelerators. Last quarter, one of these prospects released production orders to Broadcom, and we have accordingly characterized them as a qualified customer for XPUs and, in fact, have secured over $10 billion of orders of AI racks based on our XPUs. Reflecting this, we now expect the outlook for our fiscal 2026 AI revenue to improve significantly from what we had indicated last quarter. Turning to AI networking, demand continued to be strong because networking is becoming critical as LLMs continue to evolve in intelligence, and compute clusters have to grow bigger.\n\n**Hock Tan** (President, CEO &amp; Director)\nThe network is the computer, and our customers are facing challenges as they scale to clusters beyond 100,000 compute nodes. For instance, scale-up, which we all know about, is a difficult challenge when you're trying to create substantial bandwidth to share memory across multiple GPUs or XPUs within a rack. Today's AI rack scales up a mere 72 GPUs at 28.8 terabits per second bandwidth using a proprietary NVLink. On the other hand, earlier this year, we have launched Tomahawk 5 with Open Ethernet, which can scale up 512 compute nodes for customers using XPUs. Moving on to scaling out across racks, today the current architecture using 51.2 terabits per second requires three tiers of networking switches. In June, we launched Tomahawk 6 and our Ethernet-based 102 terabit per second switch, which flattens the network to two tiers, resulting in lower latency, much less power.\n\n**Hock Tan** (President, CEO &amp; Director)\nWhen you scale to clusters beyond a single data center footprint, you now need to scale computing across data centers. Over the past two years, we have deployed our Jericho 3 Ethernet router with hyperscale customers to just do this. Today, we have launched our next-generation Jericho 4 Ethernet fabric router with 51.2 terabit per second deep buffering intelligence, intelligent congestion control to handle clusters beyond 200,000 compute nodes crossing multiple data centers. We know the biggest challenge to deploying larger clusters of compute for generative AI will be in networking. For the past 20 years, what Broadcom has developed for Ethernet networking is entirely applicable to the challenges of scale-up, scale-out, and scale across in generative AI. Turning to our forecast, as I mentioned earlier, we continue to make steady progress in growing our AI revenue.\n\n**Hock Tan** (President, CEO &amp; Director)\nFor Q4 2025, we forecast AI semiconductor revenue to be approximately $6.2 billion, up 66% year-on-year. Now, turning to non-AI semiconductors, demand continues to be slow to recover, and Q3 revenue of $4 billion was flat sequentially. While broadband showed strong sequential growth, enterprise networking and server storage were down sequentially. Wireless and industrial were flat quarter on quarter, as we expect. In contrast, in Q4, driven by seasonality, we forecast non-AI semiconductor revenue to grow low double digits sequentially to approximately $4.6 billion. Broadband, server storage, and wireless are expected to improve, while enterprise networking remains down quarter on quarter. Now, let me talk about our infrastructure software segment. Q3 infrastructure software revenue of $6.8 billion was up 17% year-on-year, above our outlook of $6.7 billion as bookings continued to be strong during the quarter. We booked, in fact, total contract value over $8.4 billion during Q3.\n\n**Hock Tan** (President, CEO &amp; Director)\nHere is what I'm most excited about. After two years of engineering development by over 5,000 developers, we delivered on a promise when we acquired VMware. We released VMware Cloud Foundation 9.0, a fully integrated cloud platform which can be deployed by enterprise customers on-prem or carried to the cloud. It enables enterprises to run any application workload, including AI workloads, on virtual machines and on modern containers. This provides the real alternative to public cloud. In Q4, we expect infrastructure software revenue to be approximately $6.7 billion, up 15% year-on-year. In summary, continued strength in AI and VMware will drive our guidance for Q4 consolidated revenue to approximately $17.4 billion, up 24% year-on-year. We expect Q4 adjusted EBITDA to be 67% of revenue. With that, let me turn the call over to Kirsten.\n\n**Kirsten Spears** (CFO &amp; Chief Accounting Officer)\nThank you, Hock. Let me now provide additional detail on our Q3 financial performance. Consolidated revenue was a record $16 billion for the quarter, up 22% from a year ago. Gross margin was 78.4% of revenue in the quarter, better than we originally guided on higher software revenues and product mix within semiconductors. Consolidated operating expenses were $2 billion, of which $1.5 billion was research and development. Q3 operating income was a record $10.5 billion, up 32% from a year ago. On a sequential basis, even as gross margin was down 100 basis points on revenue mix, operating margin increased 20 basis points sequentially to 65.5% on operating leverage. Adjusted EBITDA of $10.7 billion, or 67% of revenue, was above our guidance of 66%. This figure excludes $142 million of depreciation. Now a review of the P&L for our two segments, starting with semiconductors.\n\n**Kirsten Spears** (CFO &amp; Chief Accounting Officer)\nRevenue for our semiconductor solution segment was $9.2 billion, with growth accelerating to 26% year-on-year, driven by AI. Semiconductor revenue represented 57% of total revenue in the quarter. Gross margin for our semiconductor solution segment was approximately 67%, down 30 basis points year-on-year on product mix. Operating expenses increased 9% year-on-year to $951 million on increased investment in R&D for leading-edge AI semiconductors. Semiconductor operating margin of 57% was up 130 basis points year-on-year and flat sequentially. Now moving on to infrastructure software. Revenue for infrastructure software of $6.8 billion was up 17% year-on-year and represented 43% of revenue. Gross margin for infrastructure software was 93% in the quarter compared to 90% a year ago. Operating expenses were $1.1 billion in the quarter, resulting in infrastructure software operating margin of approximately 77%. This compares to operating margin of 67% a year ago, reflecting the completion of the integration of VMware.\n\n**Kirsten Spears** (CFO &amp; Chief Accounting Officer)\nMoving on to cash flow. Free cash flow in the quarter was $7 billion and represented 44% of revenue. We spent $142 million on capital expenditures. Day sales outstanding were 37 days in the third quarter compared to 32 days a year ago. We ended the third quarter with inventory of $2.2 billion, up 8% sequentially in anticipation of revenue growth next quarter. Our days of inventory on hand were 66 days in Q3 compared to 69 days in Q2, as we continue to remain disciplined on how we manage inventory across the ecosystem. We ended the third quarter with $10.7 billion of cash and $66.3 billion of gross principal debt. The weighted average coupon rate and years to maturity of our $65.8 billion in fixed-rate debt is 3.9% and 6.9 years, respectively.\n\n**Kirsten Spears** (CFO &amp; Chief Accounting Officer)\nThe weighted average interest rate and years to maturity of our $500 million in floating rate debt is 4.7% and 0.2 years, respectively. Turning to capital allocation, in Q3, we paid stockholders $2.8 billion of cash dividends based on a quarterly common stock cash dividend of $0.59 per share. In Q4, we expect the non-GAAP diluted share count to be approximately 4.97 billion shares, excluding the potential impact of any share repurchases. Now moving to guidance, our guidance for Q4 is for consolidated revenue of $17.4 billion, up 24% year-on-year. We forecast semiconductor revenue of approximately $10.7 billion, up 30% year-on-year. Within this, we expect Q4 AI semiconductor revenue of $6.2 billion, up 66% year-on-year. We expect infrastructure software revenue of approximately $6.7 billion, up 15% year-on-year.\n\n**Kirsten Spears** (CFO &amp; Chief Accounting Officer)\nFor your modeling purposes, we expect Q4 consolidated gross margin to be down approximately 70 basis points sequentially, primarily reflecting a higher mix of XPUs and also wireless revenue. As a reminder, consolidated gross margins through the year will be impacted by the revenue mix of infrastructure software and semiconductors and product mix within semiconductors. We expect Q4 adjusted EBITDA to be 67%. We expect the non-GAAP tax rate for Q4 and fiscal year 2025 to remain at 14%. I will now pass the call back to Hock for some more exciting news.\n\n**Hock Tan** (President, CEO &amp; Director)\nI don't know about exciting, Kirsten, but I do. I thought before we move to questions, I should share an update. The board and I have agreed that I will continue as the CEO of Broadcom through 2030, at least. These are exciting times for Broadcom, and I'm very enthusiastic to continue to drive value for our shareholders. Operator, please open up the call for questions.\n\n**Operator**\nThank you. To ask a question, you will need to press *11 on your telephone. To withdraw your question, press *11 again. Due to time restraints, we ask that you please limit yourself to one question. Please stand by while we compile the Q&A roster. Our first question will come from the line of Ross Seymore with Deutsche Bank. Your line is open.\n\n**Ross Seymore** (Managing Director)\nHi, guys. Thanks for having me ask the question. Hock, thank you for sticking around for a few more years. I just wanted to talk about the AI business and specifically the XPU. When you said you're going to grow significantly faster than what you had thought a quarter ago, what's changed? Is it just the impressive prospect moving to a customer definition, that $10 billion backlog that you mentioned? Is it stronger demand across the existing three customers? Any detail on that would be helpful.\n\n**Hock Tan** (President, CEO &amp; Director)\nI think it's both, Ross, but to a large extent, it's the fourth customer that we now add on to our roster, which we will ship pretty strongly in 2026, beginning 2026, I should say. It's a combination of increasing volumes from our existing three customers, and we move through that very progressively and steadily. The addition of a fourth customer with immediate and fairly substantial demand really changes our thinking of what 2026 would be starting to look like.\n\n**Ross Seymore** (Managing Director)\nThank you.\n\n**Operator**\nOne moment for our next question. That will come from the line of Harlan Sur with JPMorgan. Your line is open.\n\n**Harlan Sur** (Executive Director - Equity Research)\nHi, good afternoon. Congratulations on a well-executed quarter and strong free cash flow. I know everybody's going to ask a lot of questions on AI, Hock. I'm going to ask about the non-AI semi-business. If I look at your guidance for Q4, it looks like the non-AI semi-business is going to be down about 7%, 8% year-over-year in fiscal 2025 if you hit the midpoint of the Q4 guidance. Good news is that the negative year-over-year trends have been improving through the year. In fact, I think you guys are going to be positive year-over-year in the fourth quarter. You've characterized it as relatively close to the cyclical bottom, relatively slow to recover. However, we have seen some green shoots of positivity, right? Broadband, server storage, enterprise networking, you're still driving the DOCSIS 4 upgrade in broadband cable. You've got next-gen PON upgrades in China and the U.S.\n\n**Harlan Sur** (Executive Director - Equity Research)\nin front of you. Enterprise spending on network upgrades is accelerating. Near-term, from the cyclical bottom, how should we think about the magnitude of the cyclical upturn? Given your 30 to 40-week lead times, are you seeing continued order improvements in the non-AI segment, which would point you to continued cyclical recovery into next fiscal year?\n\n**Hock Tan** (President, CEO &amp; Director)\nIf you take a look at that non-AI segment, you're right. From a year-on-year Q4 guidance, we are actually up, as you say, slightly, a couple, 1% or 2% from a year ago. It's not much really to shout about at this point. The big issue is the puts and takes. The puts and takes and the bottom line to all this is, other than seasonality that we perceive, if you look at it short term, we are looking year-on-year, but looking sequentially, we see in things like wireless, and we even start to see some seasonality in server storage these days. It kind of all washes out so far. The only consistent trend we've seen over the last three quarters that is moving up strongly is broadband.\n\n**Hock Tan** (President, CEO &amp; Director)\nNothing else, if you look at it from a cyclical point of view, seems to be able to sustain an uptrend so far. I don't think it's getting, but as a whole, they're not getting worse, as you pointed out, Harlan. They're not showing a V-shaped recovery as a whole that we would like to see and expect to see in cyclical semiconductor cycles. The only thing that gives us some hope is broadband at this point, and it is recovering very strongly. It was the business that was most impacted in the sharp downturn of 2024 and early 2025. One takes that with a grain of salt. The best answer for you is non-AI semiconductor is kind of slow to recover, as I said. Q4 year-on-year is up maybe low single digits is the best way to describe it at this point.\n\n**Hock Tan** (President, CEO &amp; Director)\nI'm expecting to see more of a U-shaped recovery in non-AI, and perhaps by mid 2026, late 2026, we start to see some meaningful recovery. As of right now, not clear.\n\n**Harlan Sur** (Executive Director - Equity Research)\nAre you starting to see that in your order trend, in your order book, just because your lead times are like 40 weeks, right?\n\n**Hock Tan** (President, CEO &amp; Director)\nWe have been quick before. The bookings are up, and they're up year-on-year in excess of 20%. Nothing like what AI bookings look like, but 23% is still pretty good, right?\n\n**Operator**\nThank you. One moment for our next question. That will come from the line of Vivek Arya with Bank of America. Your line is open.\n\n**Vivek Arya** (Managing Director)\nThanks for taking my question, and best wishes, Hock, for the next part of your tenure. My question is on, you know, if you could help us quantify what is the new fiscal 2026 AI guidance, because I think the last call you mentioned, fiscal 2026 could grow at the 60% growth rate. What is the updated number? Is it 60% plus the $10 billion that you mentioned? Related to that, do you expect the custom versus networking mix to stay broadly what it has been this past year or evolve more towards custom? Any quantification on this, you know, networking versus custom mix would be really helpful for fiscal 2026.\n\n**Hock Tan** (President, CEO &amp; Director)\nOkay, let's answer the first part first. If I could be so bold as to suggest to you, when I last quarter, when I said, hey, the trend of growth of 2026 will mirror that of 2025, which is 50, 60% year-on-year. That's really all I said. I didn't quote, but of course, it comes up 50, 60% because that's what 2025 is. All I'm saying, if you want to put another way of looking at what I'm saying, which is perhaps more accurate, is we're seeing the growth rate accelerate as opposed to just remain steady at that 50, 60%. We are expecting and seeing 2026 to accelerate more than the growth rate we see in 2025. I know you love me to throw in a number at you, but you know what? We're not supposed to be giving you a forecast for 2026.\n\n**Hock Tan** (President, CEO &amp; Director)\nThe best way to describe it, it will be fairly material improvement.\n\n**Vivek Arya** (Managing Director)\nThe networking versus custom?\n\n**Hock Tan** (President, CEO &amp; Director)\nGood point. Thanks for reminding me. As we see, a big part of this driver of growth will be XPUs. At the risk of repeating what I said in my remarks, it comes from the fact that we continue to gain share at our three original customers. They have to, they're on their journey, and each passing generation, they go more to XPUs. We are gaining share from these three. We now have the benefit of an additional fourth significant customer. I should say fourth and very significant customer. That combination will mean more XPUs. As I said, as the ratio, as we create more and more XPUs among four guys, the networking, we get the networking with these four guys, but now the mix of networking from outside these four guys will now be a smaller, be diluted, be a smaller share.\n\n**Hock Tan** (President, CEO &amp; Director)\nI expect actually networking percentage of the pool to be a declining percentage going into 2026.\n\n**Vivek Arya** (Managing Director)\nThank you, Hock.\n\n**Operator**\nOne moment for our next question. That will come from the line of Stacy Rasgon with Bernstein Research. Your line is open.\n\n**Stacy Rasgon** (MD &amp; Senior Analyst)\nHi, guys. Thanks for taking my question. I was wondering if you could help me parse out this $110 billion backlog. Did I hear that number right? Could you give us some color on the makeup of that? How far out does that go? How much of that $110 billion is AI versus non-AI versus software?\n\n**Hock Tan** (President, CEO &amp; Director)\nStacy, we generally don't break our backlog. I'm just giving you a total number to give you a sense of how strong the business is as a whole for the company. It's largely driven by AI in terms of growth. Software continues to add on a steady basis, and non-AI, as I indicated, has grown double digits. Nothing compared to AI, which has grown very strongly. Give you a sense, perhaps fully 50% of it at least is semiconductors.\n\n**Stacy Rasgon** (MD &amp; Senior Analyst)\nOkay. It's fair to say that of that semiconductor piece, it's going to be much more AI than non-AI.\n\n**Hock Tan** (President, CEO &amp; Director)\nRight.\n\n**Stacy Rasgon** (MD &amp; Senior Analyst)\nYeah, got it. That's helpful. Thank you.\n\n**Operator**\nOne moment for our next question. That will come from the line of Ben Reitzes with Melius. Your line is open.\n\n**Ben Reitzes** (MD &amp; Head - Technology Research)\nHey, guys. Thanks a lot. I appreciate it. Hock, congrats on being able to guide to the AI revenue well above 60% for next year. I wanted to be a little greedy and ask you about maybe 2027 and the other three customers or so. How's the dialogue going beyond these four customers? In the past, you've talked about having seven. Now we've added a fourth to production, and then there were three. Are you hearing from others? How's the trend going maybe with the other three, maybe beyond the 2026, into 2027 and beyond? How's that momentum, you think, going to shape up? Thanks so much.\n\n**Hock Tan** (President, CEO &amp; Director)\nBen, you are definitely greedy and definitely overthinking this for me. Thank you. That's asking for subjective qualification. Frankly, I don't want to give that. I'm not comfortable giving that because sometimes we stumble into production in timeframes that are fairly unexpected, surprisingly. Equally, it could get delayed. I'd rather not give you any more color on prospects than to tell you these prospects are real prospects and continue to be very closely engaged towards developing each of their own XPUs with every intent of going into substantial production, like the four we have today who are custom.\n\n**Ben Reitzes** (MD &amp; Head - Technology Research)\nYeah, you still think that million units by, you know, goal for these seven, though, is still intact?\n\n**Hock Tan** (President, CEO &amp; Director)\nOh, for the three, I said. Now they're four. That's still in only for the customers. For the prospects, no comment. I'm not positioned to judge on that. For our three, four customers now, yes.\n\n**Ben Reitzes** (MD &amp; Head - Technology Research)\nAll right. Thanks a lot. Congrats.\n\n**Operator**\nOne moment for our next question. That will come from the line of Jim Schneider with Goldman Sachs. Your line is open.\n\n**Jim Schneider** (Senior Equity Analyst)\nGood afternoon. Thanks for taking my question. Hock, I was wondering if you could give us a little bit more color, not necessarily on the prospects which you still have in the pipeline, but how you view the universe of additional prospects beyond the seven customers and prospects you've already identified. Do you still see there being additional prospects that would be worthy of a custom chip? I know you've been relatively circumspect in terms of the number of customers that are out there and the volume that they can provide and selective in terms of the opportunities you're interested in. Maybe frame for us the additional prospects as you see them beyond the seven. Thank you.\n\n**Hock Tan** (President, CEO &amp; Director)\nThat's a very good question. Let me answer it in a fairly broader basis. As I said before, and perhaps repeat a bit more, we look at this market in two broad segments. One is simply the guys, the parties, the customers who develop their own LLM. The rest of the other market I consider is collectively lumped as enterprise. That is, markets that run, that will run AI workloads for enterprise, whether it's on-prem or GPU, XPU, or whatever as a service, the enterprise. We don't address that market, to be honest. We don't. That's because that's a hard market for us to address, and we're not set up to address that. We instead address this LLM market.\n\n**Hock Tan** (President, CEO &amp; Director)\nAs I said many times before, it's a very few narrow markets, few players driving frontier models on a consistent, on a very accelerated trend towards superintelligence for one, plagiarizing the term of someone else, but you know what I mean. There are the other guys who would invest, who need to invest a lot initially, my view, on training, training of ever larger and larger clusters of ever more capable accelerators. Also, as for these guys, they got to be accountable to shareholders or accountable to being able to create cash flows that can sustain their path. They start to also invest in inference in a massive way to monetize their models. These are the players we work with. These are individually people or players who spend a lot of money on a lot of compute capacity. It's just that there are only so few of them.\n\n**Hock Tan** (President, CEO &amp; Director)\nI have indicated, identified seven, four of which now are customers, three continue to be prospects we engage with. We're very careful, I should say, shouldn't use the word picky, careful who qualifies under them. I indicated it. They are building a platform or have a platform, and are investing very much on leading LLMs models. We have seven, and I think that's about it. We may see one more, perhaps, as a prospect. Again, we are very thoughtful and careful about even making that qualification. Right now, for sure, we have seven. That's for now, it's pretty much what we have.\n\n**Jim Schneider** (Senior Equity Analyst)\nThank you.\n\n**Operator**\nOne moment for our next question. That will come from the line of Tom O'Malley with Barclays. Your line is open.\n\n**Thomas O'Malley** (Director - Equity Research)\nHey, guys. Thanks for taking my question and congrats on the really good results. I wanted to ask on the Jericho 4 commentary. NVIDIA talked about the XGS switch and now is talking about scale across. You're talking about Jericho 4. It sounds like this market is really starting to develop. Maybe you could talk about when you see material uplift in revenue there and why it's important to start thinking about those types of switches as we move more towards inferencing. Thank you, Hock.\n\n**Hock Tan** (President, CEO &amp; Director)\nGreat. Thanks for picking that up. Yes, scale across is the new term now, right, thrown in. There's scale-up, which is within a rack, you know, within a rack, which is computing within the rack. Scale-out, doing across racks, but within a data center. Now when you get to clusters that are, I'm not 100% sure where the cutoff is, but say above 100,000 GPU or XPUs, that's you're talking about probably many cases because of limitation of power shell that the data center that you don't do one single data center footprint site to hand to sit with over 100,000 of those XPUs in one site. Power may not be easily available. Land may not be. It's cumbersome. Many, some outcomes, most of all our customers now we see create multiple data center sites close at hand, not far away, within range 100 kilometers.\n\n**Hock Tan** (President, CEO &amp; Director)\nIt's kind of the level, but be able to then put in homogeneous XPUs or GPUs in this multiple location, three or four, and network across them so that they behave like, in fact, a single cluster. That's the coolest part. That technology, which requires, because of distance, deep buffering, very intelligent congestion control, is technology that exists for many, many years in the likes of the telcos of AT&T and Verizon doing network routing, except this is for even somewhat more trickier workload, but the same. We've been shipping that to a couple of hyperscalers over the last two years as Jericho 3. As the scale of these clusters and the bandwidth required for AI training extends, we now launched this Jericho 4, 51 terabit per second to handle more bandwidth. Same technology we have tested, proven for the last 10, 20 years. Nothing new.\n\n**Hock Tan** (President, CEO &amp; Director)\nDon't need to create something new for that. It's running in Ethernet and very proven, very stable. As I said, last two years under Jericho 3, which runs 256 connections on no compute nodes, we've been selling to a couple of our hyperscale customers.\n\n**Operator**\nOne moment for our next question. That will come from the line of Carl Ackerman with BNP Paribas. Your line is open.\n\n**Karl Ackerman** (MD - Equity Research, Semiconductors &amp; IT Hardware)\nYeah, thank you. Hock, have you completely converted your top 10,000 accounts from vSphere to the entire VMware Cloud Foundation virtualization stack? I ask because I think last quarter, 87% of accounts had adopted that, and that's certainly a marked increase versus less than 10% of those customers who bought the entire suite before the deal. As you address that, what interest level are you seeing with the longer tail of enterprise customers adopting VCF? Are you seeing tangible cross-selling benefits of your merchant semiconductor storage and networking business as those customers adopt VMware? Thank you.\n\n**Hock Tan** (President, CEO &amp; Director)\nOkay. To answer your first part of the question, yeah, pretty much virtually, way over 90% has bought VCF. Now, I'm careful about the choice of words. Because we have sold them on it and they've bought licenses to deploy it, it doesn't mean they're fully deployed. Here comes the other part of our work, which is to take these 10,000 customers or a big chunk of them who have taken, who have bought the vision of a private cloud on-prem and working with them to enable them to deploy it and operate it successfully on their infrastructure on-prem. That's the hard work over the next two years that we see happening. As we do it, we see expansion across their IT footprint on VCF, private cloud running on their data set within their data center. That's the key part of it. We see that continuing.\n\n**Hock Tan** (President, CEO &amp; Director)\nThat's the second phase of my VMware story. The first phase is convince these people to convert from perpetual subscription and so doing purchase VCF. The second phase now is make that purchase they made on VCF create the value they look for in private cloud on their premise, on their IT data center. That's what's happening. That will sustain for quite a while because on top of that, we will start selling advanced services, security, disaster recovery, even AI, running AI workloads on it. All that is very exciting. Your second question is, is that able to enable me to sell more hardware? No, it's quite independent. In fact, as they virtualize their data centers, we consciously accept the fact that we are commoditizing the underlying hardware in the data center, commoditizing servers, commoditizing storage, commoditizing even networking. That's fine.\n\n**Hock Tan** (President, CEO &amp; Director)\nBy so commoditizing, we're actually reducing the cost of investments in hardware in data centers for enterprises. Now, beyond the largest 10,000, are we seeing a lot of success? We're seeing some. Again, two reasons why we do not expect it to be necessarily successful. One is the value, the TCO, as they call it, that comes from it will be much less. The more important thing is the skill sets that need to not just deploy that you can get services and ourselves to help them, but to keep operating it might not be something that they can take on. We shall see. This is an area we're still learning, and it'll be interesting to see. VMware has 300,000 customers. We see the top 10,000 as making for as being people where it makes a lot of sense, derive a lot of value in deploying private cloud using VCF.\n\n**Hock Tan** (President, CEO &amp; Director)\nWe now are looking at whether the next 20,000, 30,000 mid-sized companies see the same way. Stay tuned. I'll let you know.\n\n**Karl Ackerman** (MD - Equity Research, Semiconductors &amp; IT Hardware)\nVery clear. Thank you.\n\n**Operator**\nOne moment for our next question. That will come from the line of CJ Muse with Cantor Fitzgerald. Your line is open.\n\n**CJ Muse** (Senior Managing Director)\nYeah, good afternoon. Thanks for taking the question. I was hoping to focus on gross margins. I understand that they are down 70 bps, particularly with software lower sequentially and greater contributions from wireless and XPU. To hit that 77.7%, I either have to model semiconductor margins flat, which I would think would be lower, or software gross margins to 95%, up 200 bps. Can you help me better understand the moving parts there to allow only a 70 bp drop?\n\n**Kirsten Spears** (CFO &amp; Chief Accounting Officer)\nYeah. I mean, the TPUs will be going up along with wireless, as I said on the call. Software revenue will be coming up just a bit as well.\n\n**CJ Muse** (Senior Managing Director)\nYou mean TPUs?\n\n**Kirsten Spears** (CFO &amp; Chief Accounting Officer)\nXPUs, yes.\n\n**Operator**\nOne last question.\n\n**Kirsten Spears** (CFO &amp; Chief Accounting Officer)\nWireless is typically our heaviest quarter, right, of the year for wireless. You have wireless and TPUs with generally lower margins, right, and then our software revenue coming up.\n\n**Operator**\nOne moment for our next question. That will come from the line of Joe Moore with Morgan Stanley. Your line is open.\n\n**Joseph Moore** (Managing Director)\nGreat. Thank you. In terms of the fourth customer, I think you've talked in the past about potential customers four and five were more hyperscale, and six and seven were more like, you know, the LLM makers themselves. Can you give us a sense for if you could help us categorize that? If not, that's fine. The $10 billion of orders, can you give us a timeframe on that? Thank you.\n\n**Hock Tan** (President, CEO &amp; Director)\nOkay. Yeah. No, it's towards the end of the day, all seven do LLMs. Not all of them have a current, have the huge platform we're talking about. One could imagine eventually all of them will have or create a platform. It's hard to differentiate the two. Coming on the second and third delivery of the $10 billion, I'll probably be in around, I would say, the second half of our fiscal year 2026. I would say, to be even more precise, likely to be Q3 of our fiscal 2026.\n\n**Joseph Moore** (Managing Director)\nQ3, it starts, or Q3, what timeframe does it take to deploy $10 billion?\n\n**Hock Tan** (President, CEO &amp; Director)\nIt starts and ends in Q3.\n\n**Joseph Moore** (Managing Director)\nOkay. All right. Thank you.\n\n**Operator**\nOne moment for our next question. That will come from the line of Joshua Buchhalter with TD Cowan. Your line is open.\n\n**Joshua Buchalter** (Director - Equity Research)\nHey, guys. Thank you for taking my question and congrats on the results. I was hoping you could provide some comments on momentum for scale-up Ethernet and how it compares with, you know, UA Link and PCIe solutions out there. How big of a, how meaningful is it to have the Tomahawk Ultras product out there with a lower latency? How meaningful do you think the scale-up Ethernet opportunity could be over the next year as we think about your AI networking business? Thank you.\n\n**Hock Tan** (President, CEO &amp; Director)\nThat's a good question. We ourselves are thinking about that too, because to begin with, Ethernet, our Ethernet solutions are very disaggregated from the AI accelerators anybody does. It's separate. We treat them as separate. Even though you're right, the network is the computer. We have always believed that Ethernet is open source. Anybody should be able to have choices, and we keep it separate from our XPU. The truth of the matter is, for our customers who use the XPU, we develop and we optimize our networking switches and other components that relate to being able to network signals in any clusters hand in hand with it. In fact, all these XPUs have developed with interface that handles Ethernet, very, very much so. In a way, with XPUs, with our customers, we are openly enabling Ethernet as a networking protocol of choice, very, very openly.\n\n**Hock Tan** (President, CEO &amp; Director)\nIt need not be our Ethernet switches. It could be any other, but somebody else's Ethernet switcher that does it. It just happens to be we're in the lead in this business, so we get that. Beyond it, especially when it comes to a closed system of GPUs, we see less of it, except in the hyperscalers, where the hyperscalers are able to architect the GPUs clusters very separate from the networking side, especially in scale-out. In which case, on those hyperscalers, we sell a lot of these Ethernet switches that do scaling out. We suspect when it goes to scaling across now, even more Ethernet that are disaggregated from the GPUs that are in the place. As far as XPUs are concerned, for sure, it's all Ethernet.\n\n**Joshua Buchalter** (Director - Equity Research)\nThank you.\n\n**Operator**\nOne moment for our next question. That will come from the line of Christopher Rolland with Susquehanna. Your line is open.\n\n**Christopher Rolland** (Senior Equity Analyst - Semiconducters)\nThank you for the question and congrats on the contract extension, Hock. My questions are about competition, both on the networking side and the ASIC side. You kind of answered some of that, I think, in the last question. Do you view any competition on the ASIC side, particularly from U.S. or Asian vendors, or do you think this is decreasing? On the networking side, do you think UA Link or PCIe even has a chance of displacing SUE in 2027 when it's expected to ramp? Thanks.\n\n**Hock Tan** (President, CEO &amp; Director)\nThank you for embracing SUE. Thank you. I didn't expect that to come out, and I appreciate that. You know I'm biased, to be honest. It's so obvious I can't help but being biased because Ethernet is well proven. Ethernet is so known to the engineers, the architects that sit in all these hyperscalers developing, designing AI data center, data AI infrastructure. It's the logical thing for them to use. They are using it, and they are focusing on it. The development of separate individualized protocols, frankly, it's beyond my imagination why they bought it. Ethernet is there. It's been well used. It's proven it can keep going up. The only thing people talk about is perhaps latency, especially in scaling up, hence the emergence of NVLink. Even then, as I indicated, it's not hard for us, and we are not the only ones who can do that.\n\n**Hock Tan** (President, CEO &amp; Director)\nQuite a few others in Ethernet can do it in the switches. You can just tweak the switches to make the latency super good, better than NVLink, better than Infiniband, less than 250 nanoseconds easily. That's what we did. It's not that hard. Perhaps I say that because we have been doing it as the Ethernet has been around the last 25 years at length. It's there, the technology. There's no need to go and create some cooked-up protocols that now you have to bring people around. Ethernet is the way to go. There's plenty of competition too because it's an open-source system. I think Ethernet is the way to go. For sure, in developing XPUs for our customers, all these XPUs with the agreement of customers are made compatible interface with Ethernet and not some fancy other interface that one has to keep going as bandwidth increases.\n\n**Hock Tan** (President, CEO &amp; Director)\nI assure you, we have competition, which is one of the reasons why the hyperscalers like Ethernet. It's not just us. They can find somebody else if for whatever reason they don't like us, and we're open to that. It's always good to do that. It's an open-source system, and there are players in that market, not any closed system. Switching on to XPU competition, we hear about competition and all that. It's just that it's a competition that's an area that we always see competition. Our only way to secure our position is we try to out-invest and out-innovate anybody else in this game. We have been fortunate to be the first one creating this XPU model of ASICs on silicon. We also have been fortunate to be probably one of the largest IP developers of semiconductor out there.\n\n**Hock Tan** (President, CEO &amp; Director)\nThings like serializer, deserializer sets, SERDES, being able to develop the best packaging, being able to design things that are very low power. We just have to keep investing in it, which we do, to outrun the competition in this space. I believe we're doing a fairly decent job of doing it at this point.\n\n**Christopher Rolland** (Senior Equity Analyst - Semiconducters)\nVery clear. Thanks, Hock.\n\n**Hock Tan** (President, CEO &amp; Director)\nSure.\n\n**Operator**\nThank you. We do have time for one last question. That will come from the line of Harsh Kumar with Piper Sandler. Your line is open.\n\n**Harsh Kumar** (MD &amp; Senior Research Analyst)\nHey, guys. Thanks for squeezing me in, Hock. Congratulations on all the exciting AI metrics, and thanks for everything you do for Broadcom and sticking around. Hock, my question is, you've got three to four existing customers that are ramping. As the data centers for AI clusters get bigger and bigger, it makes sense to have differentiation, efficiency, et cetera. Therefore, the case for XPUs. Why should I not think that your XPU share at these three or four customers that are existing will be bigger than the GPU share in the longer term?\n\n**Hock Tan** (President, CEO &amp; Director)\nIt will be. It's a logical conclusion. Hass, you're correct. We are seeing that step by step. As I say, it's a journey. It's a multi-year journey because it's multigenerational, because these XPUs don't stay still either. I'm doing multiple versions, at least two versions, two-generational versions for each of these customers we have. With each newer generation, they increase the consumption, the usage of the XPU. As they gain confidence, as the model improves, they deploy it even more. That's the logical trend that XPUs will keep in these few customers of ours, where they are successfully deployed and their software stabilizes, the software stack, the library that sits on these chips stabilizes and proves itself out. They will have the confidence to keep using a higher and higher % of their compute footprint in their own XPUs, for sure. We see that. That's why I say we progressively gain share.\n\n**Harsh Kumar** (MD &amp; Senior Research Analyst)\nThank you, Hock.\n\n**Operator**\nThank you. I would now like to turn the call back over to Ji Yoo, Head of Investor Relations, for any closing remarks.\n\n**Ji Yoo** (Investor Relations)\nThank you, Cheri. This quarter, Broadcom will be presenting at the Goldman Sachs Communicopia and Technology Conference on Tuesday, September 9, in San Francisco, and at the JPMorgan US All-Stars Conference on Tuesday, September 16, in London. Broadcom currently plans to report its earnings for the fourth quarter and fiscal year 2025 after close of market on Thursday, December 11, 2025. A public webcast of Broadcom's earnings conference call will follow at 2:00 P.M. Pacific. That will conclude our earnings call today. Thank you all for joining. Cheri, you may end the call.\n\n**Operator**\nThis concludes today's program. Thank you all for participating. You may now.",
      "fetched_at": "2026-02-04T15:40:00.447Z"
    },
    {
      "ticker": "AVGO",
      "title": "Broadcom Inc. (AVGO) Q2 FY2025 earnings call transcript",
      "published_date": "Jun 5, 2025, 5:00 PM EDT",
      "fiscal_year": "2025",
      "quarter": "Q2",
      "url": "https://finance.yahoo.com/quote/AVGO/earnings/AVGO-Q2-2025-earnings_call-325316.html",
      "content": "**Operator**\nWelcome to Broadcom's second quarter fiscal year 2025 financial results conference call. At this time, for opening remarks and introductions, I would like to turn the call over to Goo Gai, Head of Investor Relations of Broadcom.\n\n**Jihyung Yoo** (Head of Investor Relations)\nThank you, Operator, and good afternoon, everyone. Joining me on today's call are Hock Tan, President and CEO; Kirsten Spears, Chief Financial Officer; and Charlie Kawwas, President, Semiconductor Solutions Group. Broadcom distributed a press release and financial tables after the market closed, describing our financial performance for the second quarter of fiscal year 2025. If you did not receive a copy, you may obtain the information from the investor section of Broadcom's website at broadcom.com. This conference call is being webcast live, and an audio replay of the call can be accessed for one year through the investor section of Broadcom's website. During the prepared comments, Hock and Kirsten will be providing details of our second quarter fiscal year 2025 results, guidance for our third quarter of fiscal year 2025, as well as commentary regarding the business environment. We'll take questions after the end of our prepared comments.\n\n**Jihyung Yoo** (Head of Investor Relations)\nPlease refer to our press release today and our recent filings with the SEC for information on the specific risk factors that could cause our actual results to differ materially from the forward-looking statements made on this call. In addition to U.S. GAAP reporting, Broadcom reports certain financial measures on a non-GAAP basis. A reconciliation between GAAP and non-GAAP measures is included in the tables attached to today's press release. Comments made during today's call will primarily refer to our non-GAAP financial results. I will now turn the call over to Hock.\n\n**Hock Tan** (CEO)\nThank you, Goo, and thank you, everyone, for joining us today. In our fiscal Q2 2025, total revenue was a record $15 billion, up 20% year on year. This 20% year on year growth was all organic, as Q2 last year was the first full quarter with VMware. Now, revenue was driven by continued strength in AI semiconductors and the momentum we have achieved in VMware. Now, reflecting excellent operating leverage, Q2 consolidated adjusted EBITDA was $10 billion, up 35% year on year. Now, let me provide more color. Q2 semiconductor revenue was $8.4 billion, with growth accelerating to 17% year on year, up from 11% in Q1. And of course, driving this growth was AI semiconductor revenue of over $4.4 billion, which is up 46% year on year and continues the trajectory of nine consecutive quarters of strong growth.\n\n**Hock Tan** (CEO)\nWithin this, custom AI accelerators grew double digits year on year, while AI networking grew over 170% year on year. AI networking, which is based on Ethernet, was robust and represented 40% of our AI revenue. As a standards-based open protocol, Ethernet enables one single fabric for both scale-out and scale-up, and remains the preferred choice by our hyperscale customers. Our networking portfolio of Tomahawk switches, Jericho routers, and NICs is what's driving our success within AI clusters in hyperscalers. The momentum continues with our breakthrough Tomahawk 6 switch just announced this week. This represents the next generation 102.4 terabits per second switch capacity. Tomahawk 6 enables clusters of more than 100,000 AI accelerators to be deployed in just two tiers instead of three.\n\n**Hock Tan** (CEO)\nThis flattening of the AI cluster is huge because it enables much better performance in training next generation frontier models through a lower latency, higher bandwidth, and lower power. Turning to XPUs, or custom accelerators, we continue to make excellent progress on the multi-year journey of enabling our three customers and four prospects to deploy custom AI accelerators. As we had articulated over six months ago, we eventually expect at least three customers to each deploy 1 million AI accelerator clusters in 2027, largely for training their frontier models. We forecast and continue to do so a significant percentage of these deployments to be custom XPUs. These partners are still unwavering in their plan to invest despite this certain economic environment. In fact, what we've seen recently is that they are doubling down on inference in order to monetize their platforms.\n\n**Hock Tan** (CEO)\nReflecting this, we may actually see an acceleration of XPU demand into the back half of 2026 to meet urgent demand for inference on top of the demand we have indicated from training. Accordingly, we do anticipate now our fiscal 2025 growth rate of AI semiconductor revenue to sustain into fiscal 2026. Turning to our Q3 outlook, as we continue our current trajectory of growth, we forecast AI semiconductor revenue to be $5.1 billion, up 60% year on year, which would be the 10th consecutive quarter of growth. Turning to non-AI semiconductors in Q2, revenue of $4 billion was down 5% year on year. Non-AI semiconductor revenue is close to the bottom and has been relatively slow to recover. There are bright spots. In Q2, broadband, enterprise networking, and server storage revenues were up sequentially.\n\n**Hock Tan** (CEO)\nHowever, industrial was down, and as expected, wireless was also down due to seasonality. In Q3, we expect enterprise networking and broadband to continue to grow sequentially, but server storage, wireless, and industrial are expected to be largely flat. Overall, we forecast non-AI semiconductor revenue to stay around $4 billion. Now, let me talk about our infrastructure software segment. Q2 infrastructure software revenue of $6.6 billion was up 25% year on year, above our outlook of $6.5 billion. As we have said before, this growth reflects our success in converting our enterprise customers from perpetual vSphere to the full VCF software stack subscription. Customers are increasingly turning to VCF to create a modernized private cloud on-prem, which will enable them to repatriate workloads from public clouds while being able to run modern container-based applications and AI applications. Of our 10,000 largest customers, over 87% have now adopted VCF.\n\n**Hock Tan** (CEO)\nThe momentum from strong VCF sales over the past 18 months since the acquisition of VMware has created annual recurring revenue, or otherwise known as ARR, growth of double digits in our core infrastructure software. In Q3, we expect infrastructure software revenue to be approximately $6.7 billion, up 16% year on year. In total, we're guiding Q3 consolidated revenue to be approximately $15.8 billion, up 21% year on year. We expect Q3 adjusted EBITDA to be at least 66%. With that, let me turn the call over to Kirsten.\n\n**Kirsten Spears** (CFO)\nThank you, Hock. Let me now provide additional detail on our Q2 financial performance. Consolidated revenue was a record $15 billion for the quarter, up 20% from a year ago. Gross margin was 79.4% of revenue in the quarter, better than we originally guided on product mix. Consolidated operating expenses were $2.1 billion, of which $1.5 billion was related to R&D. Q2 operating income of $9.8 billion was up 37% from a year ago, with operating margin at 65% of revenue. Adjusted EBITDA was $10 billion, or 67% of revenue, above our guidance of 66%. This figure excludes $142 million of depreciation. Now, a review of the P&L for our two segments, starting with semiconductors. Revenue for our semiconductor solution segment was $8.4 billion, with growth accelerating to 17% year on year, driven by AI. Semiconductor revenue represented 56% of total revenue in the quarter.\n\n**Kirsten Spears** (CFO)\nGross margin for our semiconductor solution segment was approximately 69%, up 140 basis points year on year, driven by product mix. Operating expenses increased 12% year on year to $971 million on increased investment in R&D for leading-edge AI semiconductors. Semiconductor operating margin of 57% was up 200 basis points year on year. Now, moving on to infrastructure software. Revenue for infrastructure software of $6.6 billion was up 25% year on year and represented 44% of total revenue. Gross margin for infrastructure software was 93% in the quarter compared to 88% a year ago. Operating expenses were $1.1 billion in the quarter, resulting in infrastructure software operating margin of approximately 76%. This compares to operating margin of 60% a year ago. This year-on-year improvement reflects our disciplined integration of VMware. Moving on to cash flow. Free cash flow in the quarter was $6.4 billion and represented 43% of revenue.\n\n**Kirsten Spears** (CFO)\nFree cash flow as a percentage of revenue continues to be impacted by increased interest expense from debt related to the VMware acquisition and increased cash taxes. We spent $144 million on capital expenditures. Day sales outstanding were 34 days in the second quarter compared to 40 days a year ago. We ended the second quarter with inventory of $2 billion, up 6% sequentially in anticipation of revenue growth in future quarters. Our days of inventory on hand were 69 days in Q2, as we continue to remain disciplined on how we manage inventory across the ecosystem. We ended the second quarter with $9.5 billion of cash and $69.4 billion of gross principal debt. Subsequent to quarter end, we repaid $1.6 billion of debt, resulting in gross principal debt of $67.8 billion.\n\n**Kirsten Spears** (CFO)\nThe weighted average coupon rate and years to maturity of our $59.8 billion in fixed-rate debt is 3.8% and 7 years, respectively. The weighted average interest rate and years to maturity of our $8 billion in floating-rate debt is 5.3% and 2.6 years, respectively. Turning to capital allocation, in Q2, we paid stockholders $2.8 billion of cash dividends based on a quarterly common stock cash dividend of $0.59 per share. In Q2, we repurchased $4.2 billion, or approximately 25 million shares of common stock. In Q3, we expect the non-GAAP diluted share count to be approximately 4.97 billion shares, excluding the potential impact of any share repurchases. Now, moving on to guidance. Our guidance for Q3 is for consolidated revenue of $15.8 billion, up 21% year on year. We forecast semiconductor revenue of approximately $9.1 billion, up 25% year on year.\n\n**Kirsten Spears** (CFO)\nWithin this, we expect Q3 AI semiconductor revenue of $5.1 billion, up 60% year on year. We expect infrastructure software revenue of approximately $6.7 billion, up 16% year on year. For modeling purposes, we expect Q3 consolidated gross margin to be down approximately 130 basis points sequentially, primarily reflecting a higher mix of XPUs within AI revenue. As a reminder, consolidated gross margins through the year will be impacted by the revenue mix of infrastructure software and semiconductors. We expect Q3 adjusted EBITDA to be at least 66%. We expect the non-GAAP tax rate for Q3 in fiscal year 2025 to remain at 14%. With this, that concludes my prepared remarks. Operator, please open up the call for questions.\n\n**Operator**\nThank you. To ask a question, you will need to press star 11 on your telephone. To withdraw your question, please press star 11 again. Due to time restraints, we ask that you please limit yourself to one question. Please stand by while we compile the Q&A roster. Our first question will come from the line of Ross Seymore with Deutsche Bank. Your line is open.\n\n**Ross Seymore** (Managing Director)\nHi, guys. Thanks for letting me ask a question. Hock, I wanted to jump onto the AI side and specifically some of the commentary you had about next year. Can you just give a little bit more color on the inference commentary you gave? Is it more the XPU side, the connectivity side, or both that's giving you the confidence to talk about the growth rate that you have this year being matched next fiscal year?\n\n**Hock Tan** (CEO)\nThank you, Ross. Good question. I think we're indicating that what we are seeing and what we have quite a bit of visibility increasingly is increased deployment of XPUs next year, much more than we originally thought, and hand in hand with it, of course, more and more networking. It is a combination of both.\n\n**Ross Seymore** (Managing Director)\nThe inference side of things?\n\n**Hock Tan** (CEO)\nYeah. We're seeing much more inference now.\n\n**Ross Seymore** (Managing Director)\nThank you.\n\n**Operator**\nThank you. One moment for our next question. That will come from the line of Harlan Sur with JPMorgan. Your line is open.\n\n**Harlan Sur** (Executive Director of Equity Research)\nGood afternoon. Thanks for taking my question and great job on the quarterly execution. Hock, good to see the positive growth inflection quarter to quarter, year over year growth rates in your AI business. As the team has mentioned, the quarters can be a bit lumpy. If I smooth out kind of first three quarters of this fiscal year, your AI business is up 60% year over year. It is kind of right in line with your three-year kind of SAM growth figure. Given your prepared remarks and knowing that your lead times remain at 35 weeks or better, do you see the Broadcom team sustaining the 60% year over year growth rate exiting this year?\n\n**Harlan Sur** (Executive Director of Equity Research)\nI assume that that potentially implies that you see your AI business sustaining the 60% year over year growth rate into fiscal 2026, again, based on your prepared commentary, which again is in line with your SAM growth figure. Is that kind of a fair way to think about the trajectory this year and next year?\n\n**Hock Tan** (CEO)\nHarlan, that's a very insightful set of analysis here. That's exactly what we're trying to do here because over six months ago, we gave you guys a point, a year, 2027. As we come into the second half of 2025 and with improved visibility and updates we're seeing in the way our hyperscale partners are deploying data centers, AI clusters, we are providing you some level of guidance, visibility, what we are seeing, how the trajectory of 2026 might look like. I'm not giving you any update on 2027. We're just still establishing the update we have in 2027 six months ago. What we're doing now is giving you more visibility into where we're seeing 2026 headed.\n\n**Harlan Sur** (Executive Director of Equity Research)\nIs the framework that you laid out for us, like second half of last year, which implies 60% kind of growth figure in your SAM opportunity, is that kind of the right way to think about it as it relates to the profile of growth in your business this year and next year?\n\n**Hock Tan** (CEO)\nYes.\n\n**Harlan Sur** (Executive Director of Equity Research)\nOkay. Thank you, Hock.\n\n**Operator**\nThank you. One moment for our next question. That will come from the line of Ben Reitzes with Melius Research. Your line is open.\n\n**Ben Reitzes** (Managing Director and Head of Technology Research)\nHey, how you doing? Thanks, guys. Hey, Hock. AI networking was really strong in the quarter, and it seemed like it must have beat expectations. I was wondering if you could just talk about the networking in particular, what caused that, and how much of that is your acceleration into next year? When do you think you see Tomahawk kicking in as part of that acceleration? Thanks.\n\n**Hock Tan** (CEO)\nI think AI networking, as you probably would know, goes pretty hand in hand with deployment of AI accelerator clusters. It does not deploy on a timetable that is very different from the way the accelerators get deployed, whether they are XPUs or GPUs. It does happen. They deploy a lot in scale-out where Ethernet, of course, is the choice or protocol, but it is also increasingly moving into the space of what we all call scale-up within those data centers, where you have much higher, more than we originally thought, consumption or density of switches than you have in the scale-out scenario. In fact, the increased density in scale-up is 5-10 times more than in scale-out.\n\n**Hock Tan** (CEO)\nThat is the part that kind of pleasantly surprised us, which is why this past quarter, Q2, the AI networking portion continues at about 40% from what we reported a quarter ago for Q1. At that time, I said I expect it to drop. It has not.\n\n**Ben Reitzes** (Managing Director and Head of Technology Research)\nYour thoughts on Tomahawk driving acceleration for next year and when it kicks in?\n\n**Hock Tan** (CEO)\nOh, Tomahawk 6. Oh, yeah. That's extremely strong interest. Now, we're not shipping big orders or any orders other than basic proof of concepts out to customers, but there is tremendous demand for this new 102 terabit per second Tomahawk switches.\n\n**Ross Seymore** (Managing Director)\nThanks, Hock.\n\n**Operator**\nThank you. One moment for our next question. That will come from the line of Blaine Curtis with Jefferies. Your line is open.\n\n**Blayne Curtis** (Managing Director)\nHey. Thanks. Great results. I just wanted to ask maybe following up on the scale-out opportunity. Today, I guess your main customer is not really using an NVLink switch-style scale-up. I'm just kind of curious your visibility or the timing in terms of when you might be shipping a switch Ethernet scale-up network to your customers.\n\n**Hock Tan** (CEO)\nThe talking scale-up.\n\n**Blayne Curtis** (Managing Director)\nScale-up.\n\n**Hock Tan** (CEO)\nYeah. Scale-up is very rapidly converting to Ethernet now, very much so. For our fairly narrow band of hyperscale customers, scale-up is very much Ethernet.\n\n**Operator**\nThank you. One moment for our next question. That will come from the line of Stacy Rasgon with Bernstein. Your line is open.\n\n**Stacy Rasgon** (Managing Director and Senior Analyst of U.S. Semiconductors and Semiconductor Capital Equipment)\nHi, guys. Thanks for taking my questions. Hock, I still wanted to follow up on that AI 2026 question. I want to just put some numbers on it just to make sure I've got it right. So you did 60% in the first three quarters of this year. If you grow 60% year over year in Q4, it'd put you at, I don't know, $5.8 billion, something like $19 billion or $20 billion for the year. And then are you saying you're going to grow 60% in 2026, which would put you $30 billion plus in AI revenues for 2026? I just want to make sure. Is that the math that you're trying to communicate to us directly?\n\n**Hock Tan** (CEO)\nI think you're doing the math. I'm giving you the trend. I did answer that question, I think Harlan asked earlier. The rate we are seeing now so far in fiscal 2025, and we'll presumably continue. We don't see any reason why it doesn't, given lead time visibility in 2025. What we're seeing today, based on what we have visibility on 2026, is to be able to ramp up this AI revenue in the same trajectory. Yes.\n\n**Stacy Rasgon** (Analyst)\nIs the SAM going up? Is the SAM going up as well? Because now you have inference on top of training. Is the SAM still 60-90, or is the SAM higher now as you see it?\n\n**Hock Tan** (CEO)\nI'm not playing a SAM game here. I'm just giving a trajectory towards where we drew the line on 2027 before. So I have no response to is the SAM going up or not. Stop talking about SAM now. Thanks.\n\n**Stacy Rasgon** (Analyst)\nOkay. Thank you.\n\n**Operator**\nOne moment for our next question. That will come from the line of Vivek Arya with Bank of America. Your line is open.\n\n**Vivek Arya** (Managing Director)\nThanks for taking my question. I had a near and then a longer-term question on the XPU business. Hock, for near term, if your networking upside in Q2 and overall AI was in line, it means XPU was perhaps not as strong. I realize it's lumpy, but anything more to read into that, any product transition or anything else? Just a clarification there. Longer term, you have outlined a number of additional customers that you're working with. What milestones should we look forward to, and what milestones are you watching to give you the confidence that you can now start adding that addressable opportunity into your 2027 or 2028 or other numbers? How do we get the confidence that these projects are going to turn into revenue in some reasonable timeframe from now? Thank you.\n\n**Hock Tan** (CEO)\nOkay. On the first part that you're asking, it's like you're trying to count how many angels on the head of a pin here. I mean, whether it's XPU or networking. Networking is hot, but that doesn't mean XPU is any softer. It's very much along the trajectory we expect it to be. There's no lumpiness. There's no softening. It's pretty much what we expect the trajectory to go so far and into next quarter as well and probably beyond. We have a fairlyit's a fairly, I guess, in our view, a fairly clear visibility on the short-term trajectory. In terms of going on to 2027, no, we are not updating any numbers here. Six months ago, we drew a sense for the size of the SAM based on a million GPU XPU clusters for three customers.\n\n**Hock Tan** (CEO)\nThat is still very valid at that point that you will be there. We have not provided any further updates here, nor are we intending to at this point. When we get better visibility, a clearer sense of where we are, and that probably will not happen until 2026, we will be happy to give an update to the audience. Right now, though, in today's prepared remarks and answering a couple of questions, we are, as we have done here, intending to give you guys more visibility of what we are seeing, the growth trajectory in 2026.\n\n**Vivek Arya** (Managing Director)\nThank you, Hock.\n\n**Operator**\nThank you. One moment for our next question. That will come from the line of CJ Muse with Cantor Fitzgerald. Your line is open.\n\n**CJ Muse** (Senior Managing Director)\nYeah. Good afternoon. Thank you for taking the question. I was hoping to follow up on Ross's question regarding inference opportunity. Can you discuss workloads that are optimal that you're seeing for custom silicon? And that over time, what percentage of your XPU business could be inference versus training? Thank you.\n\n**Hock Tan** (CEO)\nI think there's no differentiation between training and inference in using merchant accelerators versus custom accelerators. I think the whole premise behind going towards custom accelerators continues, which is not a matter of cost alone. It is that as custom accelerators get used and get developed on a roadmap with any particular hyperscaler, there's a learning curve, a learning curve on how they could optimize the way their algorithms on their large language models get written and tied to silicon. That ability to do so is a huge value added in creating algorithms that can drive their LLMs to higher and higher performance, much more than basically a segregation approach between hardware and the software. You literally combine end-to-end hardware and software as they take that journey. It is a journey. They don't learn that in one year.\n\n**Hock Tan** (CEO)\nDo it a few cycles, get better and better at it, and then realize the fundamental value in creating your own hardware versus using a third-party merchant silicon. You are able to optimize your software to the hardware and eventually achieve way higher performance than you otherwise could. We see that happening.\n\n**Operator**\nThank you. One moment for our next question. That will come from the line of Carl Ackerman with BNP Paribas. Your line is open.\n\n**Karl Ackerman** (Managing Director and Equity Research of Semiconductors and IT Hardware)\nYes. Thank you. Hock, you spoke about the much higher content opportunity in scale-up networking. I was hoping you could discuss how important is demand adoption for copackage optics in achieving this 5-10x higher content for scale-up networks? Or should we anticipate much of the scale-up opportunity will be driven by Tomahawk and Thor NICs? Thank you.\n\n**Hock Tan** (CEO)\nI'm trying to decipher this question of yours. Let me try to answer it perhaps in a way I think you want me to clarify. First and foremost, I think most of what's scaling up, a lot of the scaling up that's going in, as I call it, which means a lot of XPU or GPU to GPU interconnects, is done on copper interconnects. Because the size of this scale-up cluster is still not that huge yet, you can get away with using copper interconnects. They're still doing it. Mostly, they're doing it today. At some point soon, I believe, when you start trying to go beyond maybe 72 GPU to GPU interconnects, you may have to push towards a different protocol mode at a different media from copper to optical.\n\n**Hock Tan** (CEO)\nWhen we do that, yeah, perhaps then things like exotic stuff like copackaging might become of silicon with optical might become relevant. Truly, what we really are talking about is that at some stage, as the clusters get larger, which means scale-up becomes much bigger, and you need to interconnect GPU or XPU to each other in scale-up, many more than just 72 or 100, maybe even 128. You start going more and more. You want to use optical interconnects simply because of distance. That is when optical will start replacing copper. When that happens, the question is, what's the best way to deliver on optical? One way is copackage optics, but it's not the only way. You can just simply use continuous, perhaps pluggable, at low-cost optics, in which case then you can interconnect the bandwidth, the radix of a switch.\n\n**Hock Tan** (CEO)\nOur switch is now 512 connections. You can now connect all these XPUs, GPUs, 512 for scale-up phenomenon. That was huge. When you go to optical, that's going to happen, in my view, within a year or two. We will be right in the forefront of it. It may be copackage optics, which we are very much in development, but it is a lock-in copackage. It could just be, as a first step, pluggable optics. Whatever it is, I think the bigger question is, when does it go from optical and from copper connecting GPU to GPU to optical connecting it? The step in that move will be huge. It is not necessarily copackage optics, though that is definitely one path we are pursuing.\n\n**Karl Ackerman** (Managing Director and Equity Research of Semiconductors and IT Hardware)\nVery clear. Thank you.\n\n**Operator**\nOne moment for our next question. That will come from the line of Joshua Buckhalter with TD Cowen. Your line is open.\n\n**Joshua Buchalter** (Director of Equity Research Semiconductors)\nHey, guys. Thank you for taking my question. I realize it's a bit nitpicky, but I wanted to ask about gross margins in the guide. Your revenue implies sort of $800 million incremental increase, but gross profit up, I think, $400 million-$450 million, which is kind of pretty well below corporate average fall-through. I appreciate that SEMIs is dilutive and custom is probably dilutive within SEMIs. Anything else going on with margins that we should be aware of? How should we think about the margin profile of custom longer term as that business continues to scale and diversify? Thank you.\n\n**Kirsten Spears** (CFO)\nYeah. We've historically said that the XPU margins are slightly lower than the rest of the business, other than wireless. There is really nothing else going on other than that. It is just exactly what I said, that the majority of it, quarter over quarter, the 130 basis point decline is being driven by more XPUs.\n\n**Hock Tan** (CEO)\nThere are more moving parts here than your simple analysis proves here. I think your simple analysis is totally wrong in that regard.\n\n**Joshua Buchalter** (Director of Equity Research Semiconductors)\nAll right then. Thank you.\n\n**Operator**\nOne moment for our next question. That will come from the line of Timothy Arcuri with UBS. Your line is open.\n\n**Timothy Arcuri** (Managing Director)\nThanks a lot, Hock. I also wanted to ask about scale-up, Hock. There's a lot of competing ecosystems. There's UA Link, which of course you left. Now there's the big GPU company opening up NVLink. They're both trying to build ecosystems. There's an argument that you're an ecosystem of one. What would you say to that debate? Does opening up NVLink change the landscape? How do you view your AI networking growth next year? Do you think it's going to be primarily driven by scale-up or will it still be pretty scale-out heavy things?\n\n**Hock Tan** (CEO)\nPeople do like to create platforms and new protocols and systems. The fact of the matter is scale-up can just be done easily. It is currently available. It is open standards, open source, Ethernet. Just as well. You do not need to create new systems for the sake of doing something that you could easily be doing in networking in Ethernet. Yeah, I hear a lot of these interesting new protocols, standards that are trying to be created. Most of them, by the way, are proprietary, much as they like to call it otherwise. What is really open source and open standards is Ethernet. We believe Ethernet will prevail as it does for the last 20 years in traditional networking. There is no reason to create a new standard for something that could be easily done in transferring bits and bytes of data.\n\n**Timothy Arcuri** (Managing Director)\nGot it, Hock. Thank you.\n\n**Operator**\nOne moment for our next question. That will come from the line of Christopher Rolland with Susquehanna. Your line is open.\n\n**Christopher Rolland** (Senior Equity Analyst Semiconductors)\nThanks for the question. Yeah. My question is for you, Hock. It's kind of a bigger picture one here. And this kind of acceleration that we're seeing in AI demand, do you think that this acceleration is because of a marked improvement in ASICs or XPUs closing the gap on the software side at your customers? Do you think it's these tokenomics around inference, test-time compute driving that? For example, what do you think is actually driving the upside here? And do you think it leads to a market share shift faster than we were expecting towards XPU from GPU? Thanks.\n\n**Hock Tan** (CEO)\nYeah. Interesting question. No, none of the foregoing that you outlined. It's very simple. The way inference has come out very, very hot lately is, remember, we're only selling to a few customers, hyperscalers with platforms and LLMs. That's it. There are not that many. We told you how many we have, and we haven't increased any. What is happening is these hyperscalers and those with LLMs need to justify all this spending they're doing. Doing training makes your frontier models smarter. That's no question. It's almost like research and science. Make your frontier models by creating very clever algorithms that consume a lot of compute for training smarter. Training makes it smarter. You want to monetize inference. That's what's driving it. Monetize, I indicated in my prepared remarks, to drive to justify a return on investment. A lot of that investment is training.\n\n**Hock Tan** (CEO)\nThat return on investment is by creating use cases, a lot of AI use cases, AI consumption out there through availability of a lot of inference. That is what we are now starting to see among a small group of customers.\n\n**Christopher Rolland** (Senior Equity Analyst Semiconductors)\nExcellent. Thank you.\n\n**Operator**\nOne moment for our next question. That will come from the line of Vijay Rakesh with Mizuho. Your line is open.\n\n**Vijay Rakesh** (Managing Director)\nYeah. Thanks. Hey, Hock. Just going back on the AI server revenue side, I know you said fiscal 2025 kind of tracking to that up 60%-ish growth. If you look at fiscal 2026, you have many new customers racking with a Meta and probably you have the four or the six hyperscalers that you're talking about. Would you expect that growth to accelerate into fiscal 2026 above that kind of 60% you talked about?\n\n**Hock Tan** (CEO)\nMy prepared remarks, which I clarified, that the rate of growth we are seeing in 2025 will sustain into 2026 based on improved visibility and the fact that we're seeing inference coming in on top of the demand for training as the clusters get bigger and bigger and bigger still stands. I don't think we are getting very far by trying to pass through my words or data here. It's just a trick.\n\n**Vijay Rakesh** (Managing Director)\nGot it.\n\n**Hock Tan** (CEO)\nWe see that going from 2025 into 2026 as the best forecast we have at this point.\n\n**Vijay Rakesh** (Managing Director)\nGot it. On the NVLink, the NVLink Fusion versus the scale-up, do you expect that market to go the route of top of the rack where you're seeing some move to the Ethernet side in kind of the scale-out? Do you expect scale-up to kind of go the same route? Thanks.\n\n**Hock Tan** (CEO)\nBroadcom do not participate in NVLink. So I'm really not qualified to answer that question, I think.\n\n**Vijay Rakesh** (Managing Director)\nGot it. Thank you.\n\n**Operator**\nThank you. One moment for our next question. That will come from the line of Aaron Rakers with Wells Fargo. Your line is open.\n\n**Aaron Rakers** (Managing Director and Technology Analyst)\nYeah. Thanks for taking the question. I think all my questions on scale-up have been asked. I guess, Hock, given the execution that you guys have been able to do with the VMware integration, looking at the balance sheet, looking at the debt structure, I'm curious if you could give us your thoughts on how the company thinks about capital return versus the thoughts on M&A and the strategy going forward. Thank you.\n\n**Hock Tan** (CEO)\nOkay. That's an interesting question. I agree. Not too untimely, I would say. Because, yeah, we have done a lot of the integration of VMware now. You can see that in the level of free cash flow we're generating from operations. As we said, the use of capital has always been very, I guess, measured and upfront with a return through dividends, which is half our free cash flow of the preceding year. Frankly, as Kirsten has mentioned three months ago and six months ago during the last two earnings call, the first choice typically of the other part of the free cash flow is to bring down our debt to a level that we feel closer to no more than two ratio of debt to EBITDA.\n\n**Hock Tan** (CEO)\nThat does not mean that opportunistically we may go out there and buy back our shares as we did last quarter. As indicated by Kirsten when we did $4.2 billion of stock buyback. Now, part of it is used to basically when employee RSUs vest, we basically buy back part of the shares used to be paying taxes on the vested RSU. The other part of it, I do admit we used opportunistically last quarter when we see an opportunity situation when basically we think that it is a good time to buy some shares back. We do. Having said all that, our use of cash outside of dividends would be at this stage used towards reducing our debt. I know you are going to ask, what about M&A?\n\n**Hock Tan** (CEO)\nThe kind of M&A we will do in our view would be significant, would be substantial enough that we need debt in any case. It is a good use of our free cash flow to bring down debt to, in a way, expand, if not preserve, our borrowing capacity if we have to do another M&A deal.\n\n**Operator**\nThank you. One moment for our next question. That will come from the line of Srini Pajjuri with Raymond James. Your line is open.\n\n**Srini Pajjuri** (Managing Director)\nThank you. Hock, a couple of clarifications. First, on your 2026 expectation, are you assuming any meaningful contribution from the four prospects that you talked about?\n\n**Hock Tan** (CEO)\nNo comment. We don't talk on prospects. We only talk on customers.\n\n**Srini Pajjuri** (Managing Director)\nOkay. Fair enough. My other clarification is that I think you talked about networking being about 40% of the mix within AI. Is that the right kind of mix that you expect going forward, or is that going to materially change as we, I guess, see XPUs ramping going forward?\n\n**Hock Tan** (CEO)\nNo. I've always said and expect that to be the case going forward in 2026 as we grow, that networking should be a ratio to XPU should be closer in the range of less than 30%, not the 40%.\n\n**Operator**\nThank you. One moment for our next question. That will come from the line of Joe Moore with Morgan Stanley. Your line is open.\n\n**Joe Moore** (Managing Director)\nGreat. Thank you. You've said you're not going to be impacted by export controls on AI. I know there's been a number of changes in the industry since the last time you made the call. Is that still the case? Can you give people comfort that there's no impact from that down the road?\n\n**Hock Tan** (CEO)\nNobody can give anybody comfort in this environment, Joe. You know that. Rules are changing quite dramatically as bilateral trade agreements continue to be negotiated in a very, very dynamic environment. I'll be honest, I don't know. I know as little as probably you probably know more than I do, maybe, in which case then I know very little about this whole thing about whether there's any export control, how the export control will take place. We're guessing. I'd rather not answer that because, no, I don't know whether it will be.\n\n**Operator**\nThank you. We do have time for one final question. That will come from the line of William Stein with Truist Securities. Your line is open.\n\n**William Stein** (Managing Director and Senior Analyst Technology)\nYeah. Great. Thank you for squeezing me in. I wanted to ask about VMware. Can you comment as to how far along you are in the process of converting customers to the subscription model? Is that close to complete, or is there still a number of quarters that we should expect that that conversion continues?\n\n**Hock Tan** (CEO)\nThat's a good question. Let me start off by saying a good way to measure it is most of our VMware contracts are typically three years. That was what VMware did before we acquired them. That's pretty much what we continue to do, three years, very traditional. Based on that, the renewals were like two-thirds of the way, almost halfway, more than halfway through the renewals. We probably have at least another year plus maybe a year and a half to go.\n\n**Operator**\nThank you. And with that, I'd like to turn the call over to Goo for closing remarks.\n\n**Operator**\nThank you, operator. Broadcom currently plans to report its earnings for the third quarter of fiscal year 2025 after close of market on Thursday, September 4, 2025. A public webcast of Broadcom's earnings conference call will follow at 2:00 P.M. Pacific. That will conclude our earnings call today. Thank you all for joining. Operator, you may end the call.",
      "fetched_at": "2026-02-04T15:40:04.717Z"
    },
    {
      "ticker": "AVGO",
      "title": "Broadcom Inc. (AVGO) Q1 FY2025 earnings call transcript",
      "published_date": "Mar 6, 2025, 5:00 PM EST",
      "fiscal_year": "2025",
      "quarter": "Q1",
      "url": "https://finance.yahoo.com/quote/AVGO/earnings/AVGO-Q1-2025-earnings_call-295237.html",
      "content": "**Operator**\nWelcome to the Broadcom Inc.'s first quarter, fiscal year 2025 financial results conference call. At this time, for opening remarks and introductions, I would like to turn the call over to Ji Yoo, Head of Investor Relations of Broadcom Inc.\n\n**Ji Yoo** (Head of Investor Relations)\nThank you, Cherie, and good afternoon, everyone. Joining me on today's call are Hock Tan, President and CEO. Kirsten Spears, Chief Financial Officer. And Charlie Kawwas, President, Semiconductor Solutions Group. Broadcom distributed a press release and financial tables after the market closed, describing our financial performance for the first quarter of fiscal year 2025. If you did not receive a copy, you may obtain the information from the investor section of Broadcom's website at broadcom.com. This conference call is being webcast live, and an audio replay of the call can be accessed for one year through the investor section of Broadcom's website. During the prepared comments, Hock and Kirsten will be providing details of our first quarter fiscal year 2025 results, guidance for our second quarter of fiscal year 2025, as well as commentary regarding the business environment. We'll take questions after the end of our prepared comments.\n\n**Ji Yoo** (Head of Investor Relations)\nPlease refer to our press release today and our recent filings with the SEC for information on the specific risk factors that could cause our actual results to differ materially from the forward-looking statements made on this call. In addition to U.S. GAAP reporting, Broadcom reports certain financial measures on a non-GAAP basis. A reconciliation between GAAP and non-GAAP measures is included in the tables attached to today's press release. Comments made during today's call will primarily refer to our non-GAAP financial results. I'll now turn the call over to Hock.\n\n**Hock Tan** (President and CEO)\nThank you, Ji, and thank you, everyone, for joining today. In our fiscal Q1 2025, total revenue was a record $14.9 billion, up 25% year-on-year, and consolidated adjusted EBITDA was a record, again, $10.1 billion, up 41% year-on-year, so let me first provide color on our semiconductor business. Q1 semiconductor revenue was $8.2 billion, up 11% year-on-year. Growth was driven by AI, as AI revenue of $4.1 billion was up 77% year-on-year. We beat our guidance for AI revenue of $3.8 billion due to stronger shipments of networking solutions to hyperscalers on AI. Our hyperscale partners continue to invest aggressively in their next-generation frontier models, which do require high-performance accelerators, as well as AI data centers with larger clusters, and consistent with this, we are stepping up our R&D investment on two fronts. One, we're pushing the envelope of technology in creating the next generation of accelerators.\n\n**Hock Tan** (President and CEO)\nWe're tipping out the industry's first 2-nanometer AI XPU, packaged in 3.5D, as we drive towards a 10,000-teraflops XPU. Secondly, we have a view towards scaling clusters of 500,000 accelerators for our hyperscale customers. We have doubled the Radix capacity of the existing Tomahawk 5. And beyond this, to enable AI clusters to scale up on Ethernet towards 1 million XPUs, we have taped out our next-generation 100-terabit Tomahawk 6 switch running 200G studies at 1.6-terabit bandwidth. We will be delivering samples to customers within the next few months. These R&D investments are very aligned with the roadmap of our three hyperscale customers, as they each race towards 1 million XPU clusters by the end of 2027.\n\n**Hock Tan** (President and CEO)\nAccordingly, we do reaffirm what we said last quarter, that we expect these three hyperscale customers will generate a serviceable, addressable market, or SAM, in the range of $60-$90 billion in fiscal 2027. Beyond these three customers, we had also mentioned previously that we are deeply engaged with two other hyperscalers in enabling them to create their own customized AI accelerator. We are on track to tape out their XPUs this year. In the process of working with the hyperscalers, it has become very clear that while they are excellent in software, Broadcom is the best in hardware. Working together is what optimizes their large language models. It is therefore no surprise to us, since our last earnings call, that two additional hyperscalers have selected Broadcom to develop custom accelerators to train their next-generation frontier models.\n\n**Hock Tan** (President and CEO)\nSo even as we have three hyperscale customers who are shipping XPUs in volume today, there are now four more who are deeply engaged with us to create their own accelerators. And to be clear, of course, these four are not included in our estimated SAM of $60-$90 billion in 2027. So we do see an exciting trend here. New frontier models and techniques put unexpected pressures on AI systems. It's difficult to serve all classes of models with a single system design point. And therefore, it is hard to imagine that a general-purpose accelerator can be configured and optimized across multiple frontier models. And as I mentioned before, the trend towards XPUs is a multi-year journey. So coming back to 2025, we see a steady ramp in deployment of our XPUs and networking products.\n\n**Hock Tan** (President and CEO)\nIn Q1, AI revenue was $4.1 billion, and we expect Q2 AI revenue to grow to $4.4 billion, which is up 44% year-on-year. Turning to non-AI semiconductors, revenue of $4.1 billion was down 9% sequentially on a seasonal decline in wireless. In aggregate, during Q1, the recovery in non-AI semiconductors continued to be slow. Broadband, which bottomed in Q4 2024, showed a double-digit sequential recovery in Q1 and is expected to be up similarly in Q2, as service providers and telcos step up spending. Server storage was down single digits sequentially in Q1, but is expected to be up high single digits sequentially in Q2. Meanwhile, enterprise networking continues to remain flattish in the first half of fiscal 2025, as customers continue to work through channel inventory. While wireless was down sequentially due to a seasonal decline, it remained flat year-on-year.\n\n**Hock Tan** (President and CEO)\nIn Q2, wireless is expected to be the same, flat again year-on-year. Resales in industrial were down double digits in Q1 and are expected to be down in Q2, so reflecting the foregoing puts and takes, we expect non-AI semiconductor revenue in Q2 to be flattish sequentially, even though we are seeing bookings continue to grow year-on-year. In summary, for Q2, we expect total semiconductor revenue to grow 2% sequentially and up 17% year-on-year to $8.4 billion. Turning now to infrastructure software segment. Q1 infrastructure software revenue of $6.7 billion was up 47% year-on-year and up 15% sequentially, exaggerated, though, by deals which slipped from Q4 into Q1. Now, this is the first quarter, Q1 2025, where the year-on-year comparables include VMware in both quarters. We're seeing significant growth in the software segment for two reasons.\n\n**Hock Tan** (President and CEO)\nOne, we're converting from a footprint of largely perpetual license to one of full subscription. And as of today, we are over 60% done. Two, these perpetual licenses were only largely for compute virtualization, otherwise called vSphere. We are upselling customers to a full-stack vCF, which enables the entire data center to be virtualized. And this enables customers to create their own private cloud environment on-prem. And as of the end of Q1, approximately 70% of our largest 10,000 customers have adopted vCF. As these customers consume vCF, we do see a further opportunity for future growth. As large enterprises adopt AI, they have to run their AI workloads on their on-prem data centers, which will include both GPU servers as well as traditional CPUs.\n\n**Hock Tan** (President and CEO)\nJust as vCF virtualizes these traditional data centers using CPUs, vCF will also virtualize GPUs on a common platform and enable enterprises to import AI models to run their own data on-prem. This platform, which virtualizes the GPU, is called the VMware Private AI Foundation. And as of today, in collaboration with NVIDIA, we have 39 enterprise customers for the VMware Private AI Foundation. Customer demand has been driven by our open ecosystem, superior load balancing and automation capabilities that allows them to intelligently pull and run workloads across both GPU and CPU infrastructure, leading to very reduced costs. Moving on to Q2 outlook for software, we expect revenue of $6.5 billion, up 23% year-on-year. So in total, we're guiding Q2 consolidated revenue to be approximately $14.9 billion, up 19% year-on-year. And we expect this will drive Q2 adjusted EBITDA to approximately 66% of revenue.\n\n**Hock Tan** (President and CEO)\nWith that, let me turn the call over to Kirsten.\n\n**Kirsten Spears** (CFO)\nThank you, Hock. Let me now provide additional detail on our Q1 financial performance. From a year-on-year comparable basis, keep in mind that Q1 of fiscal 2024 was a 14-week quarter, and Q1 of fiscal 2025 is a 13-week quarter. Consolidated revenue was $14.9 billion for the quarter, up 25% from a year ago. Gross margin was 79.1% of revenue in the quarter, better than we originally guided on higher infrastructure software revenue and more favorable semiconductor revenue mix. Consolidated operating expenses were $2 billion, of which $1.4 billion was for R&D. Q1 operating income of $9.8 billion was up 44% from a year ago, with operating margin at 66% of revenue. Adjusted EBITDA was a record $10.1 billion, or 68% of revenue, above our guidance of 66%. This figure excludes $142 million of depreciation.\n\n**Kirsten Spears** (CFO)\nNow a review of the P&L for our two segments, starting with semiconductors. Revenue for our semiconductor solutions segment was $8.2 billion and represented 55% of total revenue in the quarter. This was up 11% year-on-year. Gross margin for our semiconductor solutions segment was approximately 68%, up 70 basis points year-on-year, driven by revenue mix. Operating expenses increased 3% year-on-year to $890 million on increased investment in R&D for leading-edge AI semiconductors, resulting in semiconductor operating margin of 57%. Now moving on to infrastructure software. Revenue for infrastructure software of $6.7 billion was 45% of total revenue and up 47% year-on-year, based primarily on increased revenue from VMware. Gross margin for infrastructure software was 92.5% in the quarter, compared to 88% a year ago.\n\n**Kirsten Spears** (CFO)\nOperating expenses were approximately $1.1 billion in the quarter, resulting in infrastructure software operating margin of 76%.\n\n**Kirsten Spears** (CFO)\nThis compares to operating margin of 59% a year ago. This year-on-year improvement reflects our disciplined integration of VMware and sharp focus on deploying our vCF strategy. Moving on to cash flow. Free cash flow in the quarter was $6 billion and represented 40% of revenue. Free cash flow, as a percentage of revenue, continues to be impacted by cash interest expense from debt related to the VMware acquisition and cash taxes due to the mix of U.S. taxable income, the continued delay in the reenactment of Section 174, and the impact of corporate AMT. We spent $100 million on capital expenditures. Day sales outstanding were 30 days in the first quarter, compared to 41 days a year ago. We ended the first quarter with inventory of $1.9 billion, up 8% sequentially to support revenue in future quarters.\n\n**Kirsten Spears** (CFO)\nOur days of inventory on hand were 65 days in Q1, as we continue to remain disciplined on how we manage inventory across the ecosystem. We ended the first quarter with $9.3 billion of cash and $68.8 billion of gross principal debt. During the quarter, we repaid $495 million of fixed-rate debt and $7.6 billion of floating-rate debt with new senior notes, commercial paper, and cash on hand, reducing debt by a net $1.1 billion. Following these actions, the weighted average coupon rate and years to maturity of our $58.8 billion in fixed-rate debt is 3.8% and 7.3 years, respectively. The weighted average coupon rate and years to maturity of our $6 billion in floating-rate debt is 5.4% and 3.8 years, respectively, and our $4 billion in commercial paper is at an average rate of 4.6%. Turning to capital allocation.\n\n**Kirsten Spears** (CFO)\nIn Q1, we paid stockholders $2.8 billion of cash dividends based on a quarterly common stock cash dividend of $0.59 per share. We spent $2 billion to repurchase 8.7 million AVGO shares from employees as those shares vested for withholding taxes. In Q2, we expect the non-GAAP diluted share count to be approximately 4.95 billion shares. Now moving on to guidance. Our guidance for Q2 is for consolidated revenue of $14.9 billion, with semiconductor revenue of approximately $8.4 billion, up 17% year-on-year. We expect Q2 AI revenue of $4.4 billion, up 44% year-on-year. For non-AI semiconductors, we expect Q2 revenue of $4 billion. We expect Q2 infrastructure software revenue of approximately $6.5 billion, up 23% year-on-year. We expect Q2 adjusted EBITDA to be about 66%.\n\n**Kirsten Spears** (CFO)\nFor modeling purposes, we expect Q2 consolidated gross margin to be down approximately 20 basis points sequentially on the revenue mix of infrastructure software and product mix within semiconductors. As Hock discussed earlier, we are increasing our R&D investment in leading-edge AI in Q2, and accordingly, we expect adjusted EBITDA to be approximately 66%. We expect the non-GAAP tax rate for Q2 and fiscal year 2025 to be approximately 14%. That concludes my prepared remarks. Operator, please open up the call for questions.\n\n**Operator**\nThank you. To ask a question, you will need to press star 11 on your telephone. To withdraw your question, press star 11 again. Due to time restraints, we ask that you please limit yourself to one question. Please stand by while we compile the Q&A roster. And our first question will come from the line of Ben Rietzes with Melius. Your line is open.\n\n**Ben Reitzes** (Managing Director)\nHey, guys.\n\n**Ben Reitzes** (Managing Director)\nThanks a lot, and congrats on the results. Hock, you talked about four more customers coming online. Can you just talk a little bit more about the trend you're seeing? Can any of these customers be as big as the current three? And what does this say about the custom silicon trend overall and your optimism and upside to the business long-term? Thanks.\n\n**Hock Tan** (President and CEO)\nWell, very interesting question, Ben, and thanks for your kind wishes. But what we're seeing is, and by the way, these four are not yet customers as we define it. As I've always said, in developing and creating XPUs, we are not really the creator of those XPUs, to be honest. We enable those hyperscalers' partners we engage with to create that chip, basically to create that compute system, call it that way.\n\n**Hock Tan** (President and CEO)\nAnd it comprises the software model working closely with the compute engine, the XPU, and the networking that ties together the clusters, those multiple XPUs as a whole to train those large frontier models. And so the fact that we create the hardware, it still has to work with the software models and algorithms of those partners of ours before it becomes fully deployable at scale, which is why we define customers in this case as those where we know they're deployed at scale and we receive the production volume to enable it to run. And for that, we only have three, just to reiterate. The four are, I call it, partners who are trying to create the same thing as the first three and to run their own frontier models. Each of them don't have to train their own frontier models.\n\n**Hock Tan** (President and CEO)\nAnd as I also said, it doesn't happen overnight. To do the first chip would take typically a year and a half, and that's very accelerated, which we could accelerate given that we essentially have a framework and a methodology that works right now. It works for the three customers. No reason for it to not work for the four. But we still need those four partners to create and to develop the software, which we don't do, to make it work. And to answer your question, there's no reason why these four guys would not create a demand in the range of what we're seeing with the first three guys, but probably later. It's a journey. They started it later, and so they will probably get there later.\n\n**Ben Reitzes** (Managing Director)\nThank you very much.\n\n**Operator**\nThank you.\n\n**Operator**\nOne moment for our next question, and that will come from the line of Harlen Sur with J.P. Morgan. Your line is open.\n\n**Harlan Sur** (Executive Director)\nGood afternoon and great job on the strong quarterly execution, Hock and team. Great to see the continual momentum in the AI business here in the first half of your fiscal year and the continued broadening out of your AI ASIC customers. I know, Hock, last earnings you did call out a strong ramp in the second half of the fiscal year driven by new three-nanometer AI accelerated programs kind of ramping. Can you just help us either qualitatively, quantitatively profile this second-half step up relative to what the team just delivered here in the first half? Has the profile changed either favorably, less favorably versus what you thought maybe 90 days ago? Because quite frankly, I mean, a lot has happened since last earnings, right?\n\n**Harlan Sur** (Executive Director)\nYou've had the dynamics like DeepSeek and focus on AI model efficiency, but on the flip side, you've had strong CapEx outlooks by your cloud and hyperscale customers, so any color on the second-half AI profile would be helpful, asking me to look into the minds of my customers, and I hate to tell you, they don't show me the entire mindset here, but why we're beating the numbers so far in Q1 and seems to be encouraging in Q2. Partly from improved networking shipments, as I indicated, to cluster those XPUs and AI accelerators, even in some cases GPUs together for the hyperscalers, and that's good, and partly also we think there is some pull-ins of shipments and acceleration, call it that way, of shipments in fiscal 2025.\n\n**Hock Tan** (President and CEO)\nOn the second half that you talked about 90 days ago, the second half 3-nanometer ramp, is that still very much on track? Harlen, thank you. I only got Q2. Sorry. Let's not speculate on the second half.\n\n**Harlan Sur** (Executive Director)\nOkay. Thank you, Hock.\n\n**Hock Tan** (President and CEO)\nThank you.\n\n**Operator**\nThank you. One moment for our next question, and that will come from the line of William Stein with Truist Securities. Your line is open.\n\n**William Stein** (Managing Director)\nGreat. Thank you for taking my question. Congrats on these pretty great results. It seems from the news headlines about tariffs and about DeepSeek that there may be some disruptions. Some customers and some other complementary suppliers seem to feel a bit paralyzed, perhaps, and have difficulty making tough decisions. Those tend to be really useful times for great companies to sort of emerge as something bigger and better than they were in the past.\n\n**William Stein** (Managing Director)\nYou've grown this company in a tremendous way over the last decade-plus, and you're doing great now, especially in this AI area. But I wonder if you're seeing that sort of disruption from these dynamics that we suspect are happening based on headlines, what we see from other companies, and aside from adding these customers in AI, I'm sure there's other great stuff going on, but should we expect some bigger changes to come from Broadcom as a result of this?\n\n**Hock Tan** (President and CEO)\nYou pose a very interesting set of issues and questions, and those are very relevant, interesting issues. The only problem we have at this point is, I would say it's really too early to know where we all land. I mean, there's the threat, the noise of tariffs, especially on chips, that hasn't materialized yet. Nor do we know how it will be structured, so we don't know.\n\n**Hock Tan** (President and CEO)\nBut we do experience, and we are living it now, is the disruption that is in a positive way, I should add. A very positive disruption in semiconductors on generative AI. Generative AI, for sure. And I said that before also, at the risk of repeating here, but we feel it more than ever. It's really accelerating the development of semiconductor technology, both process and packaging, as well as design, towards higher and higher performance accelerators and networking functionality. We're seeing that innovation, those upgrades occur every month as we face new interesting challenges. And particularly with XPUs, we've been asked to optimize to frontier models of our partners, our customers, as well as our hyperscale partners. And it's a lot of, I mean, it's a privilege almost for us to participate in it and try to optimize.\n\n**Hock Tan** (President and CEO)\nTo optimize, I mean you look at an accelerator. You can look at it for simple terms, high level, to perform. You want to make the dimension not just on one single metric, which is compute capacity, how many teraflops. It's more than that. It's also tied to the fact that this is a distributed computing problem. It's not just the compute capacity of a single XPU or GPU. It's also the network bandwidth. It ties itself to the next adjacent XPU or GPU, so that has an impact, so you're doing that. You have to balance with that, then you decide, are you doing training or you're doing pre-training, post-training, fine-tuning, and again, then comes how much memory do you balance against that, and with it, how much latency you can afford, which is memory bandwidth.\n\n**Hock Tan** (President and CEO)\nYou look at at least four variables, maybe even five if you include in memory bandwidth, not just memory capacity when you go straight to inference. So we have all these variables to play with, and we try to optimize it. So all this is very, very, I mean, it's a great experience for our engineers to push their envelope on how to create all those chips. And so that's the biggest disruption we see right now from sheer trying to create and push the envelope on generative AI, trying to create the best hardware infrastructure to run it. Beyond that, yeah, there are other things too that come into play because with AI, as I indicated, it does not just drive hardware for enterprises. It drives the way they architect their data centers. Keeping data private under control becomes important.\n\n**Hock Tan** (President and CEO)\nSo suddenly, the push of workloads towards public cloud may take a little pause as large enterprises, particularly, have to take to recognize that you want to run AI workloads. You probably think very hard about running them on-prem. And suddenly, you push yourself towards saying, \"You got to upgrade your own data centers to do and manage your own data to run it on-prem.\" And that's also pushing a trend that we have been seeing now over the past 12 months. Hence my comments on VMware Private AI Foundation. It's true, especially enterprises pushing direction are quickly recognizing where do they run their AI workloads. So those are trends we see today, and a lot of it coming out of AI, a lot of it coming out of sensitive rules on sovereignty in cloud and in data.\n\n**Hock Tan** (President and CEO)\nAs far as your mentioning tariffs is concerned, I think that's too early for us to figure out where it'll all land. And probably maybe give it another three, six months, we probably have a better idea of where it's going.\n\n**William Stein** (Managing Director)\nThank you.\n\n**Operator**\nThank you. One moment for our next question. And that will come from the line of Ross Seymour with Deutsche Bank. Your line is open.\n\n**Ross Seymore** (Managing Director)\nNice to meet you. Thanks for asking the question. Hock, I want to go back to the XPU side of things. And going from the four new engagements, not yet named customers, two last quarter and two more today that you announced, I want to talk about going from kind of design win to deployment. How do you judge that?\n\n**Ross Seymore** (Managing Director)\nBecause there is some debate about tons of design wins, but the deployments actually don't happen either, that they never occur, or that the volume is never what is originally promised. How do you view that kind of conversion ratio? Is there a wide range around it, or is there some way you could help us kind of understand how that works?\n\n**Hock Tan** (President and CEO)\nRoss, that's an interesting question. I'll take the opportunity to say the way we look at design win is probably very different from the way many of our peers look at it out there. Number one, to begin with, we believe design win when we know our product is produced at scale and is actually deployed, literally deployed in production. So that takes a long time.\n\n**Hock Tan** (President and CEO)\nBecause from taping out, getting in the product, it takes a year easily from the product in the hands of our partner to when it goes into scale production. It will take six months to a year, is our experience that we've seen. Number one. And number two, I mean, producing and deploying 5,000 XPUs, that's a joke. That's not real production in our view. And so we also limit ourselves in selecting partners to people who really need that large volume. You need that large volume from our viewpoint in scale right now in mostly training, training of large language models, frontier models in a continuing trajectory. So we limit ourselves to how many customers or how many potential customers that exist out there, Ross. And we tend to be very selective who we pick to begin with. So when we say design win, it really is at scale.\n\n**Hock Tan** (President and CEO)\nIt's not something that starts in six months and die or a year and die again. Basically, it's a selection of customers. It's just the way we run our ASIC business in general for the last 15 years. We pick and choose the customers because we know this guy and we do multi-year roadmaps with these customers because we know these customers are sustainable. I put it bluntly. We don't do it for startups.\n\n**Ross Seymore** (Managing Director)\nThank you.\n\n**Operator**\nOne moment for our next question. That will come from the line of Stacy Rasgon with Bernstein Research. Your line is open.\n\n**Stacy Rasgon** (Senior Analyst)\nHi guys. Thanks for taking my question. I wanted to go to the three customers that you do have in volume today.\n\n**Stacy Rasgon** (Senior Analyst)\nWhat I wanted to ask was, is there any concern about some of the new regulations or the AI diffusion rules that are going to get put in place supposedly in May impacting any of those design wins or shipments? It sounds like you think all three of those are still on at this point. But anything you could tell us about worries about new regulations or AI diffusion rules impacting any of those wins would be helpful.\n\n**Hock Tan** (President and CEO)\nThank you. In this era or this current era of geopolitical tensions and fairly dramatic actions all around by governments, there's always some concern at the back of everybody's mind. But to answer your question directly, no. We don't have any concerns.\n\n**Stacy Rasgon** (Senior Analyst)\nGot it. So none of those are going into China or to Chinese customers then?\n\n**Hock Tan** (President and CEO)\nNo comment. Are you trying to locate who they are?\n\n**Stacy Rasgon** (Senior Analyst)\nOkay. That's helpful.\n\n**Stacy Rasgon** (Senior Analyst)\nThank you.\n\n**Hock Tan** (President and CEO)\nThank you.\n\n**Operator**\nOne moment for our next question. And that will come from the line of Vivek Arya with Bank of America. Your line is open.\n\n**Vivek Arya** (Managing Director)\nThanks for taking my question. Hock, whenever you have described your AI opportunity, you have always emphasized the training workload. But the perception is that the AI market could be dominated by the inference workload, especially with these new reasoning models. So what happens to your opportunity and share if the mix moves more toward inference? Does it create a bigger SAM for you than the $60-$90 billion? Does it keep it the same, but there's a different mix of product? Or does a more inference-heavy market favor a GPU over an XPU?\n\n**Hock Tan** (President and CEO)\nThank you. That's a good question. Interesting question. By the way, I do talk a lot about training.\n\n**Hock Tan** (President and CEO)\nOur XPUs also focus on inference as a separate product line. They do. And that's why I can say the architecture of those chips are very different from the architecture of the training chips. And so it's a combination of those two, I should add, that adds up to this $60-$90 billion. So if I have not been clear, I do apologize. It's a combination of both. But having said that, the larger part of the dollars come from training, not inference within the service of the SAM that we've talked about so far.\n\n**Vivek Arya** (Managing Director)\nThank you.\n\n**Operator**\nOne moment for our next question. That will come from the line of Harsh Kumar with Piper Sandler. Your line is open.\n\n**Harsh Kumar** (Managing Director)\nThanks, Broadcom team. Again, great execution. Just Hock, I had a quick question.\n\n**Harsh Kumar** (Managing Director)\nWe've been hearing that almost all of the large clusters that are 100K plus, they're all going to Ethernet. I was wondering if you could help us understand the importance of when the customer is making a selection, choosing between a guy that has the best switch ASIC such as you versus a guy that might have the compute there. Can you talk about what the customer is thinking and what are the final points that they want to hit upon when they make that selection for the NIC cards?\n\n**Hock Tan** (President and CEO)\nOkay. I see. No, yeah, it's down to in the case of the hyperscalers, not very much so, it's very driven by performance. And it's performance, what you're mentioning about on connecting, scaling up, and scaling out those AI accelerators, be they XPU or GPU among hyperscalers.\n\n**Hock Tan** (President and CEO)\nIn most cases among those hyperscalers we engage with when it comes to connecting those clusters, they're very driven by performance. I mean, if you are in a race to really get the best performance out of your hardware as you train and continue to train your frontier models, that matters more than anything else. The basic first thing they go for is proven. That's a proven piece of hardware. It's a proven system, subsystem in our case that makes it work. In that case, we tend to have a big advantage because, I mean, networking ASICs, switching and routing ASICs for the last 10 years at least. The fact that it's AI just makes it more interesting for our engineers to work on.\n\n**Hock Tan** (President and CEO)\nBut it's basically based on proven technology and experience in pushing the envelope on going from 800 gigabit per second bandwidth to 1.6 and moving on 3.2, which is exactly why we keep stepping up the rate of investment in coming up with a product where we take Tomahawk 5, we double the radix to deal with just one hyperscaler because they want high radix to create larger clusters while running bandwidth that are smaller. But that doesn't stop us from moving ahead to the next generation of Tomahawk 6. And I dare say we're even planning Tomahawk 7 and 8 right now. And we're speeding up the rate of development. And it's all largely for that few guys, by the way. So we're making a lot of investment for very few customers, hopefully with very large serviceable addressable markets. If anything else, that's the big bets we are placing.\n\n**Harsh Kumar** (Managing Director)\nThank you, Hock.\n\n**Operator**\nThank you. One moment for our next question, and that will come from the line of Timothy Arcuri with UBS. Your line is open.\n\n**Timothy Arcuri** (Managing Director)\nThanks a lot. Hock, in the past, you have mentioned XPU units growing from about 2 million last year to about 7 million, you said, in the 2027, 2028 timeframe. My question is, do these four new customers, do they add to that 7 million unit number? I know in the past you've sort of talked about an ASP of $20,000 by then. So the first three customers are clearly a subset of that 7 million unit. So do these new four engagements drive that 7 higher, or do they just fill in to get to that 7 million? Thanks,\n\n**Hock Tan** (President and CEO)\nand thanks, Tim, for asking that.\n\n**Hock Tan** (President and CEO)\nTo clarify, as I thought I made it clear in my comments, no, the market we're talking about, when you translate the unit, is only among the three customers we have today. The other four, we talk about engagement partners, we don't consider that as customers yet, and therefore are not in our serviceable addressable market.\n\n**Timothy Arcuri** (Managing Director)\nOkay. So they would add to that number. Okay. Thanks, Hock.\n\n**Hock Tan** (President and CEO)\nThanks.\n\n**Operator**\nOne moment for our next question. And that will come from the line of C.J. Muse with Cantor Fitzgerald.\n\n**Operator**\nYour line is open.\n\n**CJ Muse** (Senior Managing Director)\nYeah. Good afternoon. Thank you for taking the question.\n\n**CJ Muse** (Senior Managing Director)\nI guess, Hock, to follow up on your prepared remarks and comments earlier around optimization with your best hardware and hyperscalers with their great software, I'm curious how you're expanding your portfolio now to six megascale kind of frontier models will enable you to, in one fell swoop, share tremendous information, but at the same time, a world where these six truly want to differentiate, so obviously, the goal for all of these players is ExaFLOPS per second per dollar of CapEx per watt, and I guess, to what degree are you aiding them in this effort? And where does maybe the Chinese wall kind of start where they want to kind of differentiate and not share with you kind of some of the work that you're doing? Thank you.\n\n**Hock Tan** (President and CEO)\nWe only provide very basic fundamental technology in semiconductors to enable these guys to use what we have and optimize it to their own particular models and the algorithms that relate to those models. That's it. That's all we do, so that's the level of a lot of that optimization we do for each of them, and as I mentioned earlier, there are maybe five degrees of freedom that we do, and we play with that, and so even if there are five degrees of freedom, there's only so much we can do at that point, but it is, and how they basically, how we optimize it, is all tied to the partner telling us how they want to do it, so there's only so much we also have visibility on, but it's what we do now is what the XPU model is.\n\n**Hock Tan** (President and CEO)\nShare optimization translating to performance, but also power. That's very important how they play it. It's not just cost. Power translates into total cost of ownership eventually. It's how design it in power and how we balance it in terms of the size of the cluster and whether they use it for training, pre-training, post-training, inference, test time scaling. All of them have their own characteristics. And that's the advantage of doing that XPU and working closely with them to create that stuff. Now, as far as your question on China and all that, frankly, I don't have any opinion on that at all. To us, it's a technical game.\n\n**CJ Muse** (Senior Managing Director)\nThank you very much.\n\n**Operator**\nOne moment for our next question. And that will come from the line of Christopher Rolland with Susquehanna. Your line is open.\n\n**Christopher Rolland** (Senior Equity Analyst)\nHey, thanks so much for the question.\n\n**Christopher Rolland** (Senior Equity Analyst)\nAnd this one's maybe for Hock and for Kirsten. I'd love to know, just because you have kind of the complete connectivity portfolio, how you see new greenfield scale-up opportunities playing out here between could be optical or copper or really anything and what additive this could be for your company. And then, Kirsten, I think OpEx is up. Maybe just talk about where those OpEx dollars are going towards within the AI opportunity and whether they relate. Thanks so much.\n\n**Hock Tan** (President and CEO)\nYour question is very broad reaching and our portfolio. Yeah, we have the advantage. And a lot of the hyperscale customers we deal with, they're talking about a lot of expansion. But it's almost all greenfield. Less so brownfield. It's very greenfield. It's all expansion. And it all tends to be next-generation that we do it, which is very exciting. So the opportunity is very, very high.\n\n**Hock Tan** (President and CEO)\nAnd we deploy. I mean, we can do it in copper, but what we see a lot of opportunity from is when you provide the networking connectivity through optical. So there are a lot of active elements, including either multi-mode lasers, which are called vixels, or edge-emitting lasers for basically single mode. And we do both. So there's a lot of opportunity. Just as in scale-up versus scale-out, we used to do, we still do a lot of other protocols beyond Ethernet to consider PCI Express, where we are on the leading edge of that PCI Express. And the architecture or networking switching, so to speak, we offer both. One is a very intelligent switch, which is like our Jericho family with a dumb NIC or a very smart NIC with a dumb switch, which is a Tomahawk. We offer both architectures as well.\n\n**Hock Tan** (President and CEO)\nSo yeah, we have a lot of opportunities from it. All things said and done, all this nice wide portfolio and all that adds up to probably, as I said in prior quarters, about 20% of our total AI revenue, maybe going to 30%. Though last quarter, we hit almost 40%, but that's not the norm. I would say typically, all those other portfolio products still add up to a nice decent amount of revenue for us, but within the sphere of AI, they add up to, I would say on average, be close to 30%. And XPUs, the accelerators, be 70%. If that's what you're driving, perhaps that gives you some shed some light on towards where how one matters over the other. But we have a wide range of products in the connectivity networking side of it. They just add up, though, to that 30%.\n\n**Christopher Rolland** (Senior Equity Analyst)\nThanks so much, Hock.\n\n**Kirsten Spears** (CFO)\nAnd then on the R&D front, as I outlined, on a consolidated basis, we spent $1.4 billion in R&D in Q1, and I stated that it would be going up in Q2. Hock clearly outlined in his script the two areas where we're focusing on. Now, I would tell you, as a company, we focus on R&D across all of our product lines so that we can stay competitive with next-generation product offerings. But he did line out that we were focusing on taping out the industry's first 2-nanometer AI XPU packaged in 3D. That was one in his script, and that's an area that we're focusing on. And then he mentioned that we've doubled the radix capacity of existing Tomahawk 5s to enable our AI customers to scale up on Ethernet towards the one million XPUs.\n\n**Kirsten Spears** (CFO)\nSo I mean, that's a huge focus of the company.\n\n**Christopher Rolland** (Senior Equity Analyst)\nYep. Thank you very much, Kirsten.\n\n**Operator**\nAnd one moment for our next question. And that will come from the line of Vijay Rakesh with Mizuho. Your line is open.\n\n**Vijay Rakesh** (Managing Director)\nYeah. Hi, Hock. Thanks, Tim. Just a quick question on the networking side. Just wondering how much is up sequentially on the AI side and any thoughts around M&A going forward? I've seen a lot of headlines around the Intel product group, etc. So thanks.\n\n**Hock Tan** (President and CEO)\nOkay. On the networking side, as I indicated, Q1 showed a bit of a surge, but I don't expect that to be that mix of 60-40, 60% compute and 40% networking to be something that is normal. I think the norm is closer to 70-30, maybe at best 30%. And so who knows what Q2 is?\n\n**Hock Tan** (President and CEO)\nWe kind of see Q2 as continuing, but that's just, in my mind, a temporary blip. The norm will be 70, 30. And if you take it across a period of time, like six months, a year, to answer your question. M&A, no, I'm too busy. We're too busy doing AI and VMware at this point. We're not thinking of it at this point.\n\n**Vijay Rakesh** (Managing Director)\nThanks, Hock.\n\n**Operator**\nThank you. That is all the time we have for our question-and-answer session. I would now like to turn the call back over to Ji Yoo for any closing remarks.\n\n**Ji Yoo** (Head of Investor Relations)\nThank you, Cherie. Broadcom currently plans to report its earnings for the second quarter of fiscal year 2025 after close of market on Thursday, June 5th, 2025. A public webcast of Broadcom's earnings conference call will follow at 2:00 P.M. Pacific. That will conclude our earnings call today. Thank you all for joining.\n\n**Ji Yoo** (Head of Investor Relations)\nCherie, you may end the call. Thank you.\n\n**Operator**\nLadies and gentlemen, thank you for participating. This concludes today's program. You may now disconnect.",
      "fetched_at": "2026-02-04T15:40:08.962Z"
    }
  ]
}